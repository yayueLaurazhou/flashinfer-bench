{
  "Concept": [
    "divergence",
    "to introduce the following new concepts",
    "in Cooperative Groups is that of objects naming the set of threads that are part of it",
    "perspective",
    "has provided a single",
    "assumes that the CUDA threads execute on a physically separate",
    "GitHub\nrepository contains a complete example using Fabric Memory including Multicast Objects to leverage NVLINK SHARP",
    "describes synchronization patterns both within and across CUDA thread blocks",
    "Concept",
    "like NVSHMEM work internally within a",
    "bank conflict",
    "gives an overview of the PTX virtual machine model",
    "have returned a memory address that points to the GPU memory",
    "seamless cube map filtering will be performed when sampling along\nthe cube face borders",
    "execution",
    "throughput",
    "puts constraints on data accesses while both the CPU and GPU are executing concurrently",
    "span",
    "only allowed synchronization between thread blocks at a kernel completion boundary",
    "exposes the following functionality",
    "as described below",
    "latency",
    "which specifies that a kernel can run at any time following a launch and is not guaranteed to have finished until the host issues a synchronization call",
    "Permalink to this headline",
    "specify exactly which contradictions are forbidden\nbetween the orders observed by different threads",
    "all variables",
    "to include the new cluster scope",
    "specification uses the terms",
    "can be used to guarantee the completion of the asynchronous copy operations",
    "for work submission in CUDA",
    "defines the behavior of",
    "it is first necessary to understand the concept of an execution environment",
    "is a byte",
    "of groups",
    "There are four scopes",
    "apply to PTX programs with any PTX ISA version number",
    "that is the order in which a CUDA thread writes data to shared memory",
    "There is no guarantee of concurrent execution between any number of different thread blocks on a device",
    "introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks",
    "enabling a CUDA kernel to create and synchronize with new work directly on the GPU",
    "within the kernel",
    "also explains and defines how",
    "as documented in the",
    "section",
    "similar to mmap",
    "memory hierarchy",
    "and API used to enable asynchronous concurrent execution at various levels in the system",
    "If explicit semantics qualifiers\nare not specified",
    "also assumes that both the host and the device maintain their own separate memory spaces in DRAM",
    "Programming Model",
    "that does not support independent thread scheduling",
    "PTX Machine Model",
    "Therefore",
    "for every Thread in a Grid",
    "provides acceleration to memory operations via the asynchronous programming model",
    "Permalink to this image",
    "scheduling",
    "is based on primitives of threads",
    "a thread is the lowest level of abstraction for doing a computation or a memory operation",
    "is only valid for returned values of floating",
    "defines contradictions that are disallowed between communication order on the one\nhand",
    "introduced in CUDA",
    "to speed up the computations",
    "as a set of equivalent memory operations with a scalar data type",
    "works on a per user basis",
    "is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C",
    "and API interface for contexts in the Runtime",
    "If the optional",
    "perform a strong read as part of its\nread",
    "provides streams as a mechanism for programs to indicate dependence and independence among kernel launches",
    "effectively constrains the set of such candidate writes from which a read\noperation can return a value",
    "behind the CUDA programming model by outlining how they are exposed in C",
    "that spans GPU sizes from a single unit to many parallel units",
    "If the",
    "to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it",
    "for",
    "described in",
    "button class",
    "and will have undefined behavior",
    "are independent of state spaces",
    "as a pair of equivalent memory operations on the\nscalar data type",
    "by allowing a CUDA program to explicitly associate managed allocations with a CUDA stream",
    "The",
    "allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions",
    "wait",
    "If it is",
    "way to think about the",
    "title",
    "asynchronous",
    "html",
    "do not apply if a PTX program contains one or more",
    "extends the ISO C",
    "png",
    "applies",
    "and instruction set for general purpose parallel\nprogramming",
    "memory",
    "grid",
    "The interpretation of",
    "is the log iterator",
    "the definition of",
    "inter",
    "The producer deposits a value in",
    "simt",
    "automatic",
    "canonical",
    "that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU",
    "documentation",
    "They allow programmers to define a kernel as a C",
    "depend on relations between",
    "pipelined",
    "outlines the programming model",
    "span class",
    "such as CUDA contexts",
    "relates operations executed on memory locations with scalar data types",
    "above",
    "http",
    "introduced with CUDA",
    "Memory Consistency Model",
    "cuda",
    "guarantees that the asserted condition will be true",
    "for programs running on",
    "async",
    "the initial value",
    "block",
    "does not apply to texture",
    "and performance tuning of unified memory is largely similar\nto the model as described in",
    "combine to form four canonical synchronization patterns",
    "is followed",
    "coalescing",
    "developed by NVIDIA that enables dramatic increases in computing performance by harnessing the power of the GPU",
    "smem",
    "assumes a system composed of a host and a device",
    "each multiprocessor has\non",
    "defines the behavior of asynchronous operations with respect to CUDA threads",
    "https",
    "synchronization",
    "The producer threads execute",
    "bandwidth",
    "pipeline",
    "necessary to take advantage of these",
    "the returned value is the linear interpolation of the two",
    "mbarrier",
    "is explicitly parallel",
    "asynchronous pipeline and establishes execution ordering",
    "extends to a system with multiple devices attached to the same host",
    "language extensions",
    "Memory Consistency Model for",
    "consists of the following elements",
    "concept",
    "assumes a device with a weakly",
    "in such a way that the computations that require inter",
    "warp",
    "The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation",
    "This allows the initiating\nthread to continue computing while the hardware asynchronously copies the data",
    "occupancy",
    "and synchronizes with the",
    "programming",
    "thought of as __threadfence_system",
    "where each\ngeneration adds new features and retains all features of previous generations",
    "as\nwindows within the generic address space"
  ],
  "Hardware": [
    "584\n\ncudaDeviceScheduleMask",
    "cuDeviceGetAttribute\n\n\nCUDA Runtime API vRelease Version",
    "with compute capability 6",
    "-multithreading",
    "void foo_host_device",
    "in most cases",
    "code as described in",
    "code is supported\n      fp",
    "void SetElement",
    "0 and 2 to be visible and device 1 to be invisible",
    "pointer value may be used to refer to this\nmemory in cudaMemcpy",
    "and cudaVDPAUSetVDPAUDevice",
    "attribute",
    "function annotated with the",
    "cudaDeviceGetCacheConfig",
    "initializes the primary context for the specified device and makes it\ncurrent to the calling thread by calling cuCtxSetCurrent",
    "to reside in contiguous virtual address ranges as long as they have carved out enough address space",
    "using cudaMallocHost",
    "auto",
    "int peerDevice",
    "Not Supported",
    "cudaEventDestroy",
    "cudaStream_t hStream",
    "142",
    "int x",
    "has been made current to the calling\nthread",
    "to completely execute the thread",
    "CUDA_MEM_ALLOC_NODE_PARAMS_v2",
    "back to host\n    cudaMemcpy2DFromArray",
    "This is only a preference",
    "CUmemGenericAllocationHandle mcHandle",
    "Once the user successfully retains the primary\ncontext",
    "cudaMallocHost",
    "attribute CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT",
    "system call",
    "variable template cannot",
    "Base2",
    "extended lambdas",
    "Software Coherency",
    "cuMemAllocPitch",
    "and dstDevice",
    "0 accessibility even though memPool now has device 1 accessibility",
    "Synchronizes on work launched from thread",
    "asynchronously",
    "concurrent-execution-host-device",
    "96\n\nCUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_PRE_LAUNCH_SYNC",
    "649\n\ncuGraphicsD3D11RegisterResource",
    "the handle of the current context",
    "attribute cudaDevAttrKernelExecTimeout for more information",
    "left unspecified is initialized to 1",
    "to get NvSciSync attributes for",
    "cudaDeviceGetAttribute",
    "CUDA_ERROR_INVALID_VALUE",
    "and have identical allocation properties",
    "of compute capabilities 7",
    "that does not support mapped",
    "directManagedMemAccessFromHost",
    "supports accessing memory using Tensor Map",
    "to return",
    "supports exporting memory to a Win32 KMT handle with\ncuMemExportToShareableHandle",
    "cudaErrorInvalidConfiguration",
    "where the allocations are located",
    "maxTexture1DMipmap\n\nMaximum 1D mipmapped texture size",
    "the memory resides",
    "code machine size",
    "that do not support double-precision\noperations",
    "void fn",
    "code path for compute capability 7",
    "cudaD3D11GetDevice",
    "216",
    "double atomicAdd",
    "by passing flag",
    "0x03",
    "address is guaranteed to not overlap any\nvalid host pointer range and is guaranteed to have the same value across all devices that support unified\naddressing",
    "pointer\nusable in the local process",
    "to use",
    "void f3",
    "cudaExecutionCtxGetDevResource",
    "is called to reinitialize the device",
    "void PutCoord",
    "CUdevice",
    "can be created with posix file descriptor-based IPC",
    "of other nodes in the graph",
    "cudaGraphKernelNodeSetEnabled",
    "has not yet\nbeen enabled from the current device",
    "cudaExecutionContext_t ctx",
    "-side synchronization model",
    "There are however special considerations as described below when the system is in SLI mode",
    "emulation mode was removed with the CUDA 3",
    "-model",
    "23\n\ncudaDeviceRegisterAsyncNotification",
    "unsigned int flags",
    "These elements are written to the corresponding surface sample components",
    "is in cudaComputeModeExclusiveProcess and is occupied by\nanother process or if the device is in cudaComputeModeProhibited",
    "for",
    "702\n\nsrcHost",
    "pageableMemoryAccessUsesHostPageTables\n\nDevice accesses pageable memory via the host",
    "supports host memory registration via cudaHostRegister",
    "-with-only-cuda-managed-memory-support",
    "34\n\n\nCUDA Runtime API vRelease Version",
    "and set id",
    "calls initialize the runtime and the primary context\nassociated with the specified device",
    "memptr",
    "cudaMemPool_t",
    "this will result in an error",
    "Properties include",
    "graphs must be launched from another graph when launched from the device",
    "that has a zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "of the target link",
    "CU_DEVICE_INVALID",
    "and no physical memory associated yet\n    CUmemGenericAllocationHandle mcHandle",
    "dev in the NULL-terminated string pointed to by\nname",
    "function when",
    "Graph Launch by kernels in the graph is not permitted",
    "-side parameter layout",
    "that have a non-zero value for\nthe device attribute",
    "are accessible through the fields",
    "supports deferred mapping CUDA arrays and CUDA mipmapped arrays\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "void __nv_atomic_thread_fence",
    "memory can be performed concurrently with kernel execution for some devices as mentioned in",
    "-wide atomic operations",
    "-level",
    "1 access requirement left implicit",
    "decompress feature",
    "CreateSharedHandle\nwhen referring to a ID3D12Resource object",
    "memPitch\n\nMaximum pitch in bytes allowed by memory copies",
    "136",
    "for initialization errors",
    "and code is compiled for sm",
    "deviceProp",
    "int sum",
    "along with their compute capability",
    "attribute cudaDevAttrPageableMemoryAccessUsesHostPageTables",
    "number to query",
    "heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the request",
    "function with the\nspecified flags",
    "memory copy",
    "on one GPU will consume memory on other GPUs that are part of the SLI configuration of the Direct3D or OpenGL device",
    "work can be used for synchronization",
    "to device",
    "cudaErrorSetOnActiveProcess",
    "attribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS\nis zero for at least one of those GPUs",
    "s memory",
    "cuDeviceGetCount",
    "stream",
    "function\n    auto lam1",
    "104",
    "-management-cdp1",
    "temporarily runs out of physical resources to launch the larger\npreferred clusters",
    "-side updates for device-updatable kernel nodes",
    "101\n\ncuDeviceGetAttribute",
    "directManagedMemAccessFromHost\n\nHost can directly access managed memory on the device without migration",
    "to a non-zero value to force the driver to\nalways use device memory for physical storage",
    "cuDeviceGetP2PAttribute",
    "deviceNumaConfig\n\nNUMA configuration of a device",
    "for the GPUs to be used by a D3D10 device in the next frame",
    "that may not be visible in the process calling the\nAPI",
    "to get properties for",
    "IDXGIAdapter",
    "The following code snippet illustrates querying compressible memory support",
    "function parameters were previously restricted to the\nregister state space",
    "Multithreading",
    "as well as the associated stream",
    "will be selected",
    "void cudaTriggerProgrammaticLaunchCompletion",
    "void cudaGridDependencySynchronize",
    "func",
    "pointer for the requested context",
    "as the current device for the calling host thread",
    "flags",
    "deviceGraph1",
    "int x1b",
    "code during kernel execution",
    "memory\n    cuMemcpyHtoD",
    "supports the Stream Ordered Memory Allocator",
    "235\n\n6",
    "Instead",
    "as the current thread",
    "from hStream",
    "If the kernel is intended to update an existing tensor map in device memory",
    "does not natively support ordering of remote writes",
    "cuGraphicsD3D10RegisterResource",
    "pointer to allocated memory",
    "cudaMemcpy3DAsync",
    "void __nv_atomic_xor",
    "in order to preserve stream semantics",
    "class 12",
    "If len is not 0 and device_arr is NULL or if len exceeds the\nnumber of devices in the system",
    "does not natively support ordering of GPUDirect RDMA writes",
    "This occurs even if an argument was non-trivially-copyable",
    "that supports unified addressing to another peer device that\nsupports unified addressing using cudaDeviceEnablePeerAccess",
    "with Compute Capability 8",
    "float operator",
    "ordinal and the\ndevice must have a non-zero value for the device attribute cudaDevAttrConcurrentManagedAccess",
    "lambda\n  static_assert",
    "is called on devices without configurable shared memory",
    "vkPhysicalDeviceProperties2",
    "-side-launch-from-ptx",
    "as well",
    "to either pageable or pinned host memory",
    "and srcHost are the base addresses of\nthe destination and source",
    "has already been initialized",
    "functions or on variables in the",
    "to host",
    "at most\ncudaDeviceCount of the the CUDA-compatible devices corresponding to the Direct3D 10 device\npD3D10Device",
    "has completed the requested task",
    "schedulers and load balancers dynamically and adapting in response to data-driven decisions or workloads",
    "encountered a load or store instruction on a memory address\nwhich is not aligned",
    "int flags",
    "launched kernels and CUDA objects are visible to all threads in a grid",
    "copies are initiated using the standard memory copy functions with destination and source addresses residing on the same device",
    "dptr_out",
    "-graph-launch",
    "with compute capability less than 7",
    "as needed",
    "attribute\nCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES",
    "the CUDA-compatible device corresponding to the adapter pAdapter\nobtained from IDXGIFactory",
    "deviceSupportsFabricMem",
    "must not be created on a linked node adapter",
    "from stream",
    "the number of CUDA-compatible device corresponding\nto the Direct3D 11 device pD3D11Device",
    "for the GPUs to be used by a D3D9 device in the next frame",
    "This runtime-driver context\nsharing is important when using the driver API in conjunction with libraries built on the runtime API",
    "code is generated for the body of a",
    "to get the device pointer for these\nallocations",
    "on which the allocations to be directly accessed by dev reside",
    "-updatable have additional restrictions compared to regular kernel\nnodes",
    "the number of CUDA-compatible device corresponding to the\nDirect3D 9 device pD3D9Device",
    "code\n__global__ void MyKernel",
    "as described below",
    "int bar",
    "cudaInvalidDeviceId",
    "whose index precedes the invalid index are visible to CUDA applications",
    "memory subsequently allocated from this host thread using cudaMalloc",
    "variable",
    "Pointer to device memory",
    "-updatable kernel node\nwhile it is being updated from the device will result in undefined behavior",
    "There is a one-to-one mapping between CUDA\ndevices in the runtime and their primary contexts within a process",
    "pointer associated with symbol",
    "This runtime-driver context\n\n\nCUDA Runtime API vRelease Version",
    "process_shared_data",
    "has completed all work in the stream",
    "accelerated video encode",
    "register file",
    "void unknown_function",
    "was added in CUDA 11",
    "on which the function is currently loaded",
    "code matches the alignment requirement in host code and can therefore be obtained using",
    "work in between",
    "caller for this analysis",
    "which has a zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "function will synchronize on all work launched by any thread in the thread-block up to the point where",
    "cannot be used again",
    "When an action is taken in the legacy stream such as a\nkernel launch or cudaStreamWaitEvent",
    "s state",
    "was introduced in release CUDA 11",
    "bool f",
    "cudaMemsetAsync",
    "leaving it free for other tasks",
    "gpuPciDeviceID\n\nThe combined 16-bit PCI device ID and 16-bit PCI vendor ID",
    "when using the driver API",
    "can start",
    "40\n\ncudaSetDeviceFlags",
    "has no effect on Linux operating systems",
    "ordinal cudaMemLocation",
    "object contains a kernel with a parameter larger than 4KB",
    "cudaLaunchDevice",
    "Pitch\ncannot exceed CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH",
    "properties variable\ncudaGetDeviceProperties",
    "cudaGraphicsD3D9RegisterResource",
    "is specified and the destination is unified memory",
    "attr",
    "and can\nbe queried from the primary context via cuCtxGetDevResource",
    "If CU_MEM_ATTACH_HOST is specified",
    "encountered a stack error",
    "have been added",
    "pointer returned by cudaIpcOpenMemHandle",
    "does not support IPC",
    "supports deferred mapping CUDA arrays and CUDA mipmapped arrays",
    "entry function pointer",
    "int zzz",
    "compute modes",
    "cudaGraphicsD3D10RegisterResource",
    "unifiedAddressing\n\nDevice shares a unified address space with the host",
    "provided Unified Virtual Addressing is enabled",
    "the CUDA-compatible device corresponding to the adapter name\npszAdapterName obtained from EnumDisplayDevices or IDirect3D9",
    "of a vector are supported",
    "is set to this",
    "work follows the function",
    "features",
    "0\ncudaMemPoolProps poolProps",
    "632\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "is\ntrue",
    "code optimizer assumes that the function does not change any mutable state visible to caller functions",
    "functions",
    "immediately",
    "then this identifies the device on which the memory referred to by ptr\nphysically resides",
    "on this device",
    "for IPC functionality by\ncalling cuDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED\n\n\nSee also",
    "with compute capability of at least 7",
    "Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask",
    "must be\nnon-zero",
    "in the system can consistently consume GPUDirect RDMA writes to this device",
    "driver",
    "pointer of mapped pinned memory",
    "that corresponds to the Direct3D 11 device on which the objects were created can be determined by comparing the LUID of a CUDA device with that of the Direct3D 11 device",
    "unsigned int __isGridConstant",
    "variables are in the kernel",
    "320\n\ncudaDeviceGraphMemTrim",
    "-graph-upload",
    "was called",
    "ordinal",
    "This flag enables allocating pinned host memory that is accessible to the",
    "This flag cannot be used in conjunction with\ncudaGraphInstantiateFlagAutoFreeOnLaunch",
    "ID3D12Device",
    "attribute\nto 0 will result in an error",
    "that has a non-zero alue for the device attribute\nCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS",
    "of compute\ncapability 9",
    "cudaErrorNotSupported",
    "0 as current\ncudaStream_t s0",
    "accessPolicyMaxWindowSize\n\nThe maximum value of cudaAccessPolicyWindow",
    "cuGraphicsD3D11RegisterResource\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "must be among the devices returned when querying\ncudaD3D10DeviceListAll from cudaD3D10GetDevices",
    "void f6",
    "104\n\ncudaLaunchHostFunc",
    "and at any time",
    "that have a non-zero\nvalue for the device attribute",
    "deviceList",
    "graph from the host and device simultaneously will result in undefined behavior",
    "599\n\ncuGLGetDevices",
    "deviceUpdatable can\nonly be set to 0 or 1",
    "streamPrioritiesSupported\n\nDevice supports stream priorities",
    "and are given in",
    "memoryPoolsSupported\n\n1 if the device supports using the cudaMallocAsync and cudaMemPool family of APIs",
    "uses unified addressing",
    "to get primary context flags for",
    "cudaDeviceCanAccessPeer",
    "and the allocation type is\ncudaMemAllocationTypePinned",
    "Base",
    "148",
    "as seen from the GPU",
    "for CUDA execution in priority order using device_arr",
    "with zero scheduling overhead",
    "__host__",
    "corresponding to a D3D10 device",
    "cuda",
    "corresponding to a D3D11 device",
    "137",
    "operation",
    "and is used by applications at runtime to determine which hardware features and",
    "132",
    "is reset\nusing cudaDeviceReset",
    "703\n\ndstHeight",
    "system calls - in such case cudaErrorInvalidValue will\nbe returned",
    "The driver is free to modify the requested value to meet h",
    "size_t size",
    "memory\n\n\nCUDA Runtime API vRelease Version",
    "that have that advice set for that memory\nrange",
    "cudaGraphLaunch",
    "CUDA_MEMCPY3D_PEER_v1",
    "cuDeviceGet",
    "applications",
    "cuMemSetMemPool\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "execution context",
    "if all pages in the memory range have the same GPU as\ntheir preferred location",
    "Note that",
    "for this location and\nallocation type or the location",
    "will only wait for the kernel launched on the GPU\ncorresponding to that stream to complete before it begins execution",
    "host threads",
    "114\n\ncuDevicePrimaryCtxRelease",
    "corresponding to CUDA context",
    "for all GPUs used by a D3D10 device\n\n\nCUDA Runtime API vRelease Version",
    "pointer ptr",
    "pNext",
    "-initializer-list",
    "regsPerMultiprocessor\n\n32-bit registers available per multiprocessor",
    "of the SoC",
    "as occurring before any write to all memory made by the calling thread before the call to",
    "function parameters or any other",
    "which share these features are said to be part of the same family",
    "the number of CUDA-compatible devices corresponding\nto the Direct3D 11 device pD3D11Device",
    "If there",
    "pointer returned by\ncudaHostGetDevicePointer",
    "operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available",
    "indicates\nthat the CUdevice of the current context cannot directly access memory from the CUdevice of\npeerContext",
    "with only CUDA Managed Memory support",
    "that do not support concurrent data transfers",
    "with compute capability lower than 6",
    "pointer value through which kernels running in the current CUcontext may\naccess ptr then CUDA_ERROR_INVALID_VALUE is returned",
    "will be returned in all the extra space\nprovided",
    "with the",
    "storage at global scope",
    "because the memory may be mapped into other CUDA contexts via the\nCU_MEMHOSTALLOC_PORTABLE flag",
    "This lead to users unwittingly paying runtime cost of mapping all cudaMalloc allocations to peer devices",
    "ranges from zero up to the\nnumber of thread ids in that CTA dimension",
    "computePreemptionSupported\n\nDevice supports Compute Preemption",
    "pma",
    "cudaEventCreateWithFlags",
    "with compute capability 9",
    "-side-encoding-and-modification-of-a-tensor-map",
    "outside of CUDA",
    "may be registered\nand mapped through the lifetime of this CUDA context",
    "to copy a large chunk of data into shared memory",
    "memory which can no longer be used for user allocations",
    "supports HOST_NUMA location with the cuMemAllocAsync and cuMemPool family of\nAPIs",
    "may be called from within divergent code",
    "inbetween by this or another\nthread",
    "27\n\ncudaDeviceSetMemPool",
    "704\n\nsrcHeight",
    "maintain their own separate memory spaces in DRAM",
    "runtime system software will attempt to track new pending launches in a lower performance virtualized buffer",
    "with some specific flags",
    "memory to a 1D CUDA array",
    "concurrentManagedAccess\n\nDevice can coherently access managed memory concurrently with the CPU",
    "that support managed memory have to be peerto-peer compatible with each other",
    "that support Compute Data Compression can do so by setting",
    "CUdevice device",
    "void __nv_atomic_min",
    "in that specific family",
    "cannot be freed using the runtime",
    "flag",
    "deviceNumaId\n\nNUMA node ID of the GPU memory\n\n\nCUDA Runtime API vRelease Version",
    "or\ncudaDeviceUnregisterAsyncNotification",
    "alignment requirements are met",
    "of compute capability 5",
    "is returned",
    "cudaGraphKernelNodeUpdatesApply",
    "Maps the allocation into the CUDA address space",
    "-side\ncudaGraphLaunch",
    "and CDP2 functions are able to use CDP2-specific features",
    "supports virtual memory management APIs like cuMemAddressReserve",
    "dstDevice",
    "supports using the cuMemHostRegister flag CU_MEMHOSTERGISTER_READ_ONLY to\nregister memory that must be mapped as read-only to the GPU",
    "673\n\nextentDepth",
    "The\nparticular CUdevice on which the memory resides is the CUdevice of the CUcontext returned by the\nCU_POINTER_ATTRIBUTE_CONTEXT attribute of ptr",
    "is currently being used",
    "cudaSetDeviceFlags",
    "deviceUpdatable\ncan only be set to 0 or 1",
    "that either have peer-to-peer support with each other or have a non-zero value for\nthe device attribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "The default value if the parameter is zero",
    "memory between two contexts asynchronously",
    "are not CUDA capable then the call will return\nCUDA_ERROR_NO_DEVICE",
    "code cannot be initialized with the address of a",
    "have the attribute",
    "is trying to re-enable peer\naddressing on from a context which has already had peer addressing enabled",
    "CUDA_MEM_FREE_NODE_PARAMS",
    "within a single process will substantially degrade\nperformance and is strongly discouraged",
    "86\n\nCUdevice_v1",
    "is specified and the source data is unified memory",
    "vkPhysicalDevice",
    "memory through texture or surface fetching present some benefits that can make it an advantageous alternative to reading device memory from global or constant memory",
    "Query the devices used by the current OpenGL context",
    "for the GPUs to be used by the current OpenGL context in the next frame",
    "at all levels then the synchronization depth might be substantially different to the nesting depth",
    "thread-id-a",
    "warp scheduler",
    "operation but reading after the writing device operation completes are also considered to have inconsistent and incoherent proxies",
    "void __nv_atomic_add",
    "copy simultaneously with kernel execution",
    "cuKernelSetCacheConfig",
    "-with-compute-capability-5-x",
    "driver is upgraded",
    "CreateFence",
    "at a time",
    "120",
    "memory allocation",
    "return x",
    "that corresponds to the Direct3D 12 device on which the objects were created can be determined by comparing the LUID of a CUDA device with that of the Direct3D 12 device",
    "116\n\ncuDevicePrimaryCtxRetain",
    "memory oversubscription is possible for GPUs that have a non-zero value for the device\nattribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "when\ncalled",
    "using cudaDeviceReset",
    "of compute capability less than 9",
    "have additional restrictions compared to regular kernel nodes",
    "The result is much simpler code than occurs with explicitly copying data between host and device",
    "cudaDeviceEnablePeerAccess",
    "are non-migratable device allocation located on that device",
    "const char",
    "for IPC functionality by\ncalling cudaDeviceGetAttribute with cudaDevAttrIpcEventSupport\n\n\nCUDA Runtime API vRelease Version",
    "function\nconsteval __device__ int dcallee",
    "supports IPC Events",
    "name\n\nASCII string identifying device",
    "pdptr",
    "will fail if the CU_MEMHOSTALLOC_DEVICEMAP flag was not\nspecified at the time the memory was allocated",
    "resources required to support device connections have been\nexhausted",
    "runtime",
    "with compute capability of 6",
    "that do not\noccur in the source vector will be written with an unpredictable value",
    "code do not set the global",
    "runtime these functions map to device-side",
    "associated with the current OpenGL context",
    "PixelRGBA",
    "only non-constexpr function",
    "device to device",
    "pfn_cuDeviceGetUuid",
    "is still doing work in the stream when",
    "functions of Section B",
    "CUuuid",
    "if all pages in the memory range have the CPU as their preferred\nlocation",
    "starting from compute capability 9",
    "intrinsics",
    "gpuDirectRDMASupported\n\n1 if the device supports GPUDirect RDMA APIs",
    "schedules the clusters",
    "for this thread\n\n\nCUDA Runtime API vRelease Version",
    "is capturing",
    "242\n\ncudaD3D9GetDevice",
    "and the copy",
    "If the CU_MEM_ATTACH_HOST flag is specified",
    "unsigned int",
    "This is useful in situations where the typical\nway of transferring the tensor map",
    "261\n\ncudaGraphicsD3D10RegisterResource",
    "-updatable kernel\nnode while it is being updated from the device will result in undefined behavior",
    "attribute concurrentManagedAccess",
    "cudaFuncGetAttributes",
    "can allocate managed memory on this system",
    "the corresponding GPU id must be set with",
    "Otherwise",
    "111",
    "function cannot be predicated",
    "for Direct3D and",
    "code in a non-constexpr context",
    "to the multicast object is permanent during the life time of the multicast object",
    "will fail with error\ncode cudaErrorSyncDepthExceeded if the limitation is violated",
    "pointer and can therefore be read and written by kernels or via",
    "106",
    "NUMA configuration",
    "is callable may require up to 860MB of device memory",
    "as thecalling kernel",
    "Regardless of where they are created",
    "in pCtx",
    "with the location id as the device",
    "management functions of the CUDA runtime application\nprogramming interface",
    "in the copy stream",
    "encountered a load or store instruction on an invalid memory\naddress",
    "dev to an integer value specified by val",
    "cudaError_t cudaLaunchDevice",
    "maxSurface2DLayered\n\nMaximum 2D layered surface dimensions",
    "s signal before consuming shared data",
    "1\nMyKernel",
    "-coherency-vs-software-coherency",
    "this destroys the primary context of the device the host thread currently\noperates on",
    "emulation functions",
    "The imported memory pool does not inherit any accessibility set by the exporting process",
    "cudaErrorInvalidValue\ncudaErrorNotPermitted cudaErrorUnknown",
    "uuid\n\n16-byte unique identifier",
    "that has a non-zero value for the\ndevice attribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "630\n\nlaunchCompletionEvent",
    "can possibly execute multiple kernels concurrently",
    "local_processing",
    "a device handle given a PCI bus ID string",
    "The application can use the",
    "managedMemory\n\nDevice supports allocating managed memory on this system",
    "which configures the graph for launch from the device",
    "pDevPtr",
    "1\ncudaSetDevice",
    "values range from",
    "extended lambdas for which the trait",
    "302\n\ncuMulticastBindAddr",
    "if the graph is instantiated for device",
    "for this device or the device",
    "and host",
    "-updatable node also do not\nallow multiple instantiation",
    "acceleration for addition\n cg",
    "-side launches may fail for many reasons",
    "nvcc",
    "Default driver behaviour",
    "with a pointer and size parameter",
    "through cuMemCreate",
    "must\nbe added to the multicast team before any memory can be bound to any device in the team",
    "Setting this limit must be performed before any\n\n\nCUDA Runtime API vRelease Version",
    "code of a function declared with the extern qualifier is only allowed if the function is defined within the same compilation unit as the device code",
    "symbol is a variable that resides\nin global or constant memory space",
    "can share a unified address space with the host",
    "130",
    "each with their own separate memory",
    "graph structure is fixed at time of instantiation and cannot be updated without re-instantiation",
    "is\nthe base device pointer of the destination memory and dstContext is the destination context",
    "on pAdapter is CUDA-compatible then the call will fail",
    "can support",
    "supports exporting memory to a fabric handle with cuMemExportToShareableHandle",
    "luidDeviceNodeMask\n\nLUID device node mask",
    "will be returned if a device\nthat supports managed memory is used and it is not peer-to-peer compatible with any of the\nother managed memory supporting devices that were previously used in that process",
    "A portable cluster size is guaranteed to be functional on all\ncompute capabilities higher than the target compute capability",
    "to Host",
    "0x01\n\nIf set",
    "ptr",
    "it is necessary to register the resources for each separately",
    "configuration",
    "cudaGetCurrentGraphExec",
    "associated with vdpDevice is not\na compute device",
    "pointer passed to the API call is not a valid device pointer",
    "133",
    "resources required to enable peer access have been exhausted\nfor one or more of the devices passed to cudaEnablePeerAccess",
    "-updatable kernel node",
    "cudaD3D9GetDirect3DDevice\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "major",
    "cudaGetDeviceCount",
    "deviceUUID",
    "no longer needs to be staged through the host and is therefore faster",
    "103\n\ncuDeviceGetHostAtomicCapabilities",
    "cuIpcOpenMemHandle can attempt to enable peer\naccess between the devices as if the user called cuCtxEnablePeerAccess",
    "code\ntemplate",
    "per CUDA array element",
    "cudaPeekAtLastError",
    "operator",
    "it must be instantiated explicitly for device launch",
    "supports the integrated stream ordered memory allocator\nmay be queried by calling cuDeviceGetAttribute",
    "float GetElement",
    "cudaDeviceGetByPCIBusId",
    "on a pointer created on the host",
    "CreateBuffer",
    "Architecture-specific features can only be used with targets that support these\nfeatures",
    "primary contexts specified in the API are referenced instead of the current context",
    "runtime must be linked",
    "can determine\nif a mapping is possible",
    "have their\nABI tied to the major release of the CUDA runtime",
    "Memory",
    "with full CUDA Unified Memory support",
    "for device memory or cudaMemoryTypeManaged for managed memory",
    "memory\n    cudaFree",
    "The source must be coherently accessible from the device in the copy stream",
    "is the home for the physical allocation but other GPUs in the system will access the memory at reduced bandwidth over the PCIe bus",
    "-memory",
    "cudaD3D10GetDevices",
    "uuid from cudaDeviceGetProperties for this device",
    "Graph Update",
    "gpuDirectRDMAWritesOrdering\n\nSee the cudaGPUDirectRDMAWritesOrdering enum for numerical values",
    "function\n  nvstd",
    "and host compilation",
    "etc",
    "can dereference a pointer to the memory of the other device",
    "readable via texture or surface references",
    "without using the driver API to encode the",
    "symbols stored in\nCU_JIT_GLOBAL_SYMBOL_NAMES",
    "char",
    "with support for unified addressing on Linux and Windows\noperating systems",
    "cuCtxSynchronize",
    "resources required to create MPS client have been exhausted",
    "lambda expression with an instance of a placeholder type in the code sent to the host compiler",
    "struct cudaDevResource struct cudaDevSmResource struct cudaDevSmResourceGroupParams struct cudaDevWorkqueueConfigResource struct cudaDevWorkqueueResource struct cudaEglFrame struct cudaEglPlaneDesc struct cudaEventRecordNodeParams struct cudaEventWaitNodeParams\n\nCUDA Runtime API vRelease Version",
    "for all devices",
    "has a fixed number of texture bindings that can be accessed within a single kernel",
    "mytype xxx",
    "int xxx",
    "will henceforth be referred to as device graphs",
    "is in cudaComputeModeExclusiveProcess and is\noccupied by another process or if the device is in cudaComputeModeProhibited",
    "load and store operations have the same requirements on preservation of naturally aligned accesses",
    "pointer and dstPitch offset must be two byte aligned",
    "runtime launch synchronize depth",
    "is integrated with host memory",
    "because the memory\nmay be mapped into other CUDA contexts via the cudaHostRegisterPortable flag",
    "id or",
    "memory allocations from this context are invalid and must be\nreconstructed if the program is to continue using CUDA",
    "will be shared via MPS",
    "void mutex_lock",
    "or host physical memory associated with it",
    "576\n\ncuDevResourceGenerateDesc",
    "Implementation",
    "attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "lambda",
    "supports GPUDirect RDMA APIs",
    "if srcDevice or dstDevice are not valid or if they\nrepresent the same device",
    "code management by providing implicit primary context initialization\nand management",
    "where\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS is true or system-allocated\npageable memory on devices where CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS\nis true",
    "thread",
    "If cudaMemAttachHost is specified",
    "-side kernel launch functionality",
    "T __nv_atomic_fetch_min",
    "will be temporarily changed if\nneeded to suit the function",
    "function during device compilation",
    "void device_func",
    "of a memory block of 64 KB or less",
    "void free",
    "have a non-zero value for\ncudaDevAttrPageableMemoryAccess",
    "that does not have this attribute set will cause cudaHostRegister to\nerror with cudaErrorNotSupported",
    "doesn",
    "with\nattribute CU_DEVICE_ATTRIBUTE_UNIFIED_FUNCTION_POINTERS present in the system",
    "poolSupportedHandleTypes",
    "there is no distinction\nbetween a device pointer and a host pointer -- the same pointer value may be used to access memory\nfrom the host program and from a kernel running on the device",
    "that may be signed or unsigned 8",
    "will immediately be accessible by the current device",
    "90\n\nCU_DEVICE_INVALID",
    "to\ncopy a tile of data between shared and global memory",
    "attribute CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY",
    "temporarily runs out of\nphysical resources to launch the larger preferred clusters",
    "-variable-specifier",
    "134",
    "int doit",
    "cudaKernel_t kernel",
    "which uses the unified address space",
    "as launch configuration decisions can now be made at runtime by threads executing on the device",
    "the registration happened",
    "596\n\ndeviceNumaId",
    "118\n\n6",
    "handle\n\n\nCUDA Runtime API vRelease Version",
    "are not CUDA capable then the call will return\ncudaErrorNoDevice",
    "CUDA_MEMCPY3D_v2",
    "pointer value through which any peer memory may be accessed in the current context is the\nsame pointer value through which that memory may be accessed in the peer context",
    "graph to enqueue itself for a tail launch",
    "cuMemCreate\n\ncuMulticastBindAddr_v2",
    "as specified in",
    "might be associated with a device other than device 0 if device 0 turns out to be in prohibited mode or in exclusive-process mode and used by another process",
    "it is launched into its own execution environment",
    "cuDeviceGetPCIBusId",
    "On Windows the flag is a no-op",
    "to\ncudaNvSciSyncAttrWait",
    "that link to the same instance of the CUDA runtime",
    "that is different from the current device",
    "with a D3D11 device in order to achieve maximum interoperability performance",
    "has full read-write access to the memory",
    "Memory Access",
    "cudaOccupancyMaxActiveBlocksPerMultiprocessor",
    "cudaStreamWaitEvent",
    "tccDriver\n\n1 if device is a Tesla device using TCC driver",
    "__host__  void swap",
    "-side synchronization is no longer possible with CDP2 or on devices of compute capability 9",
    "of a complex-valued tensor",
    "may switch to launch the regular clusters instead to attempt to utilize as\nmuch of the physical device resources as possible",
    "symbol names that will be relocated to the corresponding host\naddresses stored in CU_JIT_GLOBAL_SYMBOL_ADDRESSES",
    "code in 64-bit mode",
    "with the following fields",
    "maxTexture2DLayered\n\nMaximum 2D layered texture dimensions",
    "On platforms without\nCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES",
    "pbase",
    "pointers",
    "memory allocated using cuMemAlloc or cuArrayCreate will not be\nevicted",
    "as well as on the number of multiprocessors and memory bandwidth of the device",
    "reserved\n\nReserved for future use",
    "emulation\nmode was removed with the CUDA 3",
    "-dependent code to allocate arrays that conform to these constraints",
    "extended lambdas cannot be",
    "cudaLimitMallocHeapSize",
    "so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described in",
    "for __constant__ variables in a CUDA C kernel in bytes",
    "lambda in bar4",
    "that do not support\nmapped pinned memory",
    "-memory-space-specifiers",
    "is made",
    "synchronization\nbetween GPU work in different processes",
    "supports unified addressing may be queried by calling cuDeviceGetAttribute",
    "The parameter specified by\nfunc must be declared as a __global__ function",
    "32\n\nCUdriverProcAddress_flags",
    "for all GPUs used by the current OpenGL context",
    "are preserved as single accesses\nfrom the point of view of the host and other devices",
    "116",
    "int divergent_compute",
    "in the system must have a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS otherwise the API will return\nan error",
    "cudaSetDoubleForHost",
    "will block routing over EGM\nand cause performance issues",
    "and\nthe number of components per array element",
    "mad",
    "code to PTX at runtime",
    "from any stream on a device that has a zero\nvalue for the device attribute cudaDevAttrConcurrentManagedAccess",
    "coherency vs",
    "void producer",
    "coherency",
    "and is defined within the immediate or nested block scope of a",
    "637\n\nsrcPos",
    "on the same multi-GPU board",
    "__constant__",
    "encountered a load or store instruction on a memory address which is not aligned",
    "for CUmemLocation",
    "enforces an alignment requirement on texture base addresses",
    "flag to the",
    "in device code cannot be used in any runtime or driver API calls",
    "cudaErrorTooManyPeers",
    "runtime call cudaDeviceSynchronize",
    "at this time",
    "supports caching globals in L1",
    "textureAlignment",
    "on which to register the callback",
    "is deinitialized then this\nallocation will vanish with that device",
    "pointer returned by cuIpcOpenMemHandle",
    "lambda body",
    "from peerDevice",
    "memory spaces",
    "via cudaGraphKernelNodeSetParam",
    "int my_thread_data",
    "-mapped host memory are permitted",
    "-accessibility-for-multi-gpu-support",
    "714",
    "memory space specifier\n\n  static __shared__ int i5",
    "cudaGraphicsD3D11RegisterResource",
    "Partition into many\nsegments and assign segments such that",
    "cuCtxGetFlags",
    "Enumeration and Properties",
    "deviceSupportsIpcHandle",
    "to 1 on a captured\nlaunch causes the resulting kernel node to be device-updatable",
    "default",
    "-luids-dir3d-11-int",
    "remains allocated for the lifetime of the CUDA context",
    "1 accessible from device 1\ncudaGraphAddMemAllocNode",
    "temporarily runs out of physical resources to\nlaunch the larger preferred clusters",
    "138",
    "accesses this memory region",
    "This is only a\npreference",
    "pCudaDevice",
    "and explicit memory transfers between them",
    "entry function to search for",
    "function body",
    "to 2",
    "cudaSetValidDevices",
    "code is encountered later in the translation unit",
    "414\n\ncuDeviceGraphMemTrim",
    "function parameter is moved to a register",
    "where the size of\nthe L1 cache and shared memory are fixed",
    "were detected by the installed CUDA driver",
    "specifies the base pointer of the\nsource",
    "supports switch multicast and reduction operations",
    "or cudaMemcpyDeviceToHost and\nstream is non-zero",
    "accesses this memory\nregion",
    "that does\nnot have the device attribute cudaDevAttrConcurrentManagedAccess set",
    "limit cudaLimitDevRuntimeSyncDepth",
    "and same major\ncompute capability",
    "acceleration for addition\n    int avg",
    "auto i",
    "that support all features of the highest-numbered architecture\nlisted in the program",
    "function input and return parameters may have their address taken via",
    "when applicable",
    "-side graph launch is per-thread and multiple launches may occur from different threads at the same time",
    "__host__  explicit operator bool",
    "595\n\naccessPolicyMaxWindowSize",
    "in the system must have a non-zero value for the device attribute\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "homeDevice",
    "117",
    "lambda defined in a host function",
    "launched kernels and CUDA objects are visible to all threads in a thread block",
    "to finish work",
    "int d1",
    "and map Direct3D or OpenGL resources to the CUDA device returned by",
    "CreateSharedHandle\nwhen referring to a ID3D12Fence object",
    "unsigned int cudaDeviceCount",
    "frameworks",
    "is not initialized",
    "are applied to the requested device",
    "dstPitch and dstHeight are ignored",
    "is achieved by using page-locked host memory as described in",
    "textureAlignment\n\nAlignment requirement for textures\n\n\nCUDA Runtime API vRelease Version",
    "the attribute",
    "void f4",
    "and are selected by using a",
    "the CUDA runtime offers an API for launching kernels",
    "handle for the specified execution context",
    "CDP1",
    "The current memory pool for a device may be set with",
    "on which the allocations to be directly accessed by device reside",
    "cuMemSetMemPool",
    "cudaHostRegister",
    "entry function to search kernel for",
    "supports buffer sharing with dma_buf mechanism",
    "Execution Context",
    "and is initialized at the first runtime function which requires an active\ncontext on this device",
    "with an OpenGL context in order to achieve maximum interoperability performance",
    "function pointer",
    "will unmap and release any physical memory reserved by graph memory nodes that is not actively in use",
    "in the following format",
    "srcY",
    "attribute\nCU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH",
    "-graph-creation",
    "code generation",
    "against which the current CUDA context was created",
    "which support unified addressing",
    "392\n\ncuLaunchHostFunc",
    "s default memory pool cannot be destroyed",
    "unifiedFunctionPointers present in the system",
    "with",
    "function bodies\n    decltype",
    "to finish",
    "as the VdpDevice for VDPAU interoperability with the CUDA device device\nand sets device as the current device for the calling host thread",
    "runtime does not allow creation or destruction of texture or surface objects from within device code",
    "vkPhysicalDeviceIDProperties",
    "1 as current\nfloat",
    "cuDeviceGetUuid",
    "maxTextureCubemap\n\nMaximum Cubemap texture dimensions\n\n\nCUDA Runtime API vRelease Version",
    "and",
    "92\n\nCU_MEMHOSTALLOC_PORTABLE",
    "numDevices",
    "vector",
    "lambda is supported",
    "0x04",
    "asynchronously copies the data",
    "dev in the NULL-terminated string pointed to by\npciBusId",
    "errors This leaves the process in an inconsistent\nstate and any further CUDA work will return the same error",
    "may directly access a peer device",
    "using unified\naddressing is current",
    "-runtime-api-in-cuda-code-cdp1",
    "-side function",
    "can be\naccessed from the driver API as described in",
    "for use with graphs back to the OS",
    "because the memory\nmay be mapped into other CUDA contexts via the cudaHostAllocPortable flag",
    "where the size of the L1\ncache and shared memory are fixed",
    "where",
    "and the current device must match",
    "or host memory",
    "for IPC functionality by\ncalling cuapiDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED\n\n\nSee also",
    "runtime synchronize depth",
    "where\ncudaDevAttrConcurrentManagedAccess is true or system-allocated pageable memory on devices\nwhere cudaDevAttrPageableMemoryAccess is true",
    "from any stream on a device that has a zero value\nfor the device attribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "which will put constraints on concurrent CPU access to that data while the",
    "address for the memory",
    "and not all allocations are required to be mapped to all the devices",
    "that does not have the device attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS set",
    "based on the",
    "graph from the device while a previous launch of the graph is running will result in an error",
    "for dev",
    "hostRegisterSupported\n\nDevice supports host memory registration via cudaHostRegister",
    "multiProcessorCount",
    "is not being accessed by any other host threads from the process when this function is called",
    "-runtime-api-in-cuda-code",
    "if either all the pages don",
    "do not need to be flushed for consumers within the scope\nindicated by the returned attribute",
    "or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly",
    "701\n\ndstHost",
    "code Applicable options",
    "execution context\nwhich is a",
    "that have a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM",
    "different than that of the specified hStream",
    "must be among the devices returned\nwhen querying CU_D3D9_DEVICES_ALL from cuD3D9GetDevices",
    "pD3D10Device",
    "cudaMemGetInfo",
    "it will be a valid device ordinal or if it returns\ncudaMemLocationTypeHostNuma",
    "accelerated decompression",
    "to interop with",
    "associated with the current context has on the corresponding\nmemory referenced by the pointer given",
    "they want to use support CUDA Virtual Memory Management",
    "functions and must appear between\na",
    "with Compute Capability 7",
    "Will return attributes for any device",
    "and the bus connecting the host to the devices",
    "and shared with the CUDA runtime API",
    "528\n\ncudaDeviceNumaConfig",
    "The\ndriver tracks the virtual memory ranges allocated with this function and automatically accelerates calls\nto functions such as cuMemcpy",
    "function declarations",
    "runtime grid launch did not occur because the depth of the child\ngrid would exceed the maximum supported number of nested grid launches",
    "and toolkits and to ensure that at least one thread block can run on an SM",
    "to try",
    "s supported range",
    "constexpr function and device code can invoke a",
    "must be allocated via this function",
    "in cudaMemLocation",
    "are specified",
    "cudaDeviceGetP2PAtomicCapabilities",
    "has its own default stream",
    "105",
    "acceleration with integral types are",
    "accesses data directly from the host",
    "-side execution",
    "requires a",
    "are busy or unavailable at the current time",
    "code generation phase",
    "of the CUDA Driver",
    "supports using the cudaMallocAsync and cudaMemPool family of APIs",
    "int dstDevice",
    "will also be synchronized",
    "of the function cannot change",
    "CUDA_ERROR_OUT_OF_MEMORY",
    "has finished its work",
    "context owning the data",
    "nvlink",
    "T __nv_atomic_fetch_or",
    "reality",
    "attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS and all the GPUs have peer-topeer support with each other",
    "myValues",
    "handle for the specified context",
    "pointer corresponding to the mapped",
    "s shared memory configuration when launching",
    "address is\nguaranteed to not overlap any valid host pointer range and is guaranteed to have the same value across\nall contexts that support unified addressing",
    "Typically",
    "and can be\nqueried from the device via cudaDeviceGetDevResource",
    "maxTextureCubemapLayered\n\nMaximum Cubemap layered texture dimensions",
    "maxTexture3DAlt\n\nMaximum alternate 3D texture dimensions",
    "memory upfront which can no longer be used for allocations",
    "or\ncudaMemcpyDefault",
    "584\n\ncudaEventBlockingSync",
    "address",
    "114",
    "attribute\ncudaDevAttrPageableMemoryAccess",
    "cudaLimitPersistingL2CacheSize",
    "are independently instantiated on every device",
    "class 5",
    "should access the memory using only of\nthe two pointers and not both",
    "0 has the advice set",
    "cudaExecutionCtxGetId",
    "is on a multi-GPU board",
    "to create interoperability context with",
    "CUtensorMap global_tensor_map",
    "109",
    "does not support stream priorities",
    "-side cudaGraphLaunch",
    "memory\nrange",
    "-updatable nodes",
    "id that represents an invalid device",
    "address may be queried using cuMemHostGetDevicePointer",
    "of other compute capability will results in error cudaErrorUnsupportedLimit being\nreturned",
    "cuCtxSynchronize_v2",
    "maxSurfaceCubemapLayered\n\nMaximum Cubemap layered surface dimensions\n\n\nCUDA Runtime API vRelease Version",
    "to a multicast object",
    "to\nthe calling thread and all the subsequent memory allocations",
    "100",
    "memory only on a cache miss",
    "the number of CUDA-compatible devices corresponding\nto the Direct3D 10 device pD3D10Device",
    "supports unified addressing may be queried by calling\ncudaGetDeviceProperties",
    "runtime is offered as a static library",
    "for devices that support the",
    "-graph-requirements",
    "This can occur\nwhen a user specifies code generation options for a particular CUDA source file that do not include\nthe corresponding device configuration",
    "memory allocations on supported devices are within this virtual address range",
    "via\ncudaGraphKernelNodeSetParam",
    "mapping as the original\ndestination pointer",
    "kernel kernel on the requested device dev",
    "ordinal of a device against which the memory was\nallocated or registered",
    "intrinsics for the following",
    "System",
    "while executing a kernel",
    "l1 cache",
    "code\n__global__ void VecAdd",
    "and the host supports only some native atomic operations",
    "0x01\n\nDevice flag - Spin default scheduling",
    "deferredMappingCudaArraySupported\n\n1 if the device supports deferred mapping CUDA arrays and CUDA mipmapped arrays",
    "dstStart",
    "lambda and the lambda is defined with trailing return type",
    "pointer of mapped host memory allocated by cudaHostAlloc or registered by\ncudaHostRegister",
    "A higher value\nmay be returned if the kernel function allows non-portable cluster sizes",
    "variables using APIs like\ncuModuleGetGlobal",
    "code optimizer assumes that the function does not access or change any mutable state visible to caller functions",
    "attribute\nCU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN",
    "s current memory pool is its default memory pool",
    "interconnect",
    "to use for this thread",
    "Otherwise the returned pool must have been set with\ncuDeviceSetMemPool",
    "603\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "dst",
    "allow a single thread block to address the full capacity of shared memory",
    "mcHandle",
    "call may request information about another device as this API allows specification of a device ID as a parameter of the call",
    "corresponding to the current OpenGL context",
    "pD3D9Device",
    "which primary context is released",
    "it must first be uploaded to the device to populate the necessary device resources",
    "1 if all CUDA-valid",
    "bool operator",
    "with this device",
    "Instruct CUDA to block the CPU thread on a synchronization primitive",
    "of allocation",
    "integrated\n\nDevice is integrated as opposed to discrete",
    "-updatable kernel nodes also cannot have their attributes copied\nto",
    "and its primary context are functionally synonymous",
    "have a non-zero value\nfor CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS",
    "and is registered when the library is loaded into atleast one context",
    "ID and 16-bit PCI vendor ID",
    "that support it",
    "pointer is assumed to be\nunsigned char",
    "numaId",
    "concurrent-execution-between-host-and-device",
    "on which to create the green context",
    "synchronize",
    "The driver\ntracks the virtual memory ranges allocated with this function and automatically accelerates calls to\nfunctions such as cudaMemcpy",
    "to use OpenGL interoperability",
    "execution space is derived for S1",
    "handle\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "can be called to set this limit",
    "Permalink to this table",
    "architecture and the memory extent",
    "-to-multicast-objects",
    "waiterAttrList",
    "that",
    "side\nsynchronization point",
    "that represents the CPU",
    "and function are all hexadecimal\nvalues",
    "15\n\ncudaDeviceGetLimit",
    "attribute\nis not specified",
    "CreateTexture1D",
    "variable cannot have a",
    "will deinitialize the primary context for the calling thread",
    "_ORDER",
    "maintain their own local memory",
    "from which allocations on peerDev are to be directly accessed",
    "featuring the Pascal architecture onwards",
    "supports Multicast Objects",
    "cuMemsetD2D8Async",
    "including NVLink-connected devices in",
    "as a",
    "runtime is only capable of operating on the device upon which it is currently executing",
    "code path for compute capability 5",
    "to enable direct access to from the current device",
    "corresponding to a Direct3D 11 device",
    "0\ncudaSetDevice",
    "compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement",
    "are able to address each other",
    "canUseHostPointerForRegisteredMem\n\nDevice can access host registered memory at the same virtual address as the CPU",
    "pointer and dstPitch offset must be four byte aligned",
    "612\n\ncuD3D9CtxCreate",
    "resource groups",
    "Memory Access\n\nThis section describes the peer device memory access functions of the CUDA runtime application\nprogramming interface",
    "for the devices used to render the next\nframe",
    "When this\nflag is specified",
    "to a non-zero value to force the driver to always\nuse device memory for physical storage",
    "__host__  function",
    "is permitted to reorder\nremote writes internally",
    "clusterLaunch\n\nIndicates device supports cluster launch",
    "in device code",
    "will be limited by the device with the least number of\nmultiprocessors",
    "to use VDPAU interoperability",
    "CUDA_ERROR_NOT_INITIALIZED",
    "attribute\ncudaDevAttrConcurrentManagedAccess",
    "which use the unified address space",
    "name\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "pointer of the\nmemory to be prefetched and location specifies the destination location",
    "memory that are aligned to their size",
    "229\n\ncudaDeviceDisablePeerAccess",
    "title",
    "Query the devices used by the current OpenGL",
    "associated with vdpDevice",
    "the CUDA device associated with a hGpu",
    "129\n\ncudaHostGetFlags",
    "as mentioned in",
    "by multiple threads will drain all work in the first call and then have no effect for the later calls",
    "CUDA array to CUDA array",
    "void cudaGraphSetConditional",
    "declares a variable that",
    "int yyy",
    "node mask for the device",
    "ptr_out",
    "This flag can only be used on\nplatforms which support unified addressing",
    "Applications are expected to return the virtual address range back to CUDA using",
    "If no call to",
    "245\n\ncudaGraphicsD3D9RegisterResource",
    "signalerAttrList",
    "libraries can be accomplished through",
    "will be inherited from the parent",
    "int get",
    "-function-parameters",
    "memory directly",
    "110\n\ncuDeviceTotalMem",
    "that was current when the block was allocated",
    "if all pages in the memory range have the CPU as their preferred location",
    "0 in s0\ncudaSetDevice",
    "pointer to any mapped page-locked memory",
    "cudaSharedMemConfig config",
    "code without any of the",
    "supports compression of memory",
    "transfer",
    "This target will only be compatible with devices of Compute Capability 10",
    "and is equivalent to",
    "enumeration or context management",
    "has ECC support enabled",
    "corresponding to pD3D9Device",
    "the device handle of the stream\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "-debug",
    "0x02\n\nIf set",
    "code is just-in-time compiled if necessary",
    "when the",
    "function names appearing in initializers represent the address of the first instruction in\nthe function",
    "acceleration for barrier operations and integration of these barriers with the",
    "see",
    "coherent systems",
    "Any host memory\nallocated from this host thread using cudaMallocHost",
    "reservedSharedMemPerBlock\n\nShared memory reserved by CUDA driver per block in bytes",
    "A relative value indicating the",
    "having different numbers of registers per SM",
    "driver releases as illustrated in",
    "The advice above only applies to virtual\npage sizes",
    "may be retrieved with the",
    "has on the memory referenced",
    "are not supported by the device runtime",
    "for which cached memory should be freed",
    "583\n\ncudaDeviceBlockingSync",
    "-runtime-api-in-cuda-code-cdp2",
    "memory fail",
    "and returns\na cudaPitchedPtr in which ptr is a pointer to the allocated memory",
    "to use for interoperability",
    "encountered an invalid program counter",
    "and host code",
    "that can be used for CUDA",
    "device is capable of directly accessing\nmemory from peerDevice and 0 otherwise",
    "to the host",
    "gpuDirectRDMAFlushWritesOptions\n\nBitmask to be interpreted according to the cudaFlushGPUDirectRDMAWritesOptions enum",
    "lambda\n  auto lam2",
    "with compute capability 8",
    "Instruct CUDA to block the CPU thread on a synchronization",
    "pointer value may be used to refer\nto this memory through cuMemcpy",
    "cuMemAllocAsync allocates from the current\nmempool of the provided stream",
    "location",
    "to a non-zero value to force the driver to always use device memory for physical storage",
    "on which the graph resides",
    "returns true for the closure type of the extended lambda",
    "0 to be invisible and device 2 to be enumerated before device 1",
    "for the GPUs to be used by a D3D11 device in the next frame\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "runtime and calls cudaDeviceSynchronize",
    "system",
    "that has a zero value for the device attribute\ncudaDevAttrConcurrentManagedAccess",
    "visible to the application have a non-zero value for the device attribute",
    "work to any stream does not have the effect of making the stream active until",
    "CUstream hStream",
    "-side kernel launches are asynchronous with respect to the launching thread",
    "0xff\n\nDevice flags mask\n\n\nCUDA Runtime API vRelease Version",
    "enumerates the valid values for\nvarious selector components",
    "needs to be declared in one of the two forms shown below before it is used",
    "srcDevice",
    "-specific texture alignment",
    "would not\ninitialize the runtime and applications would often use the no-op runtime call",
    "cudaMemcpyDeviceToHost",
    "byte-id-a",
    "cuDeviceGetAttribute",
    "or cudaExecutionCtxGetDevResource",
    "cuMemsetD2D8",
    "only waits for prior\nwork in the stream corresponding to that GPU to complete before the kernel begins execution",
    "If the current device has been set and that device\nhas already been initialized",
    "execution",
    "function and",
    "void setValue",
    "compared to from the host side",
    "on the adapter with name pszAdapterName is CUDA-compatible then the call will fail",
    "Management\n\nThis section describes the device management functions of the low-level CUDA driver application\nprogramming interface",
    "const T d1",
    "driver than the one currently installed",
    "then cudaMemLocation",
    "when",
    "Ensures that synchronous memory operations initiated on this context",
    "work",
    "This can increase latency when waiting for the device",
    "contexts",
    "If this flag is passed",
    "variables using APIs like cuModuleGetGlobal",
    "of compute capability 12",
    "without migration",
    "corresponding to pszAdapterName",
    "int ordinal",
    "where thread blocks can cooperate and synchronize as\nthey execute",
    "is issued to stream",
    "are promoted to 32 bit float during texture fetching before any filtering is performed",
    "pointer to",
    "major\n\nMajor compute capability",
    "pointers mapped onto that object must be explicitly freed using",
    "malloc",
    "associated with a VDPAU device",
    "management",
    "for IPC functionality by\ncalling cuapiDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "cuMemHostAlloc",
    "corresponding to pD3D11Device\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "asyncEngineCount\n\nNumber of asynchronous engines",
    "handle of the stream",
    "adapter",
    "runtime system software reserves memory for various management purposes",
    "and set using CUDA runtime API",
    "cuMemCreate\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "-accelerated\natomic accesses to CPU-resident memory",
    "can be generated using",
    "space",
    "when a given stage is complete",
    "memory allocation created with cudaMalloc and\nexports it for use in another process",
    "107",
    "cuStreamWaitEvent",
    "class 11",
    "that hosts the memory allocation",
    "cudaGetDevice",
    "with compute capability 7",
    "void host_device_func",
    "for that allocType and location has never been called",
    "a device ordinal given a PCI bus ID string",
    "cudaDeviceGetLimit",
    "to use for VDPAU interoperability",
    "to load-balance the blocks in a cluster to the SMs",
    "P2P",
    "if srcDevice or dstDevice are not valid or if they represent the\nsame device",
    "environments",
    "function does not imply intra-block synchronization",
    "memory oversubscription",
    "supports page-locked host memory buffer sharing with dma_buf mechanism",
    "to get the memory requirements for",
    "int X",
    "One way to accomplish this is to move more code from the host to the device",
    "s current memory pool is its default\nmemory pool",
    "in fastest to slowest order using a simple heuristic",
    "d_B",
    "has completed all work in stream",
    "return 0",
    "to host issued to stream",
    "CreateSharedHandle when\nreferring to a ID3D12Fence object",
    "s resources",
    "it is not\nnecessary to split the copy into smaller chunks",
    "can be used with CU_DEVICE_ATTRIBUTE_COMPUTE_MODE to\ndetermine the compute mode of the device",
    "code\n  char",
    "1 accessible from devices 0",
    "lambda invokes the original lambda",
    "is non-NULL then the\nCUdevice on which this CUDA context was created will be returned in",
    "function",
    "array",
    "access supported by the add node api",
    "luid",
    "0x80\n\nDevice flag - Ensure synchronous memory operations on this context will synchronize",
    "ensures that the primary context is initialized for the requested device\nbut does not make it current to the calling thread",
    "functions called from within\nkernel execution",
    "The",
    "pointer value through which any peer",
    "runtime only support\ntextures created with the Texture Object API",
    "__managed__ int",
    "size_t __cvta_generic_to_shared",
    "dev",
    "0 has the advice\nset",
    "-enumeration",
    "or\ncontext behaves as if synchronizing all streams in the context",
    "will return CUDA_ERROR_INVALID_IMAGE",
    "void f1",
    "class 7",
    "affects memory allocation and kernel execution",
    "as current will create a new primary context for this device",
    "to access any memory owned by the host process interacting with the device",
    "of other compute capability versions will result in the error\nCUDA_ERROR_UNSUPPORTED_LIMIT being returned",
    "migrations\n  cudaFree",
    "lambdas",
    "-side update",
    "code is also limited to select combinations of host platforms",
    "int foo",
    "lambda with preserved return type",
    "Graph Requirements",
    "CreateSharedHandle\nwhen referring to a ID3D12Heap object",
    "then\nCUmemLocation",
    "-lambdas",
    "243\n\ncudaD3D9GetDirect3DDevice",
    "The number of elements is computed as",
    "cuDeviceGetP2PAtomicCapabilities",
    "Requesting more shared memory per block than the device supports will trigger this error",
    "0\nMyKernel",
    "void __nv_atomic_store_n",
    "and are selected by using an",
    "43\n\n6",
    "does not have that advice set for the entire memory range",
    "made from the device runtime failed\nbecause the call was made at grid depth greater than than either the default",
    "have completed and",
    "can be used to retrieve the CUDA device associated to the handle returned by",
    "but contains kernels which call device-side\ncudaGraphLaunch",
    "cudaStreamGetFlags",
    "than the launch stream",
    "runtime API functions to be called in arbitrarily divergent kernel code without deadlock",
    "that support the",
    "pointer to bind",
    "srcPitch and srcHeight are ignored",
    "cuDevicePrimaryCtxRelease",
    "Graph Launch",
    "if needed",
    "associated with the stream reports a non-zero value for the device\nattribute cudaDevAttrPageableMemoryAccess",
    "804",
    "also returns an error if the device does not support mapped page-locked host memory",
    "int independent_computation",
    "of Compute Capability 10",
    "is the device against which was allocated",
    "for the GPUs used by the current OpenGL context in its currently rendering\nframe",
    "launch failed due to the nodes belonging to different contexts",
    "timelineSemaphoreInteropSupported\n\nExternal timeline semaphore interop is supported on the device",
    "CreateSharedHandle when\nreferring to a ID3D12Heap object",
    "10\n\ncudaDeviceFlushGPUDirectRDMAWrites",
    "if all pages\n      in the memory range have the same GPU as their preferred location",
    "CUdevprop",
    "on which the context was created",
    "to find out exactly what the limit has been set to",
    "cuGraphicsD3D9RegisterResource",
    "where cudaDevAttrConcurrentManagedAccess is true or\nsystem-allocated pageable memory on devices where cudaDevAttrPageableMemoryAccess is true",
    "-supported amount",
    "compilation phase",
    "char x",
    "GetAdapterLuid",
    "whereas",
    "see below for details",
    "cuD3D9GetDirect3DDevice",
    "hostNumaMultinodeIpcSupported\n\n1 if the device supports HostNuma location IPC between nodes in a multi-node system",
    "int d2",
    "side kernel launch will fail and return",
    "must have a non-zero value\nfor the device attribute CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS\nfor a read-only copy to be created on that device",
    "706\n\ndstHeight",
    "streams must be created using the",
    "made from\ndevice runtime is only supported on devices of compute capability",
    "The code runs successfully on devices of compute capability 6",
    "in the system can reference the variable\ndirectly",
    "function are diagnosed as error during device compilation",
    "supports Compute Preemption",
    "it may use a device other than the first device",
    "capable of executing a very large number of threads in parallel",
    "function bodies\ntemplate",
    "cuMulticastBindMem",
    "for which support exists",
    "1 in s1",
    "execution space specifier declares a function that is",
    "functions described in",
    "If the specified priority\nis outside the numerical range returned by cudaDeviceGetStreamPriorityRange",
    "corresponding to an OpenGL device",
    "the CUDA device associated with a vdpDevice",
    "cudaMemset2DAsync",
    "to use for interoperability with a CUDA device",
    "cuDeviceGetGraphMemAttribute",
    "will return an error",
    "Wait for Host attachment to occur",
    "lambdas can be called from host code",
    "driver just-in-time compiles some",
    "-side",
    "cudaGraphAddMemAllocNode",
    "of a stream",
    "On platforms without\ncudaDevAttrPageableMemoryAccessUsesHostPageTables",
    "node mask",
    "that this green context\nwas created for",
    "if all pages in the memory range were prefetched to the same GPU",
    "function\n    auto lam2",
    "use\ncudaMemcpyPeerAsync to avoid this synchronization",
    "code and nvlink",
    "into separate\nload and store operations",
    "memory to device memory",
    "Setting this limit must be performed\nbefore any launch of a kernel that uses the device runtime and calls cudaDeviceSynchronize",
    "cudaSetDevice",
    "to Multicast Objects",
    "then this identifies the device on which the memory referred physically\n\n\nCUDA Runtime API vRelease Version",
    "pointer\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "through the same pointer value through which they are\naccessed on the host",
    "can be called to get the current per-thread stack size",
    "for the arithmetic add",
    "cudalaunchdevice",
    "code formats",
    "Module-scope",
    "void Draw",
    "themselves",
    "hostNumaId\n\nNUMA ID of the host node closest to the device or -1 when system does not support NUMA\n\n\nCUDA Runtime API vRelease Version",
    "multiProcessorCount\n\nNumber of multiprocessors on device",
    "and the rest of the peer access API may not be called\nwhen a non-primary CUcontext is current",
    "extended lambda defined in a host function with a placeholder type defined in namespace scope",
    "before it is initialized",
    "-side kernel launches can be implemented using the following two APIs accessible from PTX",
    "Launch Modes",
    "ID3D11Device",
    "has a single NULL stream used for all host threads",
    "driver to ignore any binary code embedded in an application",
    "residentDevice",
    "driverVersion",
    "144",
    "function to a",
    "addresses for accessing the allocation",
    "and\nCU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT respectively",
    "-wide setting",
    "with different minor compute capability",
    "is required",
    "CUDA_MEMCPY2D_v2",
    "runtime introduces no new concurrency guarantees within the CUDA execution model",
    "return xxx",
    "with a weakly-ordered memory model",
    "-side update calls",
    "does not have that advice set for the entire memory\nrange",
    "void __nv_atomic_sub",
    "and how to use it on device",
    "Whether or not the resulting kernel node should be device-updatable",
    "virtual",
    "are exactly analogous to the Format and NumChannels members of\nthe CUDA_ARRAY_DESCRIPTOR structure",
    "even after\nresetting the device",
    "that is now contained by the GPU",
    "pointer must be\ntwo byte aligned",
    "All or any CUDA threads in the same GPU device as the initiating thread synchronizes",
    "s\nCU_DEV_RESOURCE_TYPE_SM resource",
    "associated with a hGpu",
    "version of this function only handles device to device copies and cannot be given local or\nshared pointers",
    "will return cudaErrorMemoryAllocation",
    "This can decrease latency when waiting for the device",
    "the number of CUDA-compatible devices corresponding to\nthe current OpenGL context",
    "pointers A and B are virtual aliases of the same memory allocation",
    "void foo2",
    "-management-programming-cdp1",
    "0\nint device",
    "17\n\ncudaDeviceGetNvSciSyncAttributes",
    "lambda with preserved return type\n  static_assert",
    "pageableMemoryAccess\n\nDevice supports coherently accessing pageable memory without calling cudaHostRegister on it",
    "attribute and it does not have peer-to-peer\nsupport with the other devices that have active contexts on them",
    "as they were created on",
    "to which the stream belongs",
    "sharedMemPerBlockOptin\n\nPer device maximum shared memory per block usable by special opt in",
    "attribute cudaDevAttrManagedMemory",
    "int foo_hd",
    "A retained context should always be released once\nthe user is done using it",
    "cudaRuntimeGetVersion",
    "LUIDs",
    "_DEFAULT",
    "runtime does not support legacy module-scope",
    "is recommended to restrict CUDA\nto only use those GPUs that have peer-to-peer support",
    "int len",
    "0\n\ncudaStreamBeginCapture",
    "graphs",
    "the two instructions issued every cycle are one instruction for two different warps",
    "that have a non-zero value for the device\nattribute cudaDevAttrHostRegisterSupported",
    "with\ncudaMemLocation",
    "for which the unified address space is used",
    "is shown in the following code snippet",
    "supports exporting memory to a Win32 NT handle with cuMemExportToShareableHandle",
    "objects",
    "-side operator",
    "-implementation",
    "s primary context ahead\nof time using cudaInitDevice",
    "maxSurface1D\n\nMaximum 1D surface size",
    "signaled an error\nindicating that the data is not valid for consumption",
    "A comma-separated sequence of GPU identifiers MIG support",
    "void __nv_atomic_exchange",
    "or host function",
    "A grid is outstanding from the point of\nlaunch up until the grid is known to have been completed",
    "graphs are launched via the same handle on the host and the device",
    "cudaErrorPeerAccessAlreadyEnabled",
    "if cudaDeviceCanAccessPeer",
    "S2_t",
    "or\neither device is reset using cudaDeviceReset",
    "supports overlap of data transfer and kernel execution",
    "the CUDA-compatible device corresponding to the adapter pAdapter obtained\nfrom IDXGIFactory",
    "memory otherwise",
    "id if all pages in the memory range have that GPU as their preferred location",
    "CreateVertexBuffer",
    "and a call to\ncuFlushGPUDirectRDMA will be a no-op",
    "with compute capability greater or equal to 2",
    "which will simply overwrite the previous\nsettings",
    "Instruct CUDA to actively spin when waiting for results from the",
    "function symbol",
    "as defined in",
    "to disable direct access to\n\n\nCUDA Runtime API vRelease Version",
    "dptrs",
    "in a given stream means that the memory either has global visibility or is associated with the given stream",
    "code and host code",
    "for the GPUs used by a D3D11 device in its currently rendering frame",
    "dev in the structure pointed by the uuid",
    "This can be convenient in many cases as user doesn",
    "unit may support higher cluster sizes that",
    "code for the built-in vector types are listed in",
    "void dmain",
    "pointer CUdeviceptr is defined as an unsigned integer type whose size matches the size\nof a pointer on the target platform",
    "can access allocations from a pool",
    "attribute cudaDevAttrConcurrentManagedAccess must be non-zero",
    "deviceGraph3",
    "and start allocating\nmemory using",
    "l2CacheSize\n\nSize of L2 cache in bytes",
    "and version management sections of the reference manual can be used interchangeably",
    "cudaDeviceDisablePeerAccess",
    "_MAX",
    "device",
    "If the specified context is NULL",
    "memory pointer was returned from cuMemAlloc",
    "while exiting a kernel using tensor memory",
    "decltype",
    "must be among the devices returned\nwhen querying CU_D3D10_DEVICES_ALL from cuD3D10GetDevices",
    "pciBusID\n\nPCI bus ID of the device",
    "and returns\nin",
    "attribute cudaDevAttrConcurrentManagedAccess is zero for at least one of those\nGPUs",
    "to an integer value specified by value",
    "cudaMemcpyAsync",
    "splits a memory request with bank conflicts into as many separate conflict-free requests as necessary",
    "to Array",
    "architecture",
    "launch failed because the graph contained an unsupported operation",
    "at most\ncudaDeviceCount of the CUDA-compatible devices corresponding to the Direct3D 10 device\npD3D10Device",
    "on which to create the context",
    "to create context on",
    "attrib",
    "s high-performance",
    "that have a zero value for the device attribute\ncudaDevAttrConcurrentManagedAccess",
    "__managed__ int xxx",
    "warpSize\n\nWarp size in threads",
    "using the host pointer p",
    "276\n\ncudaD3D11GetDevice",
    "enum",
    "that was added to the multicast team via cuMulticastAddDevice",
    "and srcPitch are ignored",
    "before the wait can be satisfied",
    "runtime system software is controlled via the",
    "or 0 if not",
    "or cudaMemcpyDeviceToHost and stream is nonzero",
    "607\n\nsharingScope",
    "consteval",
    "on which a kernel is running will be controllable from that kernel",
    "Host",
    "level the multiprocessor executes threads in groups of 32 called warps",
    "with compute capability 5",
    "for flag values",
    "For further details on the implications",
    "class 9",
    "maxThreadsDim\n\nMaximum size of each dimension of a block",
    "memcpy",
    "cannot access this memory at all",
    "T __nv_atomic_fetch_and",
    "memory may be mapped and read or written by the host",
    "attribute\n\n\nCUDA Runtime API vRelease Version",
    "memory to host memory at any time by the Unified Memory driver in order to make room\nfor other allocations",
    "attribute type as follows",
    "void foo_device",
    "pointer returned by\n\n\nCUDA Runtime API vRelease Version",
    "CUDA_ERROR_NOT_SUPPORTED",
    "int relaunchCount",
    "sharedData is a\nID3D12CommandQueue handle",
    "the device of the stream",
    "memory and explicitly copy data between device and host memory",
    "This means that each parent generation at which",
    "is selected and initialized with the provided flags",
    "supports coherently accessing pageable memory without calling cudaHostRegister on it",
    "memory to a value",
    "data-transfer-between-host-and-device",
    "code optimizer can recognize that the",
    "the accessing device must be peer capable with the memory pool",
    "must have a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH",
    "in\nthe system must have a non-zero value for the device attribute cudaDevAttrConcurrentManagedAccess\notherwise the API will return an error",
    "__managed__ int x",
    "waits until all preceding commands in all streams of all host threads have completed",
    "277\n\ncudaGraphicsD3D11RegisterResource",
    "on which contexts were previously created",
    "whose output is viewed by the user",
    "is deprecated",
    "dstDevice and dstPitch specify the",
    "has completed all work in hStream",
    "pointer of the\ndestination memory and dstDevice is the destination device",
    "bool __nv_atomic_compare_exchange_n",
    "encountered an error in the call stack during kernel execution",
    "-side-launch-from-ptx-cdp2",
    "builtins provided by the compiler",
    "runtime launches will require the runtime to reserve\ndevice memory that cannot be used for user allocations",
    "supports HostNuma location IPC between nodes in a multi-node system",
    "pointer value through which ptr may be accessed by kernels running in\nthe current CUcontext",
    "cudaGraphicsUnregisterResource",
    "environment the total allocatable memory is limited to the device",
    "__shared__",
    "span",
    "must be created with",
    "call",
    "-side symbols can be referenced directly",
    "size_t __cvta_generic_to_global",
    "cudaGetDeviceProperties",
    "cooperativeLaunch\n\nDevice supports launching cooperative kernels via cudaLaunchCooperativeKernel",
    "123",
    "flag may be specified on CUDA\ncontexts for devices that do not support mapped pinned memory",
    "63\n\ncudaStreamGetFlags",
    "int foo_d",
    "runtime library",
    "they want to use support Fabric Memory",
    "fetched for each texel sample is\nspecified by",
    "int D",
    "can be cast to regular pointers and vice-versa",
    "will reinitialize the device",
    "in a given process may only be opened by one CUcontext per CUdevice per other\nprocess",
    "APIs such as",
    "P2P attributes",
    "VdpDevice vdpDevice",
    "that have that\nadvice set for that memory range",
    "in CUmemLocation",
    "and\nCU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS",
    "553\n\ncuDeviceGetP2PAtomicCapabilities",
    "maxTexture1D\n\nMaximum 1D texture size",
    "architectures must load",
    "ID3D10Device",
    "that contain these features are part of the same family and these features can also be called",
    "it can\nbe read or written with much higher bandwidth than pageable memory obtained with functions such as\nmalloc",
    "cudaExecutionContext_t",
    "void block_reduce",
    "code references",
    "for devices of compute capability 5",
    "supports Fabric Memory",
    "void",
    "until the first runtime call was made after",
    "and also when executed on the host in the case where the host does not supply the function",
    "must be among the devices returned when querying\ncudaD3D11DeviceListAll from cudaD3D11GetDevices",
    "by calling",
    "648\n\ncuD3D11GetDevice",
    "extended lambda\n      std",
    "of the Tesla and Quadro Series",
    "0x08\n\nDevice flag - Support mapped pinned allocations",
    "105\n\ncuDeviceGetMemPool",
    "can be used to determine the location of the allocation",
    "int Gvar",
    "code into an assembly form",
    "__host__\n  void swap",
    "-accelerated Split Arrive",
    "597\n\n\nCUDA Runtime API vRelease Version",
    "kernel that matches entry function entryFuncAddr",
    "0 and device 1 accessibility",
    "memory can be allocated either as",
    "execution versus adding\nthem to the stream in separate API calls",
    "srcZ",
    "class 10",
    "exceeds the\nmaximum number of blocks as allowed by cuOccupancyMaxActiveBlocksPerMultiprocessor or\ncuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags times the number of multiprocessors as\nspecified by the device attribute CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT",
    "of compute capability 2",
    "__host__\n  bool operator",
    "d_data",
    "runtime launches that can be made from this context",
    "106\n\ncuDeviceGetNvSciSyncAttributes",
    "This change means that it is now very important to check the return value of",
    "properties required to detect these levels of support\nand links to the documentation specific to each level of support",
    "Also",
    "on pAdapter is CUDA-compatible the call will return CUDA_ERROR_NO_DEVICE",
    "with best match",
    "of the vector expression holds a fragment\nfrom the corresponding matrix",
    "of compute capability",
    "unsigned int atomicAddOneRelaxed",
    "specify the base pointers of the\ndestination and source",
    "-supported synchronization using the",
    "of compute capabilities 5",
    "supports compute preemption",
    "642\n\ndevicePointer",
    "that either have peer-to-peer support with each other or have a non-zero value for\nthe device attribute cudaDevAttrConcurrentManagedAccess",
    "-management",
    "cudaDeviceGetMemPool",
    "pD3D11Device",
    "after finishing all its CUDA work",
    "property",
    "that operates as a coprocessor to the",
    "isMultiGpuBoard\n\nDevice is on a multi-GPU board",
    "flag may be specified on CUDA contexts\nfor devices that do not support mapped pinned memory",
    "memory\n    Matrix d_C",
    "void __nv_atomic_load",
    "unsigned int __isGlobal",
    "will block until all preceding requested tasks are complete",
    "using the host pointer pHost",
    "via cuMulticastBindMem",
    "or cudaMemcpyDefault",
    "may be referenced from within a kernel simply via the",
    "for the GPUs used by a D3D10 device in its currently rendering frame",
    "that support mapped page-locked host memory",
    "primary",
    "This flag can only be used on platforms which support unified addressing",
    "on which the active host thread should execute the device code",
    "int qqq",
    "devPtr",
    "texturePitchAlignment\n\nPitch alignment requirement for texture references bound to pitched memory",
    "multiGpuBoardGroupID\n\nUnique identifier for a group of devices on the same multi-GPU board",
    "graph",
    "kernel corresponding to the entry function entryFuncAddr",
    "Similar rules apply to the destination when",
    "memory and device memory is accessed via 32",
    "which has a zero value for the device attribute\ncudaDevAttrConcurrentManagedAccess",
    "fuction",
    "102\n\ncuDeviceGetDefaultMemPool",
    "see\nCU_MEM_RANGE_FLAG_DMA_BUF_MAPPING_TYPE_PCIE",
    "function parameters in terms of access and sharing",
    "a multiprocessor issues one instruction per warp over one clock cycle for four warps at a time",
    "-function-specifier",
    "void compute",
    "to access the memory or NULL if no\nsuch address exists",
    "of compute\ncapability 3",
    "it is allocated on",
    "also has a non-zero value for the device attribute\ncudaDevAttrPageableMemoryAccessUsesHostPageTables",
    "cuGraphAddMemAllocNode",
    "the current device for the calling host thread",
    "class 6",
    "cudaLimitPrintfFifoSize",
    "graphs can only be updated from the host",
    "pciDomainID\n\nPCI domain ID of the device",
    "counters that an application can increment with a single instruction by calling the",
    "symbol address\n\n\nCUDA Runtime API vRelease Version",
    "for this thread",
    "without explicitly specifying the heap size",
    "and device limits returned from",
    "-updatable\nnodes cannot be removed from their graph via cuGraphDestroyNode",
    "for the GPUs to be used by the current OpenGL context in the next frame\n\n\nCUDA Runtime API vRelease Version",
    "supports flushing of outstanding remote writes",
    "handle for the execution context",
    "cudaStreamCreateWithFlags",
    "by PCI bus ID in ascending order",
    "return a",
    "executions",
    "overwriting perviously set ones",
    "supports launching cooperative kernels via cudaLaunchCooperativeKernel",
    "supports caching locals in L1",
    "void bar",
    "memory and are cached in texture cache",
    "operates on a different pool from",
    "cudaErrorInvalidDeviceFunction",
    "code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode",
    "this is\nespecially true if the application will be performing 2D memory copies between different regions of\ndevice memory",
    "it can be\nread or written with much higher bandwidth than pageable memory obtained with functions such as\nmalloc",
    "it can be read or written with much higher bandwidth than pageable\nmemory that has not been registered",
    "before using this function",
    "from\ncuDeviceGetUuid",
    "that\nsupport unified addressing is the same as the pointer value through which that memory is accessed on\nthe host",
    "maxTexture1DLayered\n\nMaximum 1D layered texture dimensions",
    "attached to the same host",
    "126",
    "cuDeviceGetMemPool",
    "resources required to enable peer access have been exhausted\nfor one or more of the devices passed to cuCtxEnablePeerAccess",
    "memory\nwhich can no longer be used for user allocations",
    "deviceSupportsMemoryPools",
    "Returns -1 when system does not support NUMA",
    "ordinal or CU_MEM_LOCATION_TYPE_HOST and CUmemLocation",
    "and associates the created CUDA context with the calling thread",
    "supports specifying the GPUDirect RDMA flag with cuMemCreate",
    "131",
    "if the\nlast prefetch location was a GPU or CU_MEM_LOCATION_TYPE_HOST if it was the\nCPU or CU_MEM_LOCATION_TYPE_HOST_NUMA if the last prefetch location was a\nspecific host NUMA node",
    "CreateHeap",
    "or -1 when system does not support NUMA",
    "corresponding to pD3D10Device",
    "cudaStreamGetPriority",
    "users may prefer to perform it on the device",
    "platform",
    "may overlap with copies or kernels in another stream",
    "and host memory can be a win for performance",
    "required parameter",
    "is selected",
    "attribute\ncudaDevAttrConcurrentManagedAccess and all the GPUs have peer-to-peer support with each\nother",
    "-scope operation",
    "636\n\ncuD3D10GetDirect3DDevice",
    "otherwise the API will return\nan error",
    "cudaHostAlloc",
    "and it will marked as non cachecoherent and contiguous",
    "runtime are detailed here",
    "generation",
    "is permitted",
    "or cudaExecutionCtxGetDevResource for\ncudaDevResourceTypeSm",
    "base address of the source data",
    "119",
    "pointer and accessing out of bounds shared memory",
    "instruction set",
    "src",
    "Base1",
    "function parameters",
    "0 as current\nfloat",
    "runtime APIs",
    "CUDA for Tegra Memory Management",
    "specified by dev",
    "0x04\n\nDeprecated This flag was deprecated as of CUDA 4",
    "using the standard CUDA",
    "or cuDeviceUnregisterAsyncNotification",
    "API is not offered by the device runtime - properties must be queried individually",
    "415\n\ncuGraphAddBatchMemOpNode",
    "supports exporting memory to a posix file descriptor with\ncuMemExportToShareableHandle",
    "supporting CUDA\n    int deviceCount",
    "back to host\n    cudaMemcpy",
    "to host issued to\nstream",
    "attribute\ncudaDevAttrPageableMemoryAccessUsesHostPageTables",
    "flag was\nset for this device",
    "and made current to the calling host thread as detailed in",
    "memory as detailed in",
    "cudaErrorInvalidPitchValue",
    "to be used for GPU executions",
    "number to get handle for",
    "of compute capability 7",
    "memory is migrated to host memory if a new context is created on a GPU that\ndoesn",
    "maxSurface3D\n\nMaximum 3D surface dimensions",
    "cudaGraphKernelNodeSetGridDim",
    "leastPriority",
    "is an integer",
    "void mutex_unlock",
    "with full efficiency",
    "and stream is non-zero",
    "cooperative launch API is used",
    "as occurring before all writes to all memory made by the calling thread after the call to",
    "and IPC capabilities",
    "resources required to device connections have been exhausted",
    "cudaGraphDeviceNode_t\n\nCUDA device node handle for device-side node update\n\n\nCUDA Runtime API vRelease Version",
    "to access any memory owned by the host process",
    "runtime system software will try to track launch data in the fixed-size pool first",
    "-code",
    "supports HOST_NUMA location with the virtual memory management APIs like\ncuMemCreate",
    "supports HOST_NUMA location IPC between nodes in a multi-node system",
    "compute capability",
    "specifies the list of devices that must map or unmap physical\nmemory",
    "The number of\nelements is computed as",
    "supports using the cudaHostRegister flag cudaHostRegisterReadOnly to register memory\nthat must be mapped as read-only to the GPU",
    "depend on its compute capability",
    "s virtual address space using the virtual memmory management APIs",
    "texturePitchAlignment",
    "should be used instead when the application needs to know that stream-launched child kernels have completed",
    "CreateTexture2D",
    "node",
    "that have that advice set",
    "0x00\n\nDevice flag - Automatic scheduling",
    "heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the requested size or alignment",
    "before the access",
    "memory and host memory are physically the same",
    "with compute capability 10",
    "ordinal CUmemLocation",
    "supports HOST_NUMA location with the cudaMallocAsync and cudaMemPool family of\nAPIs",
    "-side Launch from PTX",
    "0 as current\nMyKernel",
    "Enumeration",
    "cudaMallocFromPoolAsync",
    "on which the runtime will initialize itself",
    "where the size of the L1 cache and shared memory are fixed",
    "-managed cache is more appropriate to exploit data locality",
    "memory to shared memory",
    "cudaErrorInvalidGraphicsContext",
    "runtime should be familiar to someone who already has experience with CUDA",
    "-updatable kernel nodes",
    "is the device pointer alias through which the memory referred to by may be",
    "ipcEventSupported\n\nDevice supports IPC Events",
    "because the memory may be mapped into other CUDA contexts via the\nCU_MEMHOSTREGISTER_PORTABLE flag",
    "accesses pageable memory via the host",
    "value is of type CUdeviceNumaConfig enum",
    "entry function",
    "flag was set\nfor this device",
    "gpuPciSubsystemID\n\nThe combined 16-bit PCI subsystem ID and 16-bit PCI subsystem vendor ID",
    "environment variable before launching the program",
    "CreateTexture3D",
    "pointer",
    "-only Graph Launch Streams",
    "may be called",
    "583\n\ncudaDeviceScheduleAuto",
    "memory and",
    "-width",
    "as described in",
    "sm",
    "146",
    "in hStream without executing it",
    "with their compute capability",
    "at a depth greater than the specified maximum synchronization depth will return an error",
    "For more\ninformation on the tensor map types",
    "memory and is cached in the constant cache",
    "memory pointers and events across processes",
    "function symbol\nand must be declared as a __global__ function",
    "to use for interoperability\n\n\nCUDA Runtime API vRelease Version",
    "attribute\nCU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH",
    "of special registers",
    "writes a value which is then read by the host",
    "size_t __cvta_generic_to_constant",
    "d_C",
    "583\n\ncudaDeviceMapHost",
    "ordinal supplied by the user does not correspond to a valid CUDA\ndevice or that the action requested is invalid for the specified device",
    "visible to the\napplication",
    "attribute cudaDevAttrMemoryPoolsSupported",
    "HGPUNV hGpu",
    "pD3DDdevice is not an IDirect3DDevice9Ex",
    "code is being compiled with older architectures that are incompatible with TMA",
    "The\ndriver tracks the virtual memory ranges allocated with this function and automatically accelerates calls\nto functions such as cuMemcpyHtoD",
    "int hdfunc",
    "will attempt to cache the whole 32KB window in the set-aside L2 cache area",
    "addr",
    "function when deemed appropriate",
    "They may only be modified from the host",
    "for the allocation",
    "supports cluster launch",
    "maxDevices",
    "sparseCudaArraySupported\n\n1 if the device supports sparse CUDA arrays and sparse CUDA mipmapped arrays",
    "srcDevice and srcPitch specify the",
    "runtime follows CUDA Stream ordering semantics",
    "is represented by a version number",
    "pointer pdptr corresponding to the mapped",
    "Accessibility for Multi-GPU Support",
    "108",
    "access to memory allocations by using",
    "pointer to the memory may be obtained by calling cuMemHostGetDevicePointer",
    "memoryBusWidth\n\nGlobal memory bus width in bits",
    "void f2",
    "that fulfill the following criteria",
    "cudaErrorLaunchMaxDepthExceeded",
    "If the cudaEventBlockingSync flag\nhas not been set",
    "function bodies\ndecltype",
    "pointer exceeded the 32-bit address range",
    "for which the primary context flags are set",
    "cuDeviceGetLuid",
    "Level",
    "if it is needed to synchronize with a child kernel before a thread block ends",
    "Graph Creation",
    "work or other callbacks that are not mandated to run earlier",
    "cudaSuccess",
    "unsigned int __isConstant",
    "return 4",
    "span class",
    "launch and it contains kernels which call\ndevice-side cudaGraphLaunch",
    "cudaDeviceProp",
    "visible to the application has a zero value for the device attribute",
    "runtime pending launch count",
    "If the specified priority is outside the numerical range returned\nby cudaDeviceGetStreamPriorityRange",
    "ordinal or cudaMemLocationTypeHost and\ncudaMemLocation",
    "pointer returned\nby cuMemHostGetDevicePointer",
    "persistingL2CacheMaxSize\n\nDevice",
    "must have been called with the cudaDeviceMapHost flag in order for the\ncudaHostAllocMapped flag to have any effect",
    "in the system",
    "Memory L2 Access Management",
    "mpsEnabled\n\nIndicates if contexts created on this device will be shared via MPS\n\n\nCUDA Runtime API vRelease Version",
    "-compatible std",
    "where these types are supported by the CUDA C",
    "after this call",
    "for low-powered\ndevices",
    "runtime program may be compiled and linked in a single step",
    "with the device attribute",
    "or cudaMemcpyDeviceToHost and the stream is non-zero",
    "0x01",
    "cuDevicePrimaryCtxGetState",
    "dev\ncan support",
    "and set using",
    "execution space specifiers",
    "whose index is present in the sequence are visible to CUDA applications and they are enumerated in the order of the sequence",
    "0\n    cudaSetDevice",
    "set using cuTexRefSetFormat",
    "pointer to query",
    "sharedMemPerMultiprocessor\n\nShared memory available per multiprocessor in bytes",
    "completes the requested task",
    "cuCtxEnablePeerAccess",
    "code when compiled in conjunction with a host compiler that supports it",
    "pointers must be freed using",
    "or\ncudaMemcpyDeviceToHost passed as kind and cudaArray type passed as source or destination",
    "cudaDeviceSynchronize",
    "attribute cudaDevAttrHostRegisterReadOnlySupported",
    "the number of CUDA-compatible devices corresponding\nto the current OpenGL context",
    "configuration settings",
    "remapping during restore",
    "CUgraphDeviceNode\n\nCUDA graph device node handle",
    "613\n\ncuD3D9GetDevice",
    "actually spent executing thread instructions",
    "updates that can be made",
    "In this case",
    "of compute capability 9",
    "pciDeviceID\n\nPCI device ID of the device",
    "barrier_arrive_tx",
    "0x02\n\nDevice flag - Yield default scheduling",
    "sizes",
    "cudaStreamDestroy",
    "address will correspond to an invalid unmapped host address",
    "that was added to the multicast team via\ncuMulticastAddDevice",
    "of the system to keep them busy most of the time",
    "function bodies\n  int",
    "-side exceptions",
    "return the currently requested heap size",
    "on the system must have non-zero concurrentManagedAccess",
    "602\n\npciDomainID",
    "sharing the same unified address space",
    "must be among the devices returned\nwhen querying CU_D3D11_DEVICES_ALL from cuD3D11GetDevices",
    "runtime launches which violate\nthis limitation fail and return cudaErrorLaunchPendingCountExceeded when cudaGetLastError",
    "322\n\ncudaGetCurrentGraphExec",
    "sets the default mempool of that device as the current\nmempool for that device",
    "if the last prefetch location was the GPU\nor cudaMemLocationTypeHost if it was the CPU or cudaMemLocationTypeHostNuma if\nthe last prefetch location was a specific host NUMA node",
    "The memory allocated cannot be used for any operations other than querying properties using",
    "encoding-a-tensor-map-on-device",
    "pointer from the IPC handle that is a valid pointer within this other process",
    "launch",
    "and creates streams with the highest and lowest available priorities",
    "code to explicitly manage the asynchronous copying of data",
    "the CUDA-compatible device corresponding to the adapter name\npszAdapterName obtained from EnumDisplayDevices",
    "587\n\ncudaInvalidDeviceId",
    "the memory allocation must\nhave been created on the device specified by dev",
    "kernel image is invalid",
    "returns true for these two devices",
    "system calls",
    "pointer returned by cuMemHostGetDevicePointer",
    "Point",
    "of compute capability less than 3",
    "void putToMemory",
    "runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources",
    "at most cudaDeviceCount\nof the the CUDA-compatible devices corresponding to the Direct3D 9 device pD3D9Device",
    "-accelerated on devices with compute capability 9 and higher",
    "the CUDA runtime offers an API for launching kernels and for tracking dependencies between launches via streams and events",
    "int std_dev",
    "class 8",
    "-updatable",
    "check with",
    "CreateSharedHandle when\nreferring to a ID3D12Resource object",
    "CUmemoryPool",
    "code using",
    "function has deduced return type",
    "and\nCU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR",
    "per array element",
    "address may be queried using cudaHostGetDevicePointer",
    "so prevent page faults as much as possible",
    "function is annotated with the",
    "of more than one compute capability",
    "virtual void Draw",
    "struct and can be queried using CUDA runtime API",
    "accelerated\ndecompression",
    "T __nv_atomic_fetch_max",
    "can map host memory into CUDA address space",
    "the flags for the first device are returned",
    "1 as current\ncudaDeviceEnablePeerAccess",
    "Device memory allocations and kernel launches are made on the currently set device",
    "void putThis",
    "in the current\nprocess",
    "function parameters was introduced\nin PTX ISA version 2",
    "via constant memory and are limited to 32",
    "and is registered when the library is loaded",
    "detected by CUDA does not support this configuration",
    "which best matches criteria",
    "using this technique",
    "dev can\nsupport",
    "209",
    "to memory on another device",
    "from streams that are not associated with it will produce undefined\nresults",
    "used in that process that support managed memory have to be peer-to-peer compatible with each other",
    "unsigned int __isLocal",
    "Query all devices used by the current OpenGL context",
    "-wide and system-wide limitations per system configuration",
    "will not match the original host pointer ptr",
    "that support Compute Data Compression",
    "will fail if the cudaDeviceMapHost flag was not specified before deferred\ncontext creation occurred",
    "wide setting",
    "__device__",
    "module",
    "int calculate_buffer_space_needed",
    "code has undefined behavior",
    "lambdas defined in host code",
    "with compute capability lower than 5",
    "-graph-stream-environment",
    "ECCEnabled\n\nDevice has ECC support enabled",
    "cuDeviceGetByPCIBusId",
    "APIs",
    "cudaGraphInstantiateFlagUpload",
    "cudaMemLocationTypeHost or\ncudaMemLocationTypeHostNuma",
    "cudaMemPoolCreate\n\n\nCUDA Runtime API vRelease Version",
    "for more information about the meaningful stream priorities\nthat can be passed",
    "in the system can consistently consume remote writes to this device",
    "as illustrated in the following code sample",
    "will fail\nwith error code cudaErrorSyncDepthExceeded if the limitation is violated",
    "and whether it is callable from the host or from the device",
    "the number of CUDA-compatible device corresponding\nto the Direct3D 10 device pD3D10Device",
    "on the system",
    "with Compute Capability 9",
    "for the devices used to render the current frame",
    "pBorderColor",
    "per process",
    "-and-constant-memory-cdp1",
    "cma",
    "memory space specifier\n  static __managed__ int m1",
    "of compute capabilities 8",
    "Function",
    "284\n\ncudaVDPAUSetVDPAUDevice",
    "associated with hGpu",
    "encountered a load or store instruction on an invalid memory address",
    "UUIDs",
    "code path for compute capability 8",
    "CUcontext",
    "void hostdevice_func",
    "-launch",
    "__global__ code\n  printf",
    "however",
    "concurrentKernels\n\nDevice can possibly execute multiple kernels concurrently",
    "size_t count",
    "has completed its execution",
    "vdpDevice",
    "context when determining where the allocation will reside",
    "_ALLOC",
    "707\n\nsrcHeight",
    "void __nv_atomic_store",
    "It can also be\nused for communication between threads but has poor performance relative to non",
    "cudaMemcpyDeviceToDevice",
    "launch and it contains kernels which call device-side\ncudaGraphLaunch",
    "launches the specified kernel with the parameter buffer that is obtained by calling",
    "memory upfront\nwhich can no longer be used for allocations",
    "cudaIpcOpenMemHandle",
    "cudaErrorDeviceUnavailable",
    "is device 0",
    "GPU-8932f937 may be a valid way to refer to the above GPU UUID",
    "to set attribute of\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "cudaD3D10SetDirect3DDevice",
    "with CUDA Managed Memory without full support",
    "entry function corresponding to the symbol symbolPtr",
    "into a contiguous VA range",
    "-and-constant-memory",
    "12\n\ncudaDeviceGetByPCIBusId",
    "VkPhysicalDevice vkPhysicalDevice",
    "after initializing the CUDA runtime by calling non-device\nmanagement operations",
    "runtime objects",
    "accelerated support for non-blocking memory transactions from global to shared memory",
    "The parameter specified by entry must be declared\nas a __global__ function",
    "86\n\nCUdeviceptr_v2",
    "CUDA_MEMSET_NODE_PARAMS_v1",
    "memory is migrated to host memory if a new\ncontext is created on a GPU that doesn",
    "returns\nan error if one of the preceding tasks has failed",
    "to ensure that\nnumber of live control variables in the callers of this function that are stored in the callee save\ncontrol registers are less than specified value",
    "230\n\n6",
    "runtime syntax and semantics are largely the same as that of the host API",
    "code from host code and then",
    "errNode_out will be set to this node",
    "version of cudaFree cannot be used with a",
    "must be used to query the device pointer",
    "synchronization between GPU work in different processes",
    "-side assert triggered during kernel execution",
    "cuCtxGetDevResource",
    "maxTexture3D\n\nMaximum 3D texture dimensions",
    "corresponding to pAdapter",
    "-side Encoding and Modification of a Tensor Map",
    "of compute capability 6",
    "supports HOST location with the virtual memory management APIs like cuMemCreate",
    "int dfunc",
    "memory can be allocated and freed using either API",
    "kernels utilize this complex feature set through",
    "CUDA_ERROR_SYSTEM_DRIVER_MISMATCH",
    "If the peer capability is not checked",
    "instead",
    "dptr",
    "modification is only supported for tiled-type tensor maps",
    "should access the memory using only one of the\ntwo pointers and not both",
    "as on the host",
    "set to",
    "will attempt\n\n  - on a best-effort basis - to group thread blocks into preferred clusters over grouping them into\nregular clusters",
    "where the L1 cache and shared memory use the same hardware\nresources",
    "should access the memory using only of the two pointers and not both",
    "supporting CUDA",
    "T __nv_atomic_fetch_xor",
    "239\n\n\nCUDA Runtime API vRelease Version",
    "resources are available to host the child",
    "is CU_COMPUTEMODE_PROHIBITED",
    "-side parameter layout at which the parameter resides",
    "id that represents the CPU",
    "of compute capability 3",
    "https",
    "that did not exist at the time the application was compiled",
    "void y",
    "code using the",
    "cudaD3D11GetDevices\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "of these compute capabilities respectively",
    "code cannot be used in host code",
    "The following code sample shows how to enumerate these devices",
    "Devices with the same major revision number share the same fundamental architecture",
    "hasn",
    "API from the host program",
    "memory between two contexts",
    "memory space specifier",
    "453\n\ncudaExecutionCtxGetDevResource",
    "handle",
    "ID in the list does not exist",
    "that have that advise set for that memory range",
    "supports CIG with D3D12",
    "extended lambda with preserved return type\n      std",
    "or non-aligned",
    "memory address returned by",
    "application workload",
    "by maximizing use of on-chip memory",
    "in order to achieve maximum interoperability performance",
    "-updatable nodes cannot be removed from their graph via cuGraphDestroyNode",
    "at most\ncudaDeviceCount of the the CUDA-compatible devices corresponding to the Direct3D 11 device\npD3D11Device",
    "of specific compute capability",
    "that contains the given Vulkan physical device must have a physical device count of 1",
    "200",
    "has completed all operations in the stream specified by hStream",
    "also has a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES",
    "143",
    "cuCtxCreate",
    "graphs cannot be launched into regular CUDA streams",
    "function parameter",
    "Devices can be added to a multicast object via cuMulticastAddDevice",
    "can be used with\nCU_DEVICE_ATTRIBUTE_COMPUTE_MODE to determine the compute mode of the device",
    "in the following sequence",
    "201",
    "s virtual address space using the virtual memory management APIs",
    "The parameter specified by func must be declared\nas a __global__ function",
    "s primary context",
    "will select",
    "symbol can either be a\nvariable that resides in global or constant memory space",
    "attribute\ncudaDevAttrMaxSharedMemoryPerBlockOptin",
    "NUMA config",
    "function return parameters using",
    "pointer to CUDA object",
    "memory\n    cudaFreeArray",
    "See\nStream Memory Operations for additional details",
    "CUcontext ctx",
    "supports sparse CUDA arrays and sparse CUDA mipmapped arrays",
    "Documentation for nvidia-smi can\nbe obtained by passing a -h option to it",
    "can issue a memory operation and then\nswitch to other execution",
    "unifiedFunctionPointers\n\nIndicates device supports unified pointers",
    "Memory Space Specifiers",
    "one kernel launch",
    "on which this kernel is invoked must have a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH",
    "259\n\ncudaD3D10GetDevice",
    "void foo1",
    "CUDA_MEM_ALLOC_NODE_PARAMS_v1",
    "by checking that the",
    "memory copies that do not specify any stream parameter",
    "Permalink to this headline",
    "to host memory",
    "memory\n    cudaMemcpy2DToArray",
    "minus the amount of shared memory required for static allocation",
    "21\n\ncudaDeviceGetStreamPriorityRange",
    "based on the specified memory pool or the supplied stream",
    "attribute cudaDevAttrMultiProcessorCount",
    "follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations",
    "for the devices used to render the next frame",
    "as the calling kernel",
    "void device_fn1",
    "to use for OpenGL interoperability",
    "supports unified function pointers",
    "that\nsupport unified addressing is the same as the pointer value through which that memory is accessed\non the host",
    "Memory Accesses",
    "from the current\ndevice has already been enabled",
    "S1_t",
    "This\nis only a hint",
    "or\ncudaMemcpyDeviceToDevice",
    "and\nCU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT\nrespectively",
    "the number of CUDA-compatible devices corresponding to the\nDirect3D 9 device pD3D9Device",
    "function being invoked",
    "double",
    "s capabilities",
    "and dstPitch specify the",
    "attribute CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT",
    "may not",
    "then CUmemLocation",
    "911",
    "function directives",
    "Management\n\nThis section describes the device management functions of the CUDA runtime application\nprogramming interface",
    "in the importing process",
    "is the base device pointer of the source memory and srcContext is the source pointer",
    "may\nnot",
    "can execute concurrently depends on its compute capability and is listed in",
    "will be returned in all the extra space provided",
    "where the L1 cache and shared memory use the same hardware resources",
    "for OpenGL set of calls to identify the CUDA device handle",
    "and the host supports native atomic operations",
    "that have a zero value for the device attribute\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "pcie",
    "will be local to that device",
    "regsPerBlock\n\n32-bit registers available per block\n\n\nCUDA Runtime API vRelease Version",
    "visible to the application have a non-zero value\nfor the device attribute",
    "initialization",
    "is not necessarily the order in which the data is observed being written by another CUDA or host thread",
    "may execute out of order or concurrently with respect to commands issued to the default stream of any other device",
    "in the current process",
    "that have a non-zero value for the device attribute",
    "-side-kernel-launch-cdp1",
    "address to write to",
    "Portable Cluster SizeA portable cluster size is guaranteed to be functional\non all compute capabilities higher than the target compute capability",
    "match those of the flags parameters in cudaSetDeviceFlags",
    "-updatable nodes also do not allow multiple instantiation",
    "may be used to retrieve the host and device\naddresses from either address",
    "do not require page faults",
    "1 in s0",
    "that is already in use by a\ndifferent thread",
    "cuCtxSetLimit\n\n\nCUDA Runtime API vRelease Version",
    "supports cooperative launches by querying the device attribute",
    "from which allocations on peerDevice are to be directly accessed",
    "The pointer",
    "deviceOrdinal",
    "s\ncudaDevResourceTypeSm resource",
    "specifies the base pointer of\nthe destination and must be naturally aligned with the CUDA array elements",
    "T __nv_atomic_fetch_sub",
    "and dstPitch are ignored",
    "memory to host memory at any time by the Unified Memory\ndriver in order to make room for other allocations",
    "minor",
    "function the\nuser should be able to learn the closest NUMA node id",
    "supportsCoopLaunch",
    "or cudaMemoryTypeManaged",
    "734\n\nsize",
    "CUdevice dev",
    "prop",
    "corresponding to a D3D9 device",
    "Always returns current device ID as would be seen from host",
    "that are part of the GPU family",
    "and should only take significant time when it\ninitializes the runtime",
    "void consumer",
    "of a variable",
    "in the stream being used for the",
    "memory is superfluous and mapped page-locked memory should be used instead",
    "where the L1 cache and shared memory use the same\nhardware resources",
    "code path for compute capability 6",
    "parameter is set to",
    "Generated automatically from",
    "CreateCommittedResource",
    "that can be obtained\nvia cudaMemGetDefaultMemPool Otherwise the returned pool must have been set with\ncudaDeviceSetMemPool",
    "and will execute independently of the parent thread",
    "memory that can be retrieved using",
    "T __nv_atomic_fetch_add",
    "29\n\ncudaDeviceUnregisterAsyncNotification",
    "return",
    "to set attribute of",
    "float",
    "API",
    "with CUmemLocation",
    "__global__",
    "to 0",
    "will be returned",
    "or host mappings",
    "-to-host migrations\n  append",
    "this will\nresult in an error",
    "corresponding to a display adapter",
    "maxBlocksPerMultiProcessor\n\nMaximum number of resident blocks per multiprocessor",
    "IDirect3DDevice9",
    "variable inside",
    "Compute Capability 5",
    "bool __nv_atomic_compare_exchange",
    "-updatable\n\n\nCUDA Runtime API vRelease Version",
    "pointer must be\nfour byte aligned",
    "from the list sequentially until it\nfinds one that works",
    "and\ndstDevice",
    "as detailed in",
    "sources with the 12",
    "memory allocation created with cuMemAlloc and\nexports it for use in another process",
    "dxgiDevice",
    "-side function is added",
    "555\n\n6",
    "class 3",
    "graph launch provides a convenient way to perform dynamic control flow from the device",
    "-side synchronization point",
    "will increase the internal\nreference count on pD3D9Device",
    "1 if cudaArray can",
    "and queried with",
    "cudaFuncCache cacheConfig",
    "189\n\ncuDeviceUnregisterAsyncNotification",
    "Graph Upload",
    "init_shared_data",
    "are returned",
    "_lambda",
    "can be used to set a device from a prioritized list of devices",
    "can be queried using the device attribute\nCU_DEVICE_ATTRIBUTE_MULTICAST_SUPPORTED\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "to CUDA_NVSCISYNC_ATTR_SIGNAL",
    "devNode which can be passed to the\nvarious device-side update functions to update the node",
    "const cudaDeviceProp",
    "on adapter pAdapter is\nCUDA-compatible",
    "For formal specification of semantics of",
    "These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving",
    "cudaD3D10GetDevice\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "and it can be coherently accessed from the host in the copy stream",
    "sharedMemPerBlock\n\nShared memory available per block in bytes",
    "runtime launches which violate this limitation fail and return\ncudaErrorLaunchPendingCountExceeded when cudaGetLastError",
    "-cdp1",
    "cudaDevice",
    "If the memory referred to by ptr cannot be accessed directly by\nthe current device then this is NULL",
    "either by",
    "Device",
    "can be used to\nchange this setting",
    "architectures",
    "deviceSupportsMultiCast",
    "any more",
    "must be added to the multicast team\nbefore a virtual address range can be mapped to the multicast object",
    "array pCudaDevices",
    "abort",
    "lambda\n  auto lam3",
    "bool func",
    "This reference count\nwill be decremented upon destruction of this context through cuCtxDestroy",
    "argument may be any of the following",
    "at most\ncudaDeviceCount of the CUDA-compatible devices corresponding to the Direct3D 11 device\npD3D11Device",
    "they want to use support them",
    "615\n\ncuD3D9GetDirect3DDevice",
    "with compute capability less than 6",
    "cudaErrorNotPermitted",
    "memory allocated using cudaMalloc or cudaMallocArray will not be\nevicted",
    "memory space specifier\n  static int i2",
    "void readXY",
    "memory as long as the current set of active contexts",
    "number for an adapter",
    "attribute\nCU_DEVICE_ATTRIBUTE_MEMORY_POOLS_SUPPORTED\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "and CUDA will report a count of 1 prior to Hopper",
    "in the system must have\na non-zero value for the device attribute cudaDevAttrConcurrentManagedAccess otherwise the API\nwill return an error",
    "cuCtxGetId",
    "otherwise the returned pool must have been set with\ncuDeviceSetMemPool or cudaDeviceSetMemPool",
    "CUDA_LAUNCH_PARAMS",
    "These GPU ordered lifetime semantics enable driver-managed memory reuse",
    "with a D3D10 device in order to achieve maximum interoperability performance",
    "This runtime-driver context\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "memory into shared memory",
    "memory and copy data between this block and the block in host memory",
    "const void",
    "runtime is a functional subset of the host runtime",
    "if dev is not valid",
    "and adds it to a graph",
    "that is\ncurrently being used",
    "Shape",
    "S3_t",
    "and device emulation code was not allowed",
    "cudaMallocAsync",
    "that should participate in a Multicast Team before memory on any device is\nbound to the Multicast Object",
    "the",
    "the Direct3D device against which this CUDA context was created in\ncuD3D9CtxCreate",
    "ordinal or if it returns",
    "code as of now",
    "or __global__ function",
    "limits such as stack size will remain as-configured",
    "and device-to-host memory\noperations",
    "to a mapped address range",
    "will attempt - on a besteffort basis - to group thread blocks into preferred clusters over grouping them into regular clusters",
    "multithreading as detailed in",
    "notice the capital CU to start the string",
    "CUDA_RESOURCE_DESC_v1",
    "cudaErrorInvalidValue cudaErrorNotSupported\n\n\nCUDA Runtime API vRelease Version",
    "without configurable shared memory",
    "in specified list",
    "187\n\ncuDeviceGetPCIBusId",
    "devNode which can be passed to the various\ndevice-side update functions to update the node",
    "281\n\ncudaD3D11SetDirect3DDevice",
    "dsts",
    "persistingL2CacheMaxSize",
    "runtime API are included automatically during program compilation",
    "-Side Kernel Launch",
    "attribute CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "for this\nlocation and allocation type or the location",
    "the Direct3D device against which this CUDA context was created in\ncudaD3D9SetDirect3DDevice",
    "and host thread access ptr concurrently",
    "flag before any other CUDA call is performed",
    "cudalaunchdevice-cdp2",
    "cudaFree",
    "function allocates at least",
    "580\n\ncudaGraphExec_t",
    "corresponding to a Direct3D 10 device",
    "driver can be put in TCC",
    "encountered an illegal instruction during kernel execution This leaves the process in an\ninconsistent state and any further CUDA work will return the same error",
    "minor\n\nMinor compute capability",
    "serve this exact same purpose",
    "The cudaDeviceMapHost\nflag is implicitly set for contexts created via the runtime API",
    "executable",
    "symbol reference",
    "for the GPUs to be used by a D3D11 device in the next frame\n\n\nCUDA Runtime API vRelease Version",
    "-class-specific device code Applicable options",
    "d has compute capability",
    "must be among the devices returned when querying\ncudaD3D9DeviceListAll from cudaD3D9GetDevices",
    "memory allocation\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "have been added to the Multicast Object it needs to be backed with\nphysical memory allocated with",
    "and all threads\nconstituting the host program itself",
    "The API",
    "pointer for mapped memory",
    "654\n\ncuD3D11GetDirect3DDevice",
    "that the GPU uses for graphics to access texture and surface memory",
    "it operates on at any time by calling",
    "If no allocations had been made from the pool",
    "can consistently consume GPUDirect RDMA writes",
    "named by dev must have been\nadded to the multicast team via cuMulticastAddDevice",
    "-side updates",
    "can access host registered memory at the same virtual address as the CPU",
    "attribute is not specified",
    "274\n\ncudaD3D10UnmapResources",
    "since the function returns before the device has completed the task",
    "transfers",
    "pointer in devPtr",
    "device attribute returns\ntrue",
    "that has a non-zero alue for the device attribute\ncudaDevAttrPageableMemoryAccess",
    "128",
    "srcs",
    "have hardware support for coherent reads",
    "If the cudaMemAttachHost flag is specified",
    "pointer should be obtained\nwith cuMemHostGetDevicePointer",
    "may be\ncalled on the host pointer",
    "Compile error if code references",
    "they came from must be the same",
    "GetNodeCount",
    "const T",
    "444\n\ncudaDeviceGetExecutionCtx",
    "flag - Use blocking synchronization",
    "attribute CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT",
    "-luids",
    "exception occurs",
    "until",
    "runtime launches that can be made from the current\ncontext",
    "only constexpr function",
    "graphs can be launched from both the host and the device via",
    "have a non-zero value for\nCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS",
    "The execution context\nshould not be NULL",
    "canAccess",
    "program counter wrapped its address space",
    "If the\nCU_EVENT_BLOCKING_SYNC flag has not been set",
    "87\n\nCUgraphExec",
    "pointer via",
    "suports HOST location with the cuMemAllocAsync and cuMemPool family of APIs",
    "-accelerated",
    "than stream",
    "on which the workqueue resources are available",
    "231\n\ncudaGLGetDevices",
    "launch but contains a node of an unsupported node type",
    "ordinal of a device on which a pointer was allocated or registered",
    "to query attribute of",
    "encountered an instruction which can only operate on memory\nlocations in certain address spaces",
    "-side storage for the written value is required",
    "at most cudaDeviceCount\nof the CUDA-compatible devices corresponding to the Direct3D 9 device pD3D9Device",
    "runtime grid launch failed because the launch would exceed\nthe limit cudaLimitDevRuntimePendingLaunchCount",
    "All the devices targeted by this multidevice launch must be identical",
    "indicates how much room for improvement there is for the kernel",
    "kernel source is invalid",
    "pointer returned by cudaHostGetDevicePointer",
    "cudaGraphKernelNodeUpdate",
    "-scope fencing remains sufficient",
    "context current at a time",
    "only function",
    "vector types defined in",
    "0\n    CUdevice cuDevice",
    "Matrix GetSubMatrix",
    "By default",
    "html",
    "differs from that of another node",
    "cuDevicePrimaryCtxSetFlags",
    "lambda in host code\n      std",
    "cudaGLGetDevices",
    "1 as current\ncudaMemcpyPeer",
    "memory requires a fence in each thread block before any thread\nin the block uses the updated tensor map",
    "0x02",
    "memory\n    float",
    "and specifying",
    "function declaration or definition",
    "cudaEventRecord",
    "and returns in",
    "coherent systems provide significant performance benefits\ncompared to software coherent systems in cases where frequent concurrent accesses\nto the same memory page are made by both CPU and GPU threads",
    "is\nspecified by",
    "Start",
    "-with-full-cuda-unified-memory-support",
    "to query for CUDA devices",
    "the behavior is to clamp to the end of the supported range",
    "on the adapter with name pszAdapterName is CUDA-compatible",
    "T1 d2",
    "void __nv_atomic_and",
    "code as for any kernel",
    "with these respective compute capabilities",
    "659\n\n6",
    "is in\nMIG mode",
    "is created by the driver and is listed under",
    "accessPolicyMaxWindowSize",
    "-wide cache configuration to cudaFuncCachePreferNone will cause subsequent kernel launches\nto prefer to not change the cache configuration unless required to launch the kernel",
    "_WAITS",
    "sequentially from a default list containing all of the\navailable CUDA devices in the system",
    "cudaMemPoolCreate",
    "supports stream priorities",
    "can be added to a Multicast Team with",
    "memory\n    CUdeviceptr d_A",
    "attribute\nCU_DEVICE_ATTRIBUTE_READ_ONLY_HOST_REGISTER_SUPPORTED",
    "to\nCUDA_NVSCISYNC_ATTR_WAIT",
    "supports the integrated stream ordered memory allocator may be queried by\ncalling cudaDeviceGetAttribute",
    "717\n\nsharingScope",
    "code",
    "corresponding to pD3D11Device",
    "is still doing work in the stream hStream when cuStreamDestroy",
    "pointer for\naccessing the allocation will be the same",
    "Only 4B types are accelerated by hardware",
    "that do support concurrent data transfers",
    "-mapped host memory are",
    "lambda is not supported in",
    "to further denote which memory space the variable belongs to",
    "specifier is assumed during device compilation",
    "pointer should be obtained",
    "Function Parameters",
    "or vice-versa",
    "as shown in the code sample below",
    "cudaMalloc",
    "pfn",
    "264\n\ncudaD3D10MapResources",
    "Any streams or events created from this host thread will be\nassociated with device",
    "The returned attribute list can be used to create a",
    "-only constexpr function from host code",
    "of compute capability 8",
    "results in an undefined behavior",
    "it will be a valid device ordinal or if it returns\n\n\nCUDA Runtime API vRelease Version",
    "to Device",
    "with the attribute",
    "dev in bytes",
    "to ensure that\nnumber of live data variables in the callers of this function that are stored in the callee save\nregisters are less than specified value",
    "with compute capability later than 7",
    "from the host and specifying",
    "s primary context for the lifetime of the green context",
    "cudaStreamSynchronize",
    "int",
    "int device",
    "for all GPUs used by a D3D9 device",
    "to file-backed host memory are not supported",
    "against which the memory was allocated or registered",
    "maxThreadsPerMultiProcessor\n\nMaximum resident threads per multiprocessor",
    "memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process",
    "on the overridden and overriding functions must match",
    "function does not exist or is not compiled for the proper device architecture",
    "then device in cudaMemLocation",
    "all green\ncontexts created on the device will wait for event as well",
    "using cudaEventDefault",
    "attribute cudaDevAttrPageableMemoryAccess",
    "maxGridSize\n\nMaximum size of each dimension of a grid",
    "system call - in such case cudaErrorInvalidValue will be returned",
    "type",
    "the call will be a no-op\nand can be safely omitted for performance",
    "in a system in one of the three following modes using NVIDIA",
    "memory allocation and deallocation as well as data transfer between host and device memory",
    "unsigned int __isShared",
    "pointer for the requested library",
    "to get the execution context for",
    "acceleration",
    "streaming multiprocessor",
    "is\nspecified by CUmulticastObjectProp",
    "void foo3",
    "-side update calls cannot be updated to a",
    "636\n\ndstPos",
    "a device handle given an ordinal in the range",
    "totalConstMem\n\nConstant memory available on device in bytes",
    "will not be included",
    "the handle of the specified execution context",
    "and function are all hexadecimal values",
    "this function returns the clamped priority",
    "size_t",
    "lambda cannot first-capture",
    "with compute capability below 8",
    "and\nCU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR_V2",
    "supports the stream ordered memory allocator by calling",
    "maxThreadsPerBlock\n\nMaximum number of threads per block",
    "in use supports Compute Preemption",
    "cudaLaunchCooperativeKernel",
    "maxTexture2D\n\nMaximum 2D texture dimensions",
    "124",
    "across all processes in the system",
    "comp",
    "graph so that it can be relaunched",
    "0x07\n\nDevice schedule flags mask",
    "work to any stream does not have the effect of making the stream active until all",
    "it is no longer possible to access the modifications made by the threads in the child grid from the parent grid",
    "112\n\ncuDeviceGetProperties",
    "sType",
    "function in host code",
    "cudaDeviceGetPCIBusId\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "-memory-l2-access-management",
    "may be registered and\nmapped through the lifetime of this CUDA context",
    "122",
    "13\n\ncudaDeviceGetDefaultMemPool",
    "for cudaMemLocation",
    "int stream_compaction",
    "ids that had cudaMemAdviceSetAccessedBy set for that entire memory range",
    "0x1",
    "that corresponds to the Vulkan physical device on which the objects were created can be determined by comparing the UUID of a CUDA device with that of the Vulkan physical device",
    "function\ninput and return parameters may have their address taken via",
    "dstZ",
    "memory to host memory",
    "runtime only support\nsurfaces created with the Surface Object API",
    "code is deprecated in CUDA 11",
    "93\n\nCU_MEMHOSTREGISTER_IOMEMORY",
    "cudaSetDoubleForHost\n\n\nCUDA Runtime API vRelease Version",
    "memory pointer using",
    "memory\n    cudaMemcpy",
    "ordinal and the device must have a non-zero value for the device attribute\nCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS",
    "launch but a node",
    "code class",
    "explicitly",
    "31\n\ncudaGetDeviceCount",
    "115",
    "cudaGetLastError",
    "attribute\nand it does not have peer-to-peer support with the other devices that have active contexts on them",
    "can support a system-wide maximum of eight peer connections",
    "cuCtxGetLimit",
    "pointer should be obtained with\ncuMemHostGetDevicePointer",
    "memory copying parameters",
    "and srcDevice are the base pointers\nof the destination and source",
    "cudaDeviceGetDefaultMemPool",
    "from which to remove callback",
    "may support",
    "that does not have this attribute set will cause\ncuMemHostRegister to error with CUDA_ERROR_NOT_SUPPORTED",
    "pointer to buffer",
    "graphs from the GPU via traditional methods such as",
    "maxSurfaceCubemap\n\nMaximum Cubemap surface dimensions",
    "attribute to query",
    "upon executable graph update in order for the changes to take effect",
    "and\nCU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT respectively",
    "with the same major compute capability are in the same family",
    "visible to the application",
    "598\n\nmajor",
    "function symbol and must be declared as a __global__ function",
    "and then used to access the block from within a kernel",
    "to host\nis issued to stream",
    "symbol address",
    "has never been called",
    "or\n   context behaves as if synchronizing all streams in the context",
    "memory\n    Matrix d_A",
    "-to-host memory copies completes",
    "127",
    "function bodies\nstruct S1_derived_t",
    "graph from the device while an update is being applied will result in undefined behavior",
    "globalL1CacheSupported\n\nDevice supports caching globals in L1",
    "cuMemHostRegister",
    "and the host supports all native atomic operations",
    "pointer is returned",
    "are not peer capable",
    "-selection",
    "which configures the graph for launch from the",
    "1 as current\ncudaStream_t s1",
    "void baz",
    "QueryInterface",
    "with the specified flags",
    "0x01\n\nTell the CUDA runtime that DeviceFlags is being set in cudaInitDevice call",
    "corresponding to pD3D11Device\n\n\nCUDA Runtime API vRelease Version",
    "is still doing work in the stream stream when cudaStreamDestroy",
    "or context handle when any associated stream is in capture mode",
    "246\n\ncuMemHostGetFlags",
    "is set in flags",
    "runtime APIs have identical syntax",
    "cudaDeviceGetSharedMemConfig",
    "535\n\ncudaDevResourceType",
    "will be a part of the multicast team of size\nspecified by CUmulticastObjectProp",
    "that will be associated to the multicast object",
    "of higher compute capabilities",
    "The accessibility of the default memory pool may be modified with",
    "must be added to the multicast object before\nmemory can be bound to it",
    "and software resources are implementation-dependent and limit the scale",
    "acceleration might be used to select the thread when called with",
    "Lower value means better performance",
    "heap is in addition to memory allocated through host-side CUDA API calls such as",
    "should not be part of a device group that contains more than one Vulkan physical device",
    "cudaDevSmResourceSplit",
    "may switch to launch the regular clusters instead\nto attempt to utilize as much of the physical device resources as possible",
    "to which a stream belongs",
    "or MIG configurations which are too small to support 8 multiprocessors in which case the maximum cluster size will be reduced",
    "contains its NUMA ID",
    "or\ncuDeviceGetTexture1DLinearMaxWidth",
    "return 1",
    "44\n\ncudaDeviceSetSharedMemConfig",
    "cudaStreamGetDevResource",
    "32\n\ncudaGetDeviceProperties",
    "in the Runtime",
    "device attribute returns true",
    "unsigned int count",
    "returns true",
    "dstY",
    "to interoperate with",
    "void __nv_atomic_or",
    "can consistently consume remote writes",
    "l2 cache",
    "memory and pageable host memory",
    "-system",
    "pointer through which to access a mapped graphics resource",
    "on the system must have\nnon-zero concurrentManagedAccess",
    "maxTexture2DGather\n\nMaximum 2D texture dimensions if texture gather operations have to be performed",
    "pointers mapped onto the memory object via",
    "maxSurface1DLayered\n\nMaximum 1D layered surface dimensions",
    "cudaGraphKernelNodeSetParam",
    "126\n\ncuCtxGetDevice_v2",
    "However",
    "Wait for",
    "memory space specifier\n  static S1_t i4",
    "support shared capacities of 0",
    "maxTexture2DLinear\n\nMaximum dimensions",
    "0 as the resident device\nparams",
    "to get resource for",
    "void doit",
    "emulation\nof kernel launches",
    "at time of instantiation",
    "which is also allowed",
    "via",
    "cudaChooseDevice",
    "may be enabled",
    "property cudaDeviceProp",
    "CUdeviceptr",
    "121",
    "the copy",
    "cuMemCreate",
    "kernel",
    "will now explicitly initialize the runtime after changing the current device for the host thread",
    "the block needs to be allocated by passing the flag",
    "surfaceAlignment\n\nAlignment requirements for surfaces",
    "when the dependencies of the conditional node have been met",
    "and\nis registered when the library is loaded",
    "values from different contexts reference different memory locations",
    "return 10",
    "cudaDeviceCount",
    "bool condition_check",
    "supports CIG with Vulkan",
    "and all threads constituting the host program itself",
    "has already been initialized then this call will fail with the error\ncudaErrorSetOnActiveProcess",
    "ids that had CU_MEM_ADVISE_SET_ACCESSED_BY\nset for that entire memory range",
    "associated with the\ncontext where stream was created",
    "pD3DDevice",
    "supports using the cuMemAllocAsync and cuMemPool family of APIs",
    "Returns the execution context that corresponds to the primary",
    "that is free according to the OS",
    "where the L1 cache and shared",
    "void __nv_atomic_max",
    "will be returned if a device that supports managed memory is used and it is not peer-to-peer compatible with any of the other managed memory supporting devices that were previously used in that process",
    "which configures the graph",
    "25\n\ncudaDeviceSetCacheConfig",
    "135",
    "will be returned as can fit in the array",
    "in the",
    "localL1CacheSupported\n\nDevice supports caching locals in L1",
    "lambda to be less optimized by the host compiler than lambdas that are implicitly or explicitly",
    "memory budget",
    "for more device limitations",
    "cudaLimit limit",
    "and CUDA modules - the analogue of dynamically loaded libraries for the device",
    "that have a non-zero value for the device attribute\ncudaDevAttrCanUseHostPointerForRegisteredMem",
    "for the GPUs used by a D3D9 device in its currently rendering frame",
    "lambda or an extended",
    "deviceGraph2",
    "that already has active\nCUDA work being performed",
    "Documentation for nvidia-smi can be obtained by passing a -h option to it",
    "cudaMemPool_t memPool",
    "cudaMemPoolSetAttribute",
    "function to have the given shared memory bank size configuration",
    "cudaFlushGPUDirectRDMAWritesTarget target",
    "cudaMemset3DAsync",
    "112",
    "code with 12 bytes after field",
    "521\n\ncudaDevResource",
    "supporting managed memory",
    "-side symbols",
    "-sensitivity",
    "handle for the current context",
    "allocation",
    "associated with the stream reports a non-zero value for the device\nattribute CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS",
    "is trying to disable peer addressing which\nhas not been enabled yet via cudaDeviceEnablePeerAccess",
    "with CU_DEVICE_ATTRIBUTE_VULKAN_CIG_SUPPORTED",
    "counter of index",
    "error",
    "function not shown in this code snippet",
    "value is of type cudaDeviceNumaConfig enum",
    "on which the active host thread executes the device code",
    "ordinal bit mask\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "with fixed shared memory bank size",
    "0x04\n\nDevice flag - Use blocking synchronization",
    "cuVDPAUGetDevice",
    "program as a texture",
    "and destroyed without ever being mapped by the host or copied to host memory",
    "will match the original pointer ptr",
    "peerDev",
    "matrix or ensure that only supported hardware is visible\nduring initialization via the CUDA_VISIBLE_DEVICES environment variable",
    "code or user error",
    "Query GPU properties\nsize_t size",
    "graph being currently executed",
    "T __nv_atomic_load_n",
    "contains device memory from that device",
    "that support Compute Data\nCompression",
    "void foo",
    "counters are collected via a profiler",
    "in the system can reference the variable\n    directly",
    "are often\nbusy",
    "must\nmatch the device associated with the stream",
    "deviceSupportsVmm",
    "passed as kind and cudaArray type passed as source or destination",
    "canMapHostMemory\n\nDevice can map host memory with cudaHostAlloc",
    "has to match across all devices",
    "must have a non-zero value for the device\nattribute cudaDevAttrPageableMemoryAccess for a read-only copy to be created on that device",
    "pointer that may be\nused to access this host memory from those contexts is always equal to the returned host pointer",
    "140",
    "113",
    "for all GPUs used by a D3D11 device",
    "pointer to bind\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "and srcPitch specify the",
    "memory copies",
    "operations are asynchronous with respect to the host",
    "than\nhStream",
    "function\n    auto lam3",
    "variable or a",
    "the handle of the specified context",
    "118",
    "resources",
    "attribute CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING",
    "like FPGAs",
    "-only Launch Implementation Functions",
    "is the\nsame pointer value through which that memory may be accessed from the peer device",
    "graph id",
    "101",
    "attribute and does not\nsupport peer-to-peer with at least one of the other devices that has an active context",
    "api before the host-side launch of a kernel using the device runtime",
    "lambda\n   kernel",
    "supports Compute Data Compression by using",
    "memory heap has a fixed size that must be specified before any program using",
    "function will block the thread until all direct grid dependencies have completed",
    "it can be read or written with much higher bandwidth than pageable memory that\nhas not been registered",
    "function with the specified flags",
    "memory",
    "depending on whether\nthe last location for prefetch was a GPU or the CPU respectively",
    "is unavailable at the current time",
    "-side-launch-from-ptx-cdp1",
    "entry function that matches entry function symbolPtr",
    "Unless the flag\nCU_MEMHOSTALLOC_WRITECOMBINED is specified",
    "supports atomic reduction operations in stream batch memory operations",
    "there is no mechanism for detecting that such integer operation exceptions have occurred",
    "memory are typically done using",
    "void operator delete",
    "cudaDeviceGetP2PAttribute",
    "0 and self-initialize\nas needed to process other runtime API requests",
    "attribute and does not support peer-to-peer with at\nleast one of the other devices that has an active context",
    "must also match the\ndevice associated with the tile pool memory allocation as specified by CUarrayMapInfo",
    "function call cannot be\npredicated",
    "using cudaMalloc",
    "and the\nnumber of components per array element",
    "an explicit call to\ncuStreamAttachMemAsync will be required to enable access on such devices",
    "migrations\n  free",
    "of compute capability 10",
    "sets persistent function state that is the same\nas function state set through cuLaunchKernel API when called individually for each element in\nlaunchParamsList",
    "used in that process that support managed memory have to be peer-topeer compatible with each other",
    "have additional restrictions which do not apply to host\ngraphs",
    "Launch",
    "shares a unified address space with the host",
    "luid\n\n8-byte locally unique identifier",
    "kernel took too long to execute",
    "if the entire\n  memory range has the corresponding processor as preferred location",
    "systems",
    "ID of the device",
    "can possibly copy memory and execute a kernel concurrently",
    "23\n\nCUdevice_P2PAttribute",
    "execution space specifiers cannot be used together",
    "to create interoperability context with\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "__managed__ const int yyy",
    "This is the device",
    "cuDevice",
    "and Constant Memory",
    "108\n\ncuDeviceGetUuid",
    "via mapped pinned memory or host",
    "may combine and merge volatile",
    "-to-primary context mapping for the\nprocess provided by the CUDA Runtime API",
    "to guarantee that the appropriate asynchronous work is complete and that the GPU will not try to access the allocation",
    "-to-host memory copy into each stream",
    "int result",
    "will be\nreturned",
    "Instruct CUDA to not reduce local memory after resizing local",
    "function where thread blocks can cooperate and synchronize as they execute",
    "corresponding to a Direct3D 9 device",
    "Runtime",
    "in a given process may only be opened by one context per device per other process",
    "in the multicast team that will bind memory to this object",
    "support for 24-bit multiply",
    "function ensures the programmatic launch completion edges",
    "if all pages in\nthe memory range have the same GPU as their preferred location",
    "from device is\npossible",
    "implementation and may change in the future",
    "Runtime refers to the runtime system and APIs available to enable Kernel Functions to use Dynamic Parallelism",
    "T __nv_atomic_exchange_n",
    "coherent systems such as NVIDIA Grace Hopper offer a logically combined\npage table for both CPUs and GPUs",
    "the kernels must either have\nbeen compiled with toolchain version 3",
    "for the GPUs to be used by a D3D9 device in the next frame\n\n\nCUDA Runtime API vRelease Version",
    "to allow matching device with\ngraphics APIs",
    "by checking the attribute",
    "has been called on those devices",
    "auto fn1",
    "associated with a VdpDevice",
    "that support them",
    "CUdevWorkqueueConfigResource",
    "129",
    "-without-full-cuda-unified-memory-support",
    "totalGlobalMem\n\nGlobal memory available on device in bytes",
    "has not been\ninitialized",
    "and\nthe allocation type is CU_MEM_ALLOCATION_TYPE_PINNED",
    "If no global for the requested name name exists",
    "the previous block shape",
    "supporting\nunified addressing",
    "code\n__global__ void\n__launch_bounds__",
    "630\n\ncuD3D10GetDevice",
    "shared memory",
    "void func",
    "overload\n    return true",
    "attribute value",
    "125",
    "-memory-specifiers",
    "cudaErrorJitCompilerNotFound",
    "on which this kernel is invoked must have a non-zero value for the device attribute\ncudaDevAttrCooperativeLaunch",
    "724\n\nlaunchCompletionEvent",
    "code and all intrinsic functions that are only supported in device code",
    "work follows the callback",
    "pointer can be mapped onto an imported memory object as shown below",
    "graph cannot be launched twice from the device at the same time",
    "that has a non-zero value for the device attribute concurrentManagedAccess",
    "as the Direct3D 9 device to use for Direct3D 9 interoperability with the\nCUDA device device and sets device as the current device for the calling host thread",
    "may switch to launch the\nregular clusters instead to attempt to utilize as much of the physical device resources as possible",
    "void x",
    "0x10\n\nDevice flag - Keep local memory allocation after launch",
    "experimental",
    "-side global\ndata that does not belong to any context",
    "graphs can be launched from both the host and device",
    "memory then",
    "139",
    "float devData",
    "-uuids",
    "s progress",
    "thread that configures and launches a new grid belongs to the parent grid",
    "hostNativeAtomicSupported\n\nLink between the device and the host supports native atomic operations",
    "events using Boolean\nfunctions to increment one of the four performance counters",
    "pointer to memory to free",
    "tasks",
    "issued to stream",
    "432\n\ncudaLibraryEnumerateKernels",
    "function bodies",
    "Concurrent Kernel Execution",
    "has\nalready been initialized",
    "memory in one context to device memory in another context",
    "function hfunc",
    "supports Virtual Memory Management",
    "even though it is not\naccepted by cudaSetDeviceFlags because it is implicit in runtime API flags",
    "to\ncudaNvSciSyncAttrSignal",
    "to get identifier string for",
    "of compute capabilities lower than 6",
    "-side-kernel-launch",
    "will have the same device number as seen from the host system",
    "uuid",
    "-graph-update",
    "function described in",
    "group as returned by",
    "These objects appear similar to those presented in the C",
    "in bytes",
    "cuCtxDisablePeerAccess",
    "to the currently active CUDA device\ncontext",
    "for which primary context is destroyed",
    "side synchronization\npoint between those launches",
    "device attribute\nis deprecated",
    "It is implicit for the runtime but may be absent if a context is created using the driver API",
    "specified through the prop\nstructure",
    "function\nmov",
    "runtime offers a single implicit",
    "int value",
    "during cuMulticastCreate",
    "lambda\n    auto lam1",
    "lambda cannot capture",
    "Management",
    "memory oversubscription is possible for GPUs that have a non-zero value for the device\nattribute cudaDevAttrConcurrentManagedAccess",
    "with each other",
    "-to-device",
    "cudaDeviceGetGraphMemAttribute",
    "and its CUDA Driver API counterpart are supported on\nthe device",
    "and an internal type of float",
    "Instruct CUDA to yield its thread when waiting for results from the",
    "deviceCount",
    "function or with a functor whose",
    "size_t __cvta_generic_to_local",
    "cudaMemcpy2DAsync",
    "Devices can be added to the multicast object\nvia cuMulticastAddDevice",
    "users can execute the following command",
    "emulation mode was removed with the\nCUDA 3",
    "printf",
    "-management-programming",
    "global memory",
    "Applications may query this capability by checking the",
    "T d1",
    "must have a non-zero value for",
    "in the same family",
    "without full CUDA Unified Memory support",
    "Unless a mempool is specified in the\ncudaMallocAsync call",
    "source code cannot contain a reference to V or take the address of V",
    "GetAdapter",
    "The parameter specified by entry must be\ndeclared as a __global__ function",
    "will return",
    "for IPC functionality by\ncalling cudaDeviceGetAttribute with cudaDevAttrIpcEventSupport\n\n\nSee also",
    "345\n\ncuStreamGetFlags",
    "need to be added to the Multicast Team before binding memory on any device",
    "CUDA_ERROR_INVALID_CONTEXT",
    "0 at address p0\nMyKernel",
    "code because it is",
    "void writeXY",
    "can coherently access managed memory concurrently with the CPU",
    "code linking",
    "If there is a current device for the calling thread",
    "for IPC functionality by\ncalling cudaDeviceGetAttribute with cudaDevAttrIpcEventSupport\n\n\n\n\n\nSee also",
    "supports launching cooperative kernels via cuLaunchCooperativeKernel",
    "as nodes which contained such\ncalls at instantiate-time",
    "-move-forward",
    "variables",
    "will always return a failure code",
    "memoryPoolSupportedHandleTypes\n\nBitmask of handle types supported with mempool-based IPC",
    "must be called to set the cudaLimitDevRuntimePendingLaunchCount to be\nhigher than the upper bound of outstanding launches that can be issued to the device runtime",
    "cuMemPoolCreate",
    "or the driver",
    "properties",
    "cudaErrorUnknown",
    "attribute CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT for\nmore information",
    "can perform an asynchronous memory copy to or from the GPU concurrently with kernel execution",
    "maxTexture2DMipmap\n\nMaximum 2D mipmapped texture dimensions",
    "cudaIpcOpenMemHandle can attempt to enable\npeer access between the devices as if the user called cudaDeviceEnablePeerAccess",
    "otherwise cudaErrorInvalidHandle will be returned",
    "1 as the resident device\nparams",
    "function parameter address",
    "cuStreamGetFlags",
    "-updatable\nnodes and updates those nodes from the device from within the graph",
    "encountered an illegal instruction",
    "of Compute Capability 9",
    "tensor core",
    "management functions of the low-level CUDA driver application\nprogramming interface",
    "which was current when the memory was allocated or registered",
    "page-locked host memory can be mapped into the address space of the device",
    "side synchronization point",
    "supports",
    "pointer of the\nsource memory and srcDevice is the source device",
    "Any other value results in an undefined behavior",
    "memory\n    cudaChannelFormatDesc channelDesc",
    "cudaDeviceScheduleAuto uses a heuristic based on the power\n\n\nCUDA Runtime API vRelease Version",
    "cudaErrorSharedObjectSymbolNotFound",
    "emulation mode was removed with the CUDA\n3",
    "Struct Reference\n\nCUDA device properties",
    "that for which the multicast memory binding will be applicable",
    "even between parent and child launches",
    "number to get properties for",
    "This may often be the starting point for further\npartitioning or configuring of resources",
    "code is not able to call",
    "102",
    "features and can be queried with cuDeviceGetAttribute",
    "cuCtxDestroy",
    "context",
    "19\n\ncudaDeviceGetP2PAttribute",
    "under an SLI group may not be supported",
    "the device which has properties that best match",
    "Sensitivity",
    "has completed all preceding requested tasks",
    "wide setting set by cudaDeviceSetSharedMemConfig",
    "this\nis especially true if the application will be performing memory copies involving 2D or 3D objects",
    "code and data",
    "will attempt - on a best-effort basis to group thread blocks into preferred clusters over grouping them into regular clusters",
    "and will not block the calling CPU thread",
    "will be referred to as host graphs",
    "at most cudaDeviceCount\nof the CUDA-compatible devices corresponding to the current OpenGL context",
    "memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device",
    "or by using any other synchronization mechanisms described in",
    "This is\nonly a hint",
    "cudaFuncSetCacheConfig",
    "memory space specifier declares a variable that resides on the device",
    "that support unified addressing",
    "or resets it with cuDevicePrimaryCtxReset",
    "is destroyed or encounters an error",
    "only",
    "-memory-accesses",
    "attributes",
    "with compute capability greater than or equal to 2",
    "and it is associated with the closest host node",
    "for which primary context is requested",
    "cudaError_t cudaGetParameterBuffer",
    "Model",
    "corresponding to pD3D9Device\n\n\nCUDA Runtime API vRelease Version",
    "corresponding to a D3D10 device\n\n\nCUDA Driver API TRM-06703-001 _vRelease Version",
    "cuDeviceGetName",
    "hostRegisterReadOnlySupported\n\nDevice supports using the cudaHostRegister flag cudaHostRegisterReadOnly to register memory that\nmust be mapped as read-only to the GPU",
    "maxTexture2DLinear",
    "-specific texture pitch alignment",
    "-side work scheduler",
    "of compute capability lower than 6",
    "CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE",
    "that represents an invalid device",
    "_RetType operator",
    "Otherwise the returned pool must have been set with cuDeviceSetMemPool",
    "is using TCC driver model",
    "-launch-modes",
    "it will be a valid device\nordinal or if it returns CU_MEM_LOCATION_TYPE_HOST_NUMA",
    "for all GPUs used by a D3D10 device",
    "memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in",
    "compilation",
    "Selection",
    "Runtime API in CUDA Code",
    "code these types are always aligned on a two-word boundary",
    "maxSurface2D\n\nMaximum 2D surface dimensions",
    "584\n\ncudaDeviceScheduleYield",
    "CUDA_MEMSET_NODE_PARAMS_v2",
    "The device pointer that may be used to\naccess this host memory from those contexts is always equal to the returned host pointer",
    "cudaErrorInvalidValue",
    "threads",
    "will not be\nincluded",
    "with configurable shared memory banks",
    "-wide atomic APIs",
    "compressionSupported",
    "cuDeviceGetExecAffinitySupport",
    "of the CUDA\narray bound to the texture reference hTexRef"
  ],
  "SoftwareComponent": [
    "warp",
    "thread block",
    "context",
    "queue",
    "task",
    "kernel",
    "event",
    "graph",
    "grid",
    "stream"
  ],
  "Function": [
    "cudaLogsCallback_t",
    "cudaJitPositionIndependentCode",
    "cudaGraphExecMemcpyNodeSetParams1D",
    "cudaErrorLaunchPendingCountExceeded",
    "cudaGraphicsCubeFaceNegativeY",
    "cudaDevAttrMaxTexture2DLayeredHeight",
    "cudaErrorInvalidClusterSize",
    "cudaEnablePeerAccess",
    "__isLocal",
    "cudaEglColorFormatYVU420SemiPlanar",
    "cudaWaitExternalSemaphoresAsync",
    "cudaExternalMemoryHandleType",
    "__NV_ATOMIC_SEQ_CST",
    "__ldcv",
    "_sub",
    "_cvta",
    "cudaEglColorFormatY10V10U10_422SemiPlanar",
    "__shfl_up_sync",
    "cudaGraphDebugDotFlagsEventNodeParams",
    "cudaEglColorFormatL",
    "_update",
    "cudaDevAttrReserved93",
    "cudaExecutionContext_st",
    "cudaThreadSetCacheConfig",
    "cudaAddressModeMirror",
    "cudaAddressModeBorder",
    "cudaErrorLossyQuery",
    "cudaGraphicsUnregisterResource",
    "__shared__",
    "__nv_fp128_frexp",
    "cudaDeviceGetCacheConfig",
    "__cvta_global_to_generic",
    "cudaArray_t",
    "cudaGraphHostNodeSetParams",
    "cudaEventRecordWithFlags",
    "cudaEglResourceLocationFlags",
    "_SCALE",
    "cudaGetDeviceProperties",
    "_INT32",
    "cudaD3D10ResourceGetMappedPitch",
    "cudaMemcpySrcAccessOrderAny",
    "cudaStreamCaptureStatus",
    "cudaEventDestroy",
    "cudaLaunchAttributeProgrammaticEvent",
    "__noinline__",
    "cudaMemPoolReuseAllowOpportunistic",
    "_block",
    "_global__",
    "_005f_005fatomic",
    "cudaMemPoolAttrReservedMemHigh",
    "cudaMemLocationTypeHostNumaCurrent",
    "cudaMallocHost",
    "_16a",
    "cudaGraphicsResourceSetMapFlags",
    "_120f",
    "cudaMipmappedArrayGetMemoryRequirements",
    "_12x",
    "cudaArraySparse",
    "cudaErrorMpsServerNotReady",
    "cudaPreferBinary",
    "cudaFunction_t",
    "cudaGPUDirectRDMAWritesOrderingNone",
    "cudaEGLStreamProducerPresentFrame",
    "cudaMemRangeGetAttributes",
    "__CUDA_ARCH_FAMILY_SPECIFIC__",
    "cudaExternalMemoryHandleTypeD3D11Resource",
    "cudaEglColorFormatBayerBCCR",
    "cudaDeviceSetLimit",
    "cudaGraphExecChildGraphNodeSetParams",
    "cudaDeviceGetAttribute",
    "__reduce_add_sync",
    "_rd",
    "cudaLaunchAttributeID",
    "cudaExecutionContext_t",
    "_PTX",
    "cudaGraphDebugDotFlagsVerbose",
    "cudaStreamCaptureStatusActive",
    "cudaGraphInstantiateWithParams",
    "_DROP",
    "cudaErrorStreamCaptureUnsupported",
    "__host__make_cudaPos",
    "cudaErrorInvalidConfiguration",
    "__shfl",
    "__pipeline_commit()",
    "cudaEglStreamConnection",
    "cudaLogLevel",
    "cudaDestroySurfaceObject",
    "cudaGraphMemcpyNodeGetParams",
    "cudaEglColorFormatYUV422Planar_ER",
    "cudaD3D11GetDevice",
    "cudaErrorMemoryAllocation",
    "cudaDevAttrGPUDirectRDMASupported",
    "_PATH",
    "cudaTextureFilterMode",
    "cudaMemcpy3DParms",
    "_clusters",
    "_inline",
    "__vibmin_s32",
    "cudaD3D9RegisterResource",
    "cudaD3D10RegisterFlagsNone",
    "cudaExecutionCtxGetDevResource",
    "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags",
    "cudaGraphExecHostNodeSetParams",
    "__func__",
    "cudaGraphMemAttrUsedMemHigh",
    "cudaTextureReadMode",
    "cudaStreamNonBlocking",
    "cudaErrorArrayIsMapped",
    "__builtin_assume_aligned",
    "cudaGraphKernelNodeSetEnabled",
    "cudaErrorHostMemoryAlreadyRegistered",
    "__vimin3_s32",
    "_constant",
    "cudaGraphExecUpdateErrorParametersChanged",
    "__viaddmax_s32_relu",
    "cudaMemsetParamsV2",
    "cudaGetFuncBySymbol",
    "cudaDeviceGetMemPool",
    "cudaFlushGPUDirectRDMAWritesTarget",
    "cudaEventWaitNodeParams",
    "cudaGraphAddMemcpyNode1D",
    "cudaMemset2D",
    "cudaDevSmResourceGroupParams",
    "cudaStreamCaptureMode",
    "__dadd_rn()",
    "cudaOccupancyAvailableDynamicSMemPerBlock",
    "cudaErrorInvalidMemcpyDirection",
    "cudaDevAttrMaxTexture3DHeightAlt",
    "cudaGraphExecExternalSemaphoresSignalNodeSetParams",
    "cudaEGLStreamProducerDisconnect",
    "_rz",
    "cudaExternalSemaphoreHandleTypeKeyedMutex",
    "cudaD3D11DeviceList",
    "cudaGetParameterBuffer",
    "cudaDevAttrConcurrentKernels",
    "_async",
    "cudaEglColorFormatYVU444Planar_ER",
    "__vimax3_u32",
    "_ARCH",
    "cudaMemPool_t",
    "cudaJitCacheMode",
    "cudaGraphicsResourceGetMappedEglFrame",
    "cudaDevResourceTypeSm",
    "_6x",
    "cudaMemcpyArrayToArray",
    "cudaMemAccessFlags",
    "cudaHostUnregister",
    "__float128",
    "__any",
    "cudaSharedMemBankSizeFourByte",
    "cudaOccupancyDefault",
    "cudaDeviceGetHostAtomicCapabilities",
    "__threadfence_block()",
    "cudaEglColorFormatY",
    "_PERCENTAGE",
    "cudaErrorInvalidSource",
    "cudaEventQuery",
    "__device__cudaError_t",
    "cudaFuncSetSharedMemConfig",
    "cudaGraphExecUpdateErrorNotSupported",
    "cudaFuncSetAttribute",
    "cudaDevAttrMaxGridDimY",
    "_semaphore",
    "__cvta_generic_to_constant()",
    "cudaLibrary_t",
    "cudaGraphNodeGetDependencies",
    "cudaChannelFormatKindSignedNormalized8X1",
    "cudaGraphUpload",
    "cudaLimitPrintfFifoSize",
    "cudaDeviceGetProperties",
    "cudaFuncGetParamInfo",
    "cudaErrorIllegalAddress",
    "cudaLogsCurrent",
    "cudaMemoryTypeManaged",
    "_expect",
    "_exchange",
    "__nv_atomic_fetch_xor",
    "cudaEglColorFormatY10V10U10_444SemiPlanar_ER",
    "cudaErrorCapturedEvent",
    "cudaLibraryLoadData",
    "cudaMemRangeAttributePreferredLocationType",
    "cudaMemoryAdvise",
    "cudaGraphExecMemcpyNodeSetParamsFromSymbol",
    "__nv_fp128_ldexp",
    "cudaGetTextureObjectResourceDesc",
    "cudaErrorSetOnActiveProcess",
    "__dmul_rn()",
    "cudaStreamCreateWithFlags",
    "cudaMemHandleTypeFabric",
    "__mbarrier_maximum_count()",
    "cudaGraphicsRegisterFlagsSurfaceLoadStore",
    "cudaReadModeNormalizedFloat",
    "cudaResourceTypeMipmappedArray",
    "cudaGraphDebugDotFlagsMemsetNodeParams",
    "cudaMemcpyOperandTypePointer",
    "cudaMemAdviseUnsetPreferredLocation",
    "_110f",
    "cudaLibraryOption",
    "cudaResourceViewFormat",
    "cudaMemHandleTypeWin32",
    "cudaD3D9MapFlags",
    "__nv_fp128_log",
    "__nv_pure__",
    "cudaDevAttrMaxTexture2DHeight",
    "cudaMemPoolAttrUsedMemCurrent",
    "__nv_bfloat162",
    "cudaErrorInvalidDeviceFunction",
    "cudaD3D9ResourceSetMapFlags",
    "cudaDevAttrManagedMemory",
    "cudaDevAttrIntegrated",
    "cudaGraphDeviceNode_t",
    "cudaGraphicsCubeFace",
    "cudaLogsDumpToMemory",
    "cudaEglColorFormatBayerGBRG",
    "cudaMemcpy3DOperandType",
    "cudaDevAttrMaxSurface2DLayeredLayers",
    "cudaExternalSemaphore_t",
    "__stcs",
    "cudaMemAdviceSetAccessedBy",
    "cudaResViewFormatSignedInt1",
    "cudaEventElapsedTime",
    "_p64",
    "__nv_atomic_add",
    "cudaGLUnmapBufferObject",
    "cudaEglColorFormatBGRA",
    "cudaDevAttrConcurrentManagedAccess",
    "cudaExternalSemaphoreSignalParams",
    "__nv_is_extended_host_device_lambda_closure_type",
    "cudaGraphExecNodeSetParams",
    "cudaResViewFormatUnsignedInt4",
    "__vibmax_s16x2",
    "cudaJitFallbackStrategy",
    "cudaD3D10Typedefs",
    "cudaEGLStreamConsumerDisconnect",
    "cudaStreamGetPriority",
    "_idx2",
    "__nv_aligned_device_malloc",
    "cudaOccupancyMaxActiveClusters",
    "cudaDevAttrTccDriver",
    "__vimax3_u16x2",
    "cudaDevAttrMaxTexture3DWidthAlt",
    "cudaGraphNodeGetDependentNodes",
    "cudaD3D10DeviceListAll",
    "cudaDevAttrStreamPrioritiesSupported",
    "cudaDevAttrMaxSurface3DWidth",
    "__dsqrt_",
    "cudaDevAttrNumaId",
    "cudaAtomicOperationFloatMax",
    "cudaErrorMpsConnectionFailed",
    "cudaHostAlloc",
    "cudaAtomicCapabilityReduction",
    "cudaMemGetDefaultMemPool",
    "cudaErrorNotMapped",
    "cudaChannelFormatKindUnsignedNormalized16X1",
    "__nv_atomic_load()",
    "cudaEglColorFormatYVU422Planar_ER",
    "cudaComputeModeProhibited",
    "cudaEglColorFormatYVU420Planar",
    "cudaMemcpy3DAsync",
    "cudaDevAttrPciDeviceId",
    "_HANDLE",
    "cudaStreamEndCapture",
    "cudaDeviceGraphMemTrim",
    "cudaStreamPerThread",
    "CUDAAPI",
    "__nv_atomic_max",
    "cudaD3D9ResourceGetMappedPointer",
    "cudaEglColorFormatYUYV_ER",
    "cudaChannelFormatKindUnsignedBlockCompressed1SRGB",
    "cudaGraphInstantiateFlags",
    "cudaDev1",
    "cudaGraphMemAllocNodeGetParams",
    "cudaResourceTypePitch2D",
    "cudaEglColorFormatBayerIspGRBG",
    "cudaJitOverrideDirectiveValues",
    "cudaAtomicOperationFloatAdd",
    "__threadfence()",
    "cudaD3D9DeviceList",
    "cudaDeviceGetExecutionCtx",
    "cudaMemcpyDefault",
    "__nv_atomic_add()",
    "_FIRST",
    "cudaGraphAddChildGraphNode",
    "cudaGLDeviceListCurrentFrame",
    "cudaErrorInvalidPitchValue",
    "cudaDevAttrComputeCapabilityMinor",
    "cudaDevAttrCooperativeLaunch",
    "cudaFlushGPUDirectRDMAWritesTargetCurrentDevice",
    "_create",
    "__syncthreads()",
    "_vectorsize",
    "cudaExternalMemoryHandleTypeNvSciBuf",
    "cudaAddressModeWrap",
    "cudaErrorSymbolNotFound",
    "cudaGraphAddEmptyNode",
    "cudaErrorInvalidDevice",
    "_return",
    "cudaVersion",
    "__isGlobal()",
    "cudaGraphicsEGLRegisterImage",
    "__vibmin_u16x2",
    "cudaExecutionCtxGetDevice",
    "cudaEglColorFormatY10_ER",
    "cudaGraphKernelNodeSetGridDim",
    "cudaResViewFormatFloat2",
    "cudaMemRangeAttributeLastPrefetchLocationType",
    "__mbarrier_t",
    "__viaddmax_u16x2",
    "cudaD3D10UnmapResources",
    "__nv_fp128_pow",
    "cudaDevAttrMaxTexture1DLayeredLayers",
    "__nv_fp128_fmin",
    "cudaInvalidDeviceId",
    "cudaGraphInstantiateError",
    "cudaDevResourceDesc_t",
    "cudaJitCacheOptionCA",
    "cudaDeviceGetTexture1DLinearMaxWidth",
    "cudaDeviceScheduleMask",
    "__nv_atomic_compare_exchange_n",
    "cudaGetErrorName",
    "cudaUserObject_t",
    "__drcp_",
    "cudaUUID_t",
    "cudaEglColorFormatYUVA",
    "cudaDeviceMask",
    "cudaDevAttrMaxBlockDimY",
    "_nctarank",
    "__mbarrier_arrive",
    "cudaErrorLaunchTimeout",
    "cudaDevAttrMaxSurface2DLayeredHeight",
    "cudaIpcGetEventHandle",
    "__shfl_up_sync()",
    "cudaEglColorFormatYVU444Planar",
    "cudaGraphNodeParams",
    "__nv_atomic_or",
    "cudaDriverEntryPointSymbolNotFound",
    "cudaDevSmResourceGroup_flags",
    "_dst",
    "cudaFuncAttribute",
    "cudaStreamCaptureModeGlobal",
    "__nv_atomic_store_n",
    "__nv_dl_wrapper_t",
    "__nv_atomic_store",
    "__frcp_",
    "__vimax_s16x2_relu",
    "__vimax_s32_relu",
    "cudaD3D10RegisterFlags",
    "cudaGraphNodeTypeHost",
    "cudaResViewFormatSignedChar4",
    "cudaMemsetAsync",
    "cudaD3D11DeviceListAll",
    "cudaErrorTextureFetchFailed",
    "cudaDevAttrLocalL1CacheSupported",
    "cudaHostRegisterPortable",
    "_blank",
    "cudaOccupancyMaxPotentialBlockSize",
    "__NV_ATOMIC_ACQ_REL",
    "cudaErrorIllegalState",
    "cudaMallocFromPoolAsync",
    "_LIBRARIES",
    "cudaLaunchDevice",
    "cudaJitLogVerbose",
    "cudaEglColorFormatR",
    "cudaGraphicsD3D9RegisterResource",
    "cudaIpcMemHandle",
    "_seq",
    "cudaPointerAttributes",
    "__vimax3_s32_relu",
    "cudaErrorInvalidGraphicsContext",
    "cudaEglColorFormatBayer14BGGR",
    "cudaLibraryGetKernelCount",
    "_INVALID",
    "cudaMemPool",
    "cudaEGL",
    "cudaGraphicsUnmapResources",
    "cudaGraphicsD3D10RegisterResource",
    "__logf",
    "cudaGetChannelDesc",
    "cudaGraphNodeTypeMemFree",
    "cudaChannelFormatKindUnsignedNormalized8X1",
    "_103f",
    "_not_",
    "cudaGraphNodeTypeMemset",
    "cudaEglColorFormatBayer14GBRG",
    "__nv_atomic_fetch_and",
    "_control",
    "cudaGraphicsMapFlags",
    "__nv_atomic_fetch_min",
    "cudaUserObjectRetain",
    "cudaErrorIncompatibleDriverContext",
    "_120a",
    "cudaGetSurfaceObjectResourceDesc",
    "cudaHostAllocWriteCombined",
    "cudaGreenCtxCreate",
    "cudaGLMapFlagsWriteDiscard",
    "cudaErrorDuplicateVariableName",
    "__nv_atomic_compare_exchange",
    "__fsub_",
    "_32a",
    "cudaErrorInsufficientDriver",
    "cudaMemGetMemPool",
    "cudaGLSetBufferObjectMapFlags",
    "cudaGLMapBufferObject",
    "cudaLaunchMemSyncDomain",
    "__sincosf",
    "cudaDevSmResourceSplitIgnoreSmCoscheduling",
    "cudaDevAttrMaxTexture2DWidth",
    "_SHARED",
    "__NV_THREAD_SCOPE_CLUSTER",
    "cudaD3D10RegisterFlagsArray",
    "cudaDevAttrVulkanCigSupported",
    "_PRIORITY",
    "cudaDevP2PAttrCudaArrayAccessSupported",
    "cudaResViewFormatFloat1",
    "cudaDevAttrMaxTextureCubemapLayeredWidth",
    "cudaErrorLaunchFailure",
    "__nv_atomic_fetch_add",
    "cudaLaunchAttributeSynchronizationPolicy",
    "__cvta_generic_to_local",
    "cudaGraphAddMemFreeNode",
    "_global",
    "cudaErrorHardwareStackError",
    "cudaErrorDeviceNotLicensed",
    "cudaDevAttrHostNativeAtomicSupported",
    "cudaGL",
    "cudaErrorNotSupported",
    "cudaMemcpy3DOperand",
    "cudaGetSymbolSize",
    "__host__cudaCreateChannelDesc",
    "cudaStreamFireAndForget",
    "cudaMemcpy",
    "_cst",
    "_groups",
    "cudaErrorInvalidSurface",
    "cudaExternalSemaphoreHandleTypeD3D12Fence",
    "cudaErrorLaunchMaxDepthExceeded",
    "cudaMemcpyFlagDefault",
    "cudaEventWaitExternal",
    "cudaGLDeviceListAll",
    "cudaMemcpyHostTo",
    "cudaMemcpyKind",
    "__nv_fp128_tanh",
    "cudaChannelFormatKindUnsignedNormalized16X4",
    "cudaErrorSoftwareValidityNotEstablished",
    "cudaEglColorFormatYUV422Planar",
    "_KMT",
    "cudaEglColorFormatRGBA",
    "cudaHostNodeParams",
    "cudaDeviceCanAccessPeer",
    "cudaUserObjectFlags",
    "_or",
    "cudaSuccess",
    "__any_sync",
    "cudaMemPoolImportPointer",
    "__viaddmin_u32",
    "__nv_fp128_trunc",
    "cudaGraphNodeGetEnabled",
    "__host__",
    "_TENSOR",
    "cudaDeviceProp",
    "cudaGraphNodeType",
    "cudaHostGetFlags",
    "cudaGraphExecKernelNodeSetParams",
    "__nv_fp128_log10",
    "_layout",
    "_fence",
    "cudaDriverEntryPointQueryResult",
    "CUDART_VERSION",
    "cudaBoundaryModeClamp",
    "cudaDeviceSyncMemops",
    "cudaEglColorFormatUYVY709_ER",
    "cudaResViewFormatUnsignedBlockCompressed6H",
    "cudaIpcEventHandle_t",
    "cudaIpcCloseMemHandle",
    "cudaLaunchAttributeIgnore",
    "__nv_fp128_log2",
    "cudaMemRangeAttributeAccessedBy",
    "__shfl_xor_sync()",
    "_tensormap",
    "cudaErrorLaunchOutOfResources",
    "cudaStreamGetId",
    "cudaGraphNodeTypeConditional",
    "cudaEglColorFormatYUYV422",
    "cudaResViewFormatSignedBlockCompressed6H",
    "cudaAtomicCapabilitySigned",
    "cudaMemcpy3DBatchOp",
    "cudaGraphLaunch",
    "cudaGPUDirectRDMAWritesOrderingAllDevices",
    "cudaKernelNodeAttrID",
    "cudaArraySurfaceLoadStore",
    "__NV_THREAD_SCOPE_SYSTEM",
    "cudaDevAttrMaxTexture3DHeight",
    "cudaGraphSetConditional",
    "cudaMemcpyFromArray",
    "cudaGraphNodeGetContainingGraph",
    "cudaEglColorFormatYVYU_ER",
    "cudaEvent_t",
    "cudaErrorECCUncorrectable",
    "cudaDeviceNumaConfigNumaNode",
    "cudaMemcpy2DArrayToArray",
    "cudaCreateSurfaceObject",
    "cudaDeviceSynchronize",
    "cudaChannelFormatKindSigned",
    "cudaNvSciSyncAttrWait",
    "cudaDevAttrReserved122",
    "cudaDeviceFlushGPUDirectRDMAWrites",
    "_BIT",
    "cudaDevAttrOnlyPartialHostNativeAtomicSupported",
    "__CUDA_ARCH__",
    "__shfl_down_sync",
    "cudaDevAttrReserved141",
    "cudaGraphChildGraphOwnershipClone",
    "cudaDevAttrMaxTextureCubemapLayeredLayers",
    "__nv_fp128_tan",
    "cudaDeviceGetDefaultMempool",
    "__cvta_shared_to_generic()",
    "cudaJitMaxRegisters",
    "cudaDevAttrMaxSurface3DHeight",
    "cudaAtomicCapabilityScalar32",
    "cudaDev0",
    "cudaExternalSemaphoreHandleTypeTimelineSemaphoreFd",
    "cudaSurfaceBoundaryMode",
    "cudaErrorInvalidResourceHandle",
    "_ID",
    "_TYPE_INVALID",
    "cudaStreamDestroy",
    "cudaGraphicsMapFlagsWriteDiscard",
    "cudaDevWorkqueueConfigScopeGreenCtxBalanced",
    "cudaAsyncNotificationInfo_t",
    "_scan",
    "cudaD3D9MapFlagsReadOnly",
    "cudaEventSynchronize",
    "cudaEnablePerThreadDefaultStream",
    "__device__cudaGraphExec_t",
    "cudaEglColorFormatY12_709_ER",
    "cudaErrorDeviceUninitialized",
    "cudaGraphExternalSemaphoresWaitNodeSetParams",
    "cudaDevAttrNumaConfig",
    "cudaEglColorFormatYUV444SemiPlanar",
    "cudaDevAttrMaxSurface1DLayeredWidth",
    "cudaGraphEventWaitNodeGetEvent",
    "cudaEventCreateWithFlags",
    "cudaComputeModeExclusiveProcess",
    "cudaDevAttrMaxTexture2DGatherHeight",
    "cudaExternalSemaphoreHandleTypeTimelineSemaphoreWin32",
    "cudagraphinstantiateflagautofreeonlaunch",
    "cudaErrorTensorMemoryLeak",
    "__nv_fp128_copysign",
    "cudaMemAllocationTypeManaged",
    "_Equal",
    "cudaDevSmResourceSplitMaxPotentialClusterSize",
    "__launch_bounds__",
    "__nv_atomic_exchange()",
    "cudaErrorStreamDetached",
    "cudaStreamGraphTailLaunch",
    "cudaEglResourceLocationVidmem",
    "__alignof",
    "cudaGraphInstantiate",
    "_object",
    "__viaddmax_s16x2_relu",
    "cudaGraphDependencyTypeProgrammatic",
    "cudaClusterSchedulingPolicyLoadBalancing",
    "cudaGraphDebugDotFlagsConditionalNodeParams",
    "cudaResViewFormatHalf4",
    "__restrict__",
    "cudaCreateTextureObject",
    "cudaGraphKernelNodeUpdatesApply",
    "cudaGraphNodeSetEnabled",
    "cudaMemRangeAttributeLastPrefetchLocation",
    "cudaMemAllocationTypeMax",
    "__nv_fp128_fma",
    "cudaStreamAttachMemAsync",
    "cudaSetDeviceFlags",
    "cudaGraphKernelNodeFieldGridDim",
    "cudaFuncAttributeClusterDimMustBeSet",
    "cudaEglColorFormatUYVY422",
    "cudaDevResourceTypeInvalid",
    "__nv_fp128_log1p",
    "__nv_aligned_device_malloc()",
    "cudaErrorUnsupportedExecAffinity",
    "cudaResViewFormatUnsignedShort2",
    "cudaDevAttrMaxTexture3DDepth",
    "_CG_QUALIFIER",
    "cudaEglColorFormatYUV420SemiPlanar_ER",
    "cudaEglColorFormatBayer12GRBG",
    "_is",
    "cudaEglColorFormatY10V10U10_444SemiPlanar_709_ER",
    "cudaDevAttrMaxTexture3DWidth",
    "cudaEglColorFormatA",
    "cudaDevAttrMaxTexture2DLinearPitch",
    "__match_sync()",
    "__nv_atomic_exchange_n",
    "cudaIpcEventHandle",
    "cudaMemcpySrcAccessOrderStream",
    "cudaMemPoolAttrReleaseThreshold",
    "cudaFreeAsync",
    "cudaEglColorFormatBayer10RGGB",
    "cudaErrorCallRequiresNewerDriver",
    "__all",
    "cudaInitDevice",
    "cudaVDPAU",
    "cudaD3D9RegisterFlagsArray",
    "_isGridConstant",
    "cudaEglColorFormatYUV422SemiPlanar_ER",
    "cudaErrorFileNotFound",
    "cudaChannelFormatKindUnsignedNormalized8X2",
    "_prior",
    "cudaGLDeviceListNextFrame",
    "__mbarrier_test_wait",
    "__viaddmax_u32",
    "cudaMemRangeAttributeReadMostly",
    "cudaEglColorFormatY10V10U10_422SemiPlanar_709",
    "__half2",
    "cudaGraphExecUpdateErrorUnsupportedFunctionChange",
    "_idx",
    "cudaExternalSemaphoreHandleTypeOpaqueFd",
    "cudaStreamBeginCaptureToGraph",
    "cudaKernelNodeAttributeAccessPolicyWindow",
    "cudaGraphAddHostNode",
    "__device__",
    "cudaMemAttachGlobal",
    "cudaMemcpy2DFromArray",
    "cudaEglColorFormatBayerGRBG",
    "cudaEglColorFormatYUV420SemiPlanar_2020",
    "cudaGraphExecDestroy",
    "cudaErrorStubLibrary",
    "cudaGraphInstantiateFlagUpload",
    "cudaDevAttrMaxSurface2DHeight",
    "_QUEUES",
    "cudaExternalMemoryMipmappedArrayDesc_st",
    "cudaDeviceEnablePeerAccess",
    "cudaMemAllocationTypeInvalid",
    "cudaDevAttrDirectManagedMemAccessFromHost",
    "cudaLaunchKernelEx",
    "cudaMemAllocNodeParamsV2",
    "cudaGraphNodeFindInClone",
    "cudaGraphInstantiateFlagUseNodePriority",
    "cudaFormatModeAuto",
    "cudaErrorAlreadyAcquired",
    "blockdim",
    "cudaDeviceRegisterAsyncNotification",
    "cudaDevAttrKernelExecTimeout",
    "_generic",
    "cudaD3D9UnregisterResource",
    "_70",
    "cudaGraphAddExternalSemaphoresSignalNode",
    "cudaTypedefs",
    "cudaEglColorFormatYUV420Planar_2020",
    "cudaLaunchConfig_t",
    "cudaResViewFormatSignedBlockCompressed5",
    "cudaErrorPeerAccessNotEnabled",
    "cudaEglColorFormatY12V12U12_420SemiPlanar",
    "cudaDevAttrClockRate",
    "cudaDevAttrHostRegisterSupported",
    "_rank",
    "cudaDeviceGeHostAtomicCapabilities",
    "_idx1",
    "cudaErrorMpsClientTerminated",
    "cudaLaunchAttribute",
    "cudaComputeMode",
    "__ldcg",
    "cudaMemcpyPeerAsync",
    "cudaDevWorkqueueResource",
    "cudaExecutionCtxRecordEvent",
    "cudaGraphNodeTypeExtSemaphoreWait",
    "cudaStreamAddCallback",
    "cudaLaunchAttributeDeviceUpdatableKernelNode",
    "cudaGLMapFlagsReadOnly",
    "cudaGraphMemcpyNodeSetParamsFromSymbol",
    "__log10f",
    "_CONNECTIONS",
    "cudaD3D9Typedefs",
    "_ALLOC",
    "_11x",
    "cudaGraphCondTypeWhile",
    "__nv_fp128_sqrt",
    "_NONE",
    "cudaFuncCache",
    "cudaMemcpyDeviceToDevice",
    "__shfl_sync()",
    "cudaDevP2PAttrOnlyPartialNativeAtomicSupported",
    "cudaSurfaceFormatMode",
    "cudaGraphicsMapFlagsReadOnly",
    "__nv_fp128_acos",
    "cudaFuncGetAttributes",
    "__pipeline_arrive_on",
    "__nv_atomic_fetch_and()",
    "cudaEglColorFormatBayer20RGGB",
    "cudaChannelFormatKindNone",
    "cudaGraphicsCubeFaceNegativeZ",
    "cudaGraphEdgeData",
    "_broadcast",
    "__nv_atomic_fetch_min()",
    "cudaEglColorFormatBayer14GRBG",
    "cudaIpcOpenMemHandle",
    "cudaArrayDefault",
    "cudaErrorDeviceUnavailable",
    "cudaErrorInvalidHostPointer",
    "cudaLaunchMemSyncDomainMap",
    "cudaErrorSharedObjectInitFailed",
    "cudaChannelFormatKindSignedNormalized8X4",
    "cudaResViewFormatSignedShort4",
    "cudaD3D10SetDirect3DDevice",
    "cudaFuncCachePreferEqual",
    "_9rvR6bGxpo9OdTnXTR1",
    "cudaMemPoolAttr",
    "cudaErrorOperatingSystem",
    "_compare",
    "cudaLaunchAttributeMemSyncDomain",
    "cudaDeviceGetStreamPriorityRange",
    "__nv_atomic_store()",
    "cudaErrorProfilerNotInitialized",
    "cudaGraphEventRecordNodeGetEvent",
    "cudaMemFreeAsync",
    "cudaMemHandleTypePosixFileDescriptor",
    "__cudaOccupancyB2DHelper",
    "cudaAtomicOperationCAS",
    "cudaDevAttrTotalConstantMemory",
    "__nv_is_extended_device_lambda_closure_type",
    "cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt",
    "cudaGraphInstantiateSuccess",
    "cudaDevAttrHostNumaMemoryPoolsSupported",
    "_v1",
    "__nv_atomic_thread_fence()",
    "cudaMemcpy3DBatchAsync",
    "cudaDevAttrMaxTexture2DMipmappedWidth",
    "cudaEglColorFormatYUV420SemiPlanar",
    "cudaResourceType",
    "cudaExternalSemaphoreHandleTypeKeyedMutexKmt",
    "cudaDevAttrGlobalMemoryBusWidth",
    "cudaHostRegisterReadOnly",
    "__nv_fp128_atan",
    "cudaGetKernel",
    "cudaChannelFormatKindSignedBlockCompressed6H",
    "cudaDevAttrMaxTexture2DLayeredLayers",
    "cudaVDPAUGetDevice",
    "cudaMemoryPool_t",
    "cudaMemPoolReuseAllowInternalDependencies",
    "__align__",
    "__Complex",
    "cudaMemoryType",
    "cudaAccessPropertyPersisting",
    "cudaMipmappedArray",
    "cudaEglColorFormatY10V10U10_420SemiPlanar_709",
    "_ptsz",
    "__exp10f",
    "cudaEglColorFormatYUV422SemiPlanar",
    "_replace",
    "__host__make_cudaPitchedPtr",
    "_preserved",
    "cudaChannelFormatKindSignedNormalized8X2",
    "cudaLimitMaxL2FetchGranularity",
    "cudaGraphNodeTypeWaitEvent",
    "__nv_atomic_and()",
    "cudaEglColorFormatBayer12CRBC",
    "cudaGraphicsRegisterFlagsNone",
    "cudaExternalSemaphoreWaitParams",
    "cudaGraphAddEventWaitNode",
    "cudaMipmappedArrayGetSparseProperties",
    "cudaErrorNoDevice",
    "cudaMemcpyToArrayAsync",
    "cudaEglColorFormatBayerCBRC",
    "_COPY",
    "_RetType",
    "__shfl_xor",
    "__nv_fp128_exp10",
    "_map",
    "cudaChannelFormatKindSignedNormalized16X4",
    "cudaEGLStreamProducerReturnFrame",
    "cudaArrayMemoryRequirements",
    "cudaFuncAttributeRequiredClusterWidth",
    "cudaDevAttrGpuPciDeviceId",
    "_ctarank",
    "cudaConditionalNodeParams",
    "__nv_fp128_add",
    "cudaLibraryLoadFromFile",
    "_PERSISTING",
    "cudaDeviceP2PAttr",
    "__NV_THREAD_SCOPE_THREAD",
    "cudaEglColorFormatY10_709_ER",
    "cudaGraphicsRegisterFlagsTextureGather",
    "cudaGraphConditionalNodeType",
    "cudaDevAttrHostMemoryPoolsSupported",
    "cudaLogsUnregisterCallback",
    "__vimin3_u32",
    "cudaDevP2PAttrNativeAtomicSupported",
    "cudaErrorGraphExecUpdateFailure",
    "__sync_fetch_and_add",
    "cudaGetErrorString",
    "cudaDevAttrMemSyncDomainCount",
    "cudaD3D10GetDirect3DDevice",
    "cudaEGLStreamConsumerAcquireFrame",
    "__forceinline__",
    "__shfl_down_sync()",
    "__NV_ATOMIC_CONSUME",
    "cudaDevAttrMaxPitch",
    "__builtin_unreachable",
    "__nv_dl_tag",
    "__builtin_assume",
    "cudaGetCurrentGraphExec",
    "cudaGraphNodeTypeExtSemaphoreSignal",
    "cudaDriverGetVersion",
    "cudaErrorSystemNotReady",
    "__popc",
    "__match_all_sync",
    "cudaErrorInvalidSymbol",
    "cudaDevAttrMemoryPoolSupportedHandleTypes",
    "cudaMemcpyAttributes",
    "cudaGetDeviceCount",
    "__half2float",
    "_unreachable",
    "cudaDevAttrReservedSharedMemoryPerBlock",
    "cudaMemRangeAttributeLastPrefetchLocationId",
    "cudaMemFree",
    "__host___",
    "cudaPeekAtLastError",
    "cudaErrorAddressOfConstant",
    "cudaLaunchAttrMemSyncDomain",
    "_ERROR",
    "_group",
    "__nv_fp128_cosh",
    "cudaDevAttrMaxSurface1DLayeredLayers",
    "cudaGraphEventRecordNodeSetEvent",
    "cudaLimitDevRuntimeSyncDepth",
    "cudaDeviceGetByPCIBusId",
    "cudaAsyncCallback",
    "cudaArraySparseProperties",
    "cudaGraphDebugDotFlagsExtSemasWaitNodeParams",
    "cudaGraphNodeTypeCount",
    "cudaErrorNotYetImplemented",
    "cudaErrorInvalidContext",
    "cudaDevWorkqueueConfigScopeDeviceCtx",
    "cudafree",
    "cudaD3D10GetDevices",
    "__lanemask_lt()",
    "cudaPreferPtx",
    "cudaChannelFormatKindUnsignedBlockCompressed3",
    "cudaResViewFormatSignedShort1",
    "cudaStreamCaptureModeThreadLocal",
    "cudaResViewFormatUnsignedBlockCompressed1",
    "__host__make_cudaExtent",
    "cudaAtomicOperationOr",
    "_IDLE",
    "_win32",
    "cudaDevAttrPageableMemoryAccessUsesHostPageTables",
    "cudaMemcpyOperandTypeMax",
    "blockidx",
    "cudaDevAttrTexturePitchAlignment",
    "cudaComputeModeDefault",
    "cudaAtomicOperationXOR",
    "cudaJitOptimizationLevel",
    "cudaEglColorFormatBayer20BGGR",
    "cudaDevAttrCanUseHostPointerForRegisteredMem",
    "_ON",
    "cudaDevAttrReserved128",
    "cudaFuncCachePreferL1",
    "__isShared()",
    "cudaErrorInvalidHandle",
    "cudaEglColorFormatY_ER",
    "cudaDevP2PAttrPerformanceRank",
    "cudaGraphCreate",
    "cudaDestroyTextureObject",
    "__nv_fp128_div",
    "cudaDevAttrMaxTexture1DWidth",
    "_Float128",
    "cudaSynchronizationPolicy",
    "cudaDevAttrMaxBlocksPerMultiprocessor",
    "__fdividef",
    "__syncthreads_and",
    "cudaClusterSchedulingPolicyDefault",
    "_LOADING",
    "cudaMemcpySrcAccessOrderDuringApiCall",
    "__nv_bfloat16",
    "__sinf",
    "cudaEglColorFormatABGR",
    "cudaEglColorFormatBayer12CCCC",
    "__viaddmax_s16x2",
    "cudaDevAttrAsyncEngineCount",
    "cudaExternalMemoryHandleTypeOpaqueWin32Kmt",
    "cudaKernelSetAttributeForDevice",
    "cudaEglColorFormat",
    "__INTEL_COMPILER_USE_INTRINSIC_PROTOTYPES",
    "cudaStreamIsCapturing",
    "cudaMemAllocationHandleType",
    "cudaGraphAddMemcpyNodeToSymbol",
    "cudaEglColorFormatUYVY_ER",
    "cudaDeviceGetMempool",
    "cudaGraphAddExternalSemaphoresWaitNode",
    "cudaMemAccessFlagsProtReadWrite",
    "_OP",
    "cudaMemAdviseSetAccessedBy",
    "__cvta_generic_to_global",
    "_75",
    "cudaErrorInvalidResourceType",
    "cudaArraySparsePropertiesSingleMipTail",
    "cudaEglFrameTypePitch",
    "cudaDevAttrIsMultiGpuBoard",
    "__isShared",
    "cudaMemRangeAttributePreferredLocationId",
    "cudaD3D10GetDevice",
    "__nv_atomic_sub",
    "cudaD3D10RegisterResource",
    "cudaMemSetMemPool",
    "cudaGLSetGLDevice",
    "cudaMemcpyHostToHost",
    "cudaErrorStreamCaptureUnmatched",
    "__dmul_",
    "cudaDevAttrMaxTexture1DLayeredWidth",
    "cudaGetSymbolAddress",
    "cudaGraphKernelNodeFieldParam",
    "cudaGraphAddKernelNode",
    "cudaMemAdvise",
    "cudaGraphExecUpdateResultInfo",
    "cudaMemPoolCreateUsageHwDecompress",
    "cudaEglColorFormatYUV444Planar",
    "__nv_atomic_compare_exchange()",
    "cudaGraphicsMapResources",
    "_src",
    "cudaLaunchCooperativeKernelMultiDevice",
    "__nv_atomic_fetch_sub",
    "cudaGetTextureObjectTextureDesc",
    "cudaOccupancyMaxActiveBlocksPerMultiprocessor",
    "cudaOccupancyDisableCachingOverride",
    "cudaStreamWaitEvent",
    "cudaGraphClone",
    "cudaResViewFormatSignedBlockCompressed4",
    "__expf",
    "cudaIpcEventRecord",
    "cudaDevAttrSparseCudaArraySupported",
    "cudaLogIterator",
    "cudaD3D11",
    "__cvta_generic_to_constant",
    "__syncwarp",
    "__nv_fp128_hypot",
    "cudaExternalSemaphoreHandleDesc",
    "__maxnreg__",
    "cudaDevAttrWarpSize",
    "cudaD3D10MapResources",
    "cudaKernel_t",
    "cudaDeviceMapHost",
    "cudaGraphExec_t",
    "cudaLibraryGetKernel",
    "_fmt",
    "cudaFilterModeLinear",
    "cudaExternalSemaphoreWaitSkipNvSciBufMemSync",
    "__nv_fp128_ceil",
    "cudaDriverEntryPointVersionNotSufficent",
    "cudaLimitMallocHeapSize",
    "cudaJitMinCtaPerSm",
    "cudaResViewFormatSignedChar1",
    "cudaAsyncNotificationType",
    "cudaEventCreateFromEGLSync",
    "cudaDevAttrMaxSurface2DWidth",
    "cudaD3D9MapFlagsWriteDiscard",
    "cudaLimitDevRuntimePendingLaunchCount",
    "cudaGraphGetId",
    "cudaChannelFormatKindUnsignedBlockCompressed2SRGB",
    "cudaMemRangeAttribute",
    "cudaGraphRetainUserObject",
    "_121f",
    "cudaGetDriverEntryPointByVersion",
    "cudaDevAttrMaxPersistingL2CacheSize",
    "cudaDevAttrIpcEventSupport",
    "cudaAsyncNotificationTypeOverBudget",
    "cudaDevAttrMultiProcessorCount",
    "cudaFormatModeForced",
    "__ballot_sync",
    "cudaAccessPolicyWindow",
    "__viaddmin_s16x2",
    "cudaDeviceSetGraphMemAttribute",
    "cudaGetDeviceFlags",
    "cudaFreeHost",
    "_isGlobal",
    "__mbarrier_init",
    "__global__",
    "cudaStreamCopyAttributes",
    "cudaD3D10MapFlags",
    "cudaSetDoubleForHost",
    "_arrive",
    "_local",
    "cudaHostAllocPortable",
    "__ldg()",
    "cudaFuncAttributes",
    "__nv_atomic_fetch_add()",
    "__nv_fp128_cos",
    "cudaErrorIllegalInstruction",
    "cudaJitErrorLogBuffer",
    "cudaJit_CacheMode",
    "__assume",
    "__constant__",
    "_p32",
    "__inline_hint__",
    "cudaChannelFormatKindFloat",
    "cudaD3D10ResourceGetMappedArray",
    "__viaddmin_s32",
    "_system",
    "cudaLibraryGetUnifiedFunction",
    "_one",
    "__isGridConstant",
    "cudaErrorTooManyPeers",
    "cudaErrorOutOfMemory",
    "cudaChildGraphNodeParams",
    "_static",
    "cudaDevAttrMaxGridDimX",
    "cudaErrorInvalidTexture",
    "cudaGridDependencySynchronize",
    "cudaExternalMemoryMipmappedArrayDesc",
    "cudaMemAdviseSetPreferredLocation",
    "cudaGraphConditionalHandle",
    "cudaErrorCooperativeLaunchTooLarge",
    "_diag",
    "__powf",
    "cudaMemcpy3DPeerAsync",
    "cudaDevAttrReserved145",
    "_idx3",
    "cudaChannelFormatDesc",
    "cudaFlushGPUDirectRDMAWritesOptions",
    "__nv_fp128_mul",
    "cudaLaunchKernel",
    "cudaEglColorFormatVYUY_ER",
    "cudaResViewFormatUnsignedBlockCompressed7",
    "_pure",
    "cudaAsyncCallbackEntry",
    "cudaResViewFormatUnsignedInt1",
    "__host__cudaError_t",
    "_isConstant",
    "_lambda",
    "cudaGraphicsD3D11RegisterResource",
    "cudaErrorUnsupportedDevSideSync",
    "cudaMemoryTypeUnregistered",
    "__nv_fp128_rint",
    "cudaMemPoolProps",
    "cudaMemHandleTypeWin32Kmt",
    "cudaGraphNodeGetToolsId",
    "cudaExternalMemoryHandleDesc",
    "cudaEglFrameTypeArray",
    "cudaDeviceScheduleBlockingSync",
    "cudaMemcpyFlagPreferOverlapWithCompute",
    "cudaAccessPropertyNormal",
    "cudaSetValidDevices",
    "__fsqrt_",
    "cudaEglColorFormatYUV420Planar",
    "_type",
    "cudaClusterSchedulingPolicy",
    "cudaDevice",
    "cudaFlushGPUDirectRDMAWritesScope",
    "cudaDevAttrGlobalL1CacheSupported",
    "cudaExtent",
    "__uuidof",
    "_CUDA",
    "cudaAddressModeClamp",
    "cudaDevAttrMaxRegistersPerBlock",
    "cudaDevAttrMaxThreadsPerBlock",
    "cudaEGLStreamConsumerReleaseFrame",
    "cudaD3D",
    "cudaDevAttrMaxSurface2DLayeredWidth",
    "cudaError",
    "cudaDeviceSetMempool",
    "cudaErrorApiFailureBase",
    "cudaEglColorFormatBayer20GRBG",
    "cudaErrorDuplicateSurfaceName",
    "cudaD3D10ResourceGetSurfaceDimensions",
    "cudaCGScopeReserved",
    "cudaStreamUpdateCaptureDependencies",
    "_FILE",
    "_aligned",
    "cudaEglColorFormatBayerIspGBRG",
    "cudaErrorNotPermitted",
    "cudaDevAttrMaxSurfaceCubemapLayeredLayers",
    "cudaDevAttrMaxThreadsPerMultiProcessor",
    "cudaErrorInvalidKernelImage",
    "cudaErrorMpsRpcFailure",
    "cudaEglColorFormatBayer12RGGB",
    "_math",
    "cudaHostAllocDefault",
    "cudaErrorSystemDriverMismatch",
    "cudaDestroyExternalSemaphore",
    "cudamempool",
    "cudaStreamCaptureStatusInvalidated",
    "cudaDevAttrMaxTexture2DMipmappedHeight",
    "cudaErrorCdpNotSupported",
    "_managed",
    "cudaMemcpy2DToArrayAsync",
    "cudaErrorInitializationError",
    "cudaExternalSemaphoreHandleType",
    "__builtin_assume_aligned()",
    "cudaMemcpyFromArrayAsync",
    "cudaGraphGetNodes",
    "cudaGraphExecMemcpyNodeSetParams",
    "cudaGraphExecUpdateErrorAttributesChanged",
    "cudaEglColorFormatYVU420SemiPlanar_709",
    "__cvta_generic_to_shared",
    "__nv_fp128_sin",
    "__nv_atomic_and",
    "cudaGraphAddNode",
    "cudaAtomicOperationCapability",
    "cudaD3D11GetDirect3DDevice",
    "cudaMemLocationTypeHostNuma",
    "cudaMallocAsync",
    "cudaMemPoolImportFromShareableHandle",
    "cudaGLUnregisterBufferObject",
    "cudaResourceViewDesc",
    "cudaMemcpy3DPeerParms",
    "cudaGraphHostNodeGetParams",
    "cudaLaunchAttributeCooperative",
    "__mbarrier_arrive_and_drop",
    "__nv_fp128_remainder",
    "_shared",
    "_TYPE",
    "syncthreads",
    "cudaHostRegister",
    "cudaGraphKernelNodeSetParams",
    "cudaMemset3D",
    "cudaGraphAddEventRecordNode",
    "cudaEglColorFormatYUV420Planar_ER",
    "_32B",
    "__vibmax_s32",
    "cudaGraphEventWaitNodeSetEvent",
    "cudaSharedMemBankSizeDefault",
    "__ddiv_",
    "cudaDevAttrSingleToDoublePrecisionPerfRatio",
    "cudaGraphNodeSetParams",
    "cudaHostNodeParamsV2",
    "cudagetparameterbuffer",
    "cudaFreeMipmappedArray",
    "cudaDevAttrReserved92",
    "cudaOccupancyMaxPotentialClusterSize",
    "cudaGraphMemAttrReservedMemCurrent",
    "_tensor",
    "__NV_ATOMIC_ACQUIRE",
    "_assume",
    "cudaFuncAttributePreferredSharedMemoryCarveout",
    "cudaMemPoolSetAccess",
    "cudaD3D10UnregisterResource",
    "cudaMemPoolDestroy",
    "cudaErrorValue",
    "cudaMemcpyPeer",
    "cudaIpcMemLazyEnablePeerAccess",
    "__reduce_min_sync",
    "cudaSharedMemBankSizeEightByte",
    "cudaGraphicsGLRegisterImage",
    "cudart",
    "_matrix",
    "cudaArrayCubemap",
    "cudaGraphUserObjectMove",
    "_LOG",
    "__cvta_generic_to_global()",
    "cudaThreadExchangeStreamCaptureMode",
    "cudaFree",
    "_90",
    "cudaDeviceNumaConfig",
    "cudaProfilerTypedefs",
    "cudaGraphMemcpyNodeSetParamsToSymbol",
    "__viaddmax_s32",
    "cudaGraphExecUpdateErrorTopologyChanged",
    "cudaEGLStreamProducerConnect",
    "cudaErrorContextIsDestroyed",
    "cudaEglFrameType",
    "cudaGraphInstantiateResult",
    "cudaDeviceGetPCIBusId",
    "__stcg",
    "cudaGraphExternalSemaphoresWaitNodeGetParams",
    "cudaEglColorFormatY12_ER",
    "__tanhf",
    "__reduce_xor_sync",
    "cudaStreamGetCaptureInfo",
    "__nv_fp128_asin",
    "cudaErrorInvalidPc",
    "cudaLaunchAttributeClusterDimension",
    "cudaErrorCudartUnloading",
    "cudaExecutionCtxStreamCreate",
    "cudaDevAttrTextureAlignment",
    "__shfl_sync",
    "cudaDevAttrMpsEnabled",
    "cudaLaunchAttributePreferredSharedMemoryCarveout",
    "cudaDevAttrD3D12CigSupported",
    "cudamalloc",
    "cudaErrorNvlinkUncorrectable",
    "cudaKernelNodeAttrValue",
    "cudaGraphNodeTypeMemcpy",
    "_BUS",
    "__nv_atomic_max()",
    "cudaSurfaceObject_t",
    "cudaExternalSemaphoreHandleTypeOpaqueWin32",
    "__host____device__const",
    "cudaChannelFormatKind",
    "cudaDriverEntryPointSuccess",
    "cudaStreamSetCaptureDependencies",
    "_grid",
    "cudaGPUDirectRDMAWritesOrdering",
    "cudaDevAttrMaxSharedMemoryPerBlock",
    "_LAUNCH",
    "cudaEglColorFormatYUV420SemiPlanar_709",
    "_restrict",
    "cudaEglColorFormatAYUV",
    "cudaResViewFormatUnsignedBlockCompressed2",
    "cudaDevAttrHostNumaMultinodeIpcSupported",
    "cudaGraphExecGetFlags",
    "cudaErrorUnsupportedPtxVersion",
    "cudaGLMapFlags",
    "cudaExternalSemaphoreWaitNodeParams",
    "cudaD3D9RegisterFlags",
    "_nv",
    "cudaLaunchMemSyncDomainDefault",
    "_fd",
    "cudaDeviceGetNvSciSyncAttributes",
    "cudaGraphExecEventWaitNodeSetEvent",
    "__nv_fp128_fabs",
    "cudaDevResourceGenerateDesc",
    "cudaOccupancyMaxPotentialBlockSizeWithFlags",
    "cudaLibraryEnumerateKernels",
    "cudaFlushGPUDirectRDMAWritesToOwner",
    "cudaD3D9",
    "__block_size__",
    "cudaFuncGetName",
    "cudaMemcpy2D",
    "cudaFlushGPUDirectRDMAWrites",
    "__nv_is_extended_device_lambda_with_preserved_return_type",
    "_fetch",
    "__threadfence_system()",
    "cudaGraphKernelNodeUpdate",
    "cudaStreamCreateWithPriority",
    "cudaErrorMissingConfiguration",
    "cudaHostRegisterDefault",
    "cudaEglColorFormatY12V12U12_420SemiPlanar_ER",
    "cudaMemGetInfo",
    "__nv_atomic_compare_exchange_n()",
    "cudaEglColorFormatY10V10U10_422SemiPlanar_2020",
    "cudaGraphicsResourceGetMappedMipmappedArray",
    "cudaEglColorFormatBayerRCCB",
    "cudaEglColorFormatBayer10GBRG",
    "__const___",
    "cudaAtomicOperation",
    "cudaD3D9SetDirect3DDevice",
    "cudaGLGetDevices",
    "cudaDeviceReset",
    "__cvta_shared_to_generic",
    "__NV_ATOMIC_RELEASE",
    "_vec",
    "cudaGraphCondTypeSwitch",
    "cudaProfilerStart",
    "cudaMemLocationTypeNone",
    "cudaMemcpyHostToDevice",
    "cudaEglColorFormatY10V10U10_420SemiPlanar_ER",
    "cudaLibraryHostUniversalFunctionAndDataTable",
    "cudagraphupload",
    "_fenceproxy",
    "_SWIZZLE",
    "cudaGraphExecUpdateResult",
    "cudaEventRecord",
    "cudaMemcpyFromSymbolAsync",
    "cudaD3D10DeviceListNextFrame",
    "cudaDeviceGetP2PAtomicCapabilities",
    "cudaMemAccessDesc",
    "cudaAtomicOperationAnd",
    "cudaMemcpyToSymbol",
    "cudaDevAttrReserved127",
    "cudaEglColorFormatYUV420Planar_709",
    "__CUDACC_RELAXED_CONSTEXPR__",
    "cudaExecutionCtxWaitEvent",
    "cudaD3D11DeviceListCurrentFrame",
    "_suppress",
    "cudaStreamAttributeAccessPolicyWindow",
    "cudaGLRegisterBufferObject",
    "cudaArrayDeferredMapping",
    "cudaChannelFormatKindUnsignedNormalized1010102",
    "cudaExecutionCtxDestroy",
    "cudaEglColorFormatBayer10GRBG",
    "__prof_trigger",
    "__nv_fp128_exp2",
    "__nv_atomic_fetch_sub()",
    "cudaJitInfoLogBufferSizeBytes",
    "cudaUserObjectRetainFlags",
    "cudaChannelFormatKindSignedNormalized16X1",
    "__builtin_expect()",
    "cudaGraphMemAttributeType",
    "cudaGraphKernelNodeFieldInvalid",
    "cudaMemPrefetchBatchAsync",
    "_WAITS",
    "__builtin_unreachable()",
    "cudaMemPoolCreate",
    "__nv_atomic_min()",
    "cudaGraphAddMemcpyNode",
    "_DISABLE",
    "__nv_atomic_thread_fence",
    "cudaGraphInstantiateMultipleDevicesNotSupported",
    "cudaEventCreate",
    "cudaGraphInstantiateConditionalHandleUnused",
    "cudaExternalMemoryDedicated",
    "cudaMemAdviseUnsetReadMostly",
    "__viaddmin_s16x2_relu",
    "cudaDevAttrMemoryClockRate",
    "cudaSharedmemCarveoutDefault",
    "cudaDeviceLmemResizeToMax",
    "cudaEglColorFormatY12V12U12_444SemiPlanar_ER",
    "cudaGraph_t",
    "__fadd_",
    "cudaMalloc",
    "__pipeline_memcpy_async",
    "_VALUE",
    "cudaGraphDependencyTypeDefault",
    "cudaEglColorFormatY10V10U10_420SemiPlanar_2020",
    "cudaEnableLegacyStream",
    "cudaLimit",
    "cudaGraphDebugDotFlagsKernelNodeParams",
    "_min",
    "cudaMemset",
    "cudaLimitPersistingL2CacheSize",
    "cudaExternalSemaphoreHandleTypeD3D11Fence",
    "cudaEnableDefault",
    "cudaAtomicOperationIntegerIncrement",
    "cudaDeviceBlockingSync",
    "cudaMemcpyFlags",
    "__nv_fp128_sub",
    "_scope",
    "cudaDevSmResourceSplitByCount",
    "cudaEventDefault",
    "cudaLibraryGetManaged",
    "cudaEglColorFormatBayer10BGGR",
    "cudaExecutionCtxGetId",
    "_BLOCKING",
    "cudaMallocMipmappedArray",
    "cudaStreamSynchronize",
    "__int128",
    "_bfloat16",
    "cudaMalloc3D",
    "cudaBoundaryModeZero",
    "cudaErrorStreamCaptureWrongThread",
    "cudaDevAttrGpuPciSubsystemId",
    "cudaGraphDebugDotFlagsMemcpyNodeParams",
    "__linux__",
    "cudaLaunchMemSyncDomainRemote",
    "cudaD3D9GetDevice",
    "_arch",
    "__nv_atomic_min",
    "cudaGraphExecMemsetNodeSetParams",
    "cudaEglPlaneDesc",
    "cudaJitThreadsPerBlock",
    "cudaDevAttrMaxBlockDimX",
    "cudaDevAttrMaxTexture1DLinearWidth",
    "cudaDeviceResourceGenerateDesc",
    "cudaTextureAddressMode",
    "cudaMemAdviseUnsetAccessedBy",
    "cudaMemRangeGetAttribute",
    "cudaMemLocationTypeHost",
    "cudaMallocPitch",
    "cudaEglColorFormatBayer12CBRC",
    "_relaxed",
    "cudaGraphAddMemsetNode",
    "cudaErrorStreamCaptureImplicit",
    "cudaD3D9MapResources",
    "cudaErrorInvalidDevicePointer",
    "cudaSharedmemCarveoutMaxL1",
    "cudaErrorStreamCaptureMerge",
    "cudaUserObjectNoDestructorSync",
    "cudaEglColorFormatY10V10U10_420SemiPlanar",
    "cudaIpcGetMemHandle",
    "cudaAtomicOperationFloatMin",
    "_with",
    "cudaJitErrorLogBufferSizeBytes",
    "cudaCGScopeInvalid",
    "cudaDevAttrMaxSurface1DWidth",
    "cudaLogsDumpToFile",
    "cudaCreateChannelDesc",
    "cudaD3D9UnmapResources",
    "_half",
    "cudaChannelFormatKindNV12",
    "cudaStreamCreate",
    "cudaArrayTextureGather",
    "cudaDevAttrMax",
    "CUDAGDB",
    "_DATA",
    "__nv_atomic_load",
    "cudaEGLStreamConsumerConnect",
    "cudaAtomicCapabilityScalar64",
    "cudaMemPoolExportToShareableHandle",
    "cudaExternalMemoryHandleTypeOpaqueFd",
    "cudaErrorDeviceAlreadyInUse",
    "cudaExternalSemaphoreHandleTypeNvSciSync",
    "_LIMIT",
    "cudaStreamGraphFireAndForgetAsSibling",
    "cudaMemFreeNodeParams",
    "cudaGraphNodeTypeEventRecord",
    "cudaMemcpyOperandTypeArray",
    "__CUDA_MINIMUM_ARCH__",
    "cudaD3D10MapFlagsWriteDiscard",
    "cudaMemPoolReuseFollowEventDependencies",
    "_TO",
    "__vibmin_u32",
    "cudaLaunchCooperativeKernel",
    "cudaEglColorFormatBayer12BGGR",
    "cudaEglPlaneDesc_st",
    "cudaEglColorFormatARGB",
    "_FORCE",
    "_noinline",
    "_max",
    "cudaSharedMemConfig",
    "_MANAGED",
    "cudaStreamQuery",
    "cudaDeviceScheduleYield",
    "cudaGetDriverEntryPoint",
    "cudaGetMipmappedArrayLevel",
    "_100f",
    "cudaArrayGetMemoryRequirements",
    "cudaCreateChannelDescHalf",
    "_BOOST",
    "cudaLaunchKernelExC",
    "cudaDevAttrReserved129",
    "cudaDevAttrGpuOverlap",
    "_threads",
    "__nv_fp128_acosh",
    "cudamallocasync",
    "_blocks",
    "__ldca",
    "cudaResViewFormatHalf1",
    "cudaErrorInvalidPtx",
    "cudaIpcMemHandles",
    "cudaMemFabricHandle_t",
    "cudaMemAccessFlagsProtNone",
    "_90a",
    "cudaEglColorFormatYVYU",
    "_images",
    "cudaJitInfoLogBuffer",
    "__nv_fp128_atanh",
    "__nanosleep",
    "cudaGraphicsCubeFacePositiveZ",
    "cudaResViewFormatSignedInt2",
    "cudaGraphKernelNodeSetAttribute",
    "cudaMemPoolGetAttribute",
    "cudaAtomicOperationExchange",
    "cudaDevAttrMaxGridDimZ",
    "cudaReadModeElementType",
    "_to",
    "_xor",
    "_101f",
    "cudaMemcpyBatchAsync",
    "__shfl_up",
    "cudaD3D9DeviceListAll",
    "cudaGraphChildGraphNodeOwnership",
    "_wait",
    "cudaEglColorFormatBayerCRBC",
    "_ORDER",
    "cudaExternalMemoryHandleTypeD3D12Heap",
    "cudaGraphDestroy",
    "cudaGraphKernelNodeGetAttribute",
    "_PRELOAD",
    "cudaD3D11Typedefs",
    "cudaErrorPeerAccessUnsupported",
    "cudaJitWallTime",
    "cudaMemLocationType",
    "_vRelease",
    "__nv_fp128_exp",
    "cudaGLDeviceList",
    "cudaExternalSemaphoreWaitNodeParamsV2",
    "cudaMemAllocationTypePinned",
    "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags",
    "cudaEglColorFormatBayerIspRGGB",
    "cudaGetLastError",
    "cudaErrorInvalidAddressSpace",
    "cudaEglColorFormatBayerIspBGGR",
    "cudaErrorMemoryValueTooLarge",
    "cudaErrorContained",
    "_host",
    "cudaGraphDebugDotFlags",
    "__mbarrier_inval",
    "__nv_atomic_fetch_xor()",
    "_order",
    "cudaJit_Fallback",
    "cudaDeviceGetDefaultMemPool",
    "cudaMemcpy3DPeer",
    "cudaDevAttrMaxTexture2DLayeredWidth",
    "cudaEglColorFormatBayerBGGR",
    "__VA_ARGS__",
    "__nv_fp128_fmod",
    "cudaCGScope",
    "cudaMemPoolAttrUsedMemHigh",
    "cudaDevAttrMaxSharedMemoryPerMultiprocessor",
    "cudaStreamBeginCapture",
    "_NODE",
    "cudaWGLGetDevice",
    "cudaGraphicsCubeFacePositiveY",
    "__grid_constant__",
    "__reduce_max_sync",
    "cudaEventRecordNodeParams",
    "cudaJitGenerateLineInfo",
    "cudaComputeModeExclusive",
    "cudaKernelNodeParams",
    "_MEMORY",
    "__launch_bounds__()",
    "_DEVICE",
    "cudaMemPrefetchAsync",
    "_rn",
    "cudaD3D10MapFlagsReadOnly",
    "CUDART_CB",
    "cudaEglColorFormatBayer12GBRG",
    "cudaGraphDebugDotFlagsHandles",
    "cudaD3D11GetDevices",
    "cudaGraphMemcpyNodeSetParams",
    "cudaResViewFormatUnsignedBlockCompressed3",
    "cudaDevAttrComputeMode",
    "cudaExternalMemoryGetMappedMipmappedArray",
    "cudaLaunchAttributeNvlinkUtilCentricScheduling",
    "cudaGraphNode_t",
    "cudaEventBlockingSync",
    "cudaMemoryTypeDevice",
    "cudaPointerGetAttributes",
    "cudaTextureObject_t",
    "cudaGraphicsSubResourceGetMappedArray",
    "__dsub_",
    "cudaAsyncCallbackHandle_t",
    "cudaD3D9ResourceGetMappedArray",
    "cudaDeviceGetSharedMemConfig",
    "cudaGraphChildGraphNodeGetGraph",
    "_mode",
    "cudaSetDoubleForDevice",
    "cudaGraphicsVDPAURegisterOutputSurface",
    "cudaLogsRegisterCallback",
    "cudaResViewFormatUnsignedChar4",
    "cudaGraphKernelNodePortDefault",
    "_list",
    "cudaHostAllocMapped",
    "__attribute__",
    "cudaMemcpyFromSymbol",
    "cudamempoolreuseallowopportunistic",
    "cudaGraphDestroyNode",
    "cudaDevSmResourceSplit",
    "cudaGraphicsGLRegisterBuffer",
    "cudaResViewFormatSignedChar2",
    "_JIT",
    "cudaMemPoolAttrReservedMemCurrent",
    "cudaArrayLayered",
    "cudaStreamGetDevResource",
    "cudaStreamUpdateCaptureDependenciesFlags",
    "cudaGraphicsResourceGetMappedPointer",
    "cudaAtomicOperationIntegerMin",
    "__cvta_local_to_generic()",
    "threadidx",
    "cudaDevAttrPageableMemoryAccess",
    "cudaErrorNotReady",
    "cudaGraphRemoveDependencies",
    "cudaErrorMpsMaxClientsReached",
    "cudaGraphExecUpdateError",
    "cudaD3D10",
    "__cosf",
    "__cluster_dims__",
    "cudamempoolreuseallowinternaldependencies",
    "cudaMallocArray",
    "cudaDevAttrHostNumaId",
    "_isShared",
    "cudaGraphKernelNodePortLaunchCompletion",
    "cudaMemAllocationType",
    "cudaChannelFormatKindUnsignedBlockCompressed5",
    "cudaErrorInvalidTextureBinding",
    "cudaArray_const_t",
    "cudaEglColorFormatYUV444SemiPlanar_ER",
    "cudaDevAttrEccEnabled",
    "__ldlu",
    "_usage",
    "cudaGraphKernelNodeSetParam",
    "cudaEglColorFormatYUV444Planar_ER",
    "_copy",
    "cudaEglResourceLocationSysmem",
    "cudaMemHandleTypeNone",
    "cudaStreamCaptureModeRelaxed",
    "__viaddmin_u16x2",
    "cudaDevAttrClusterLaunch",
    "cudaDevAttrDeferredMappingCudaArraySupported",
    "cudaMapHost",
    "cudaChooseDevice",
    "__vibmax_u32",
    "cudaIpcOpenEventHandle",
    "cudaGraphConditionalHandleCreate",
    "cudaTextureDesc",
    "cudaFreeArray",
    "cudaAtomicOperationIntegerMax",
    "cudaEglColorFormatY12V12U12_420SemiPlanar_709_ER",
    "cudaOccupancyMaxPotentialBlockSizeVariableSMem",
    "cudaDevWorkqueueConfigScope",
    "cudaErrorMisalignedAddress",
    "cudaDevAttrMaxTexture2DGatherWidth",
    "cudaMemcpyToArray",
    "cudaChannelFormatKindUnsigned",
    "__nv_fp128_ilogb",
    "__trap()",
    "cudaGraphDependencyType",
    "cudaDeviceScheduleAuto",
    "cudaExternalMemoryBufferDesc_st",
    "cudaKernelNodeParamsV2",
    "cudaDeviceCount",
    "cudaResViewFormatSignedInt4",
    "cudaSharedCarveout",
    "cudaGraphExecUpdateErrorNodeTypeChanged",
    "cudaResViewFormatNone",
    "_scan_update",
    "cudaGraphKernelNodeGetParams",
    "cudaGraphNodeTypeGraph",
    "__vimin_s32_relu",
    "cudaDestroyExternalMemory",
    "cudaEglColorFormatBayer12RCCB",
    "cudaExternalSemaphoreSignalSkipNvSciBufMemSync",
    "_VISIBLE",
    "cudaGraphCondTypeIf",
    "cudaExternalMemoryHandleTypeD3D12Resource",
    "cudaGraphInstantiateFlagDeviceLaunch",
    "cudaMemcpyDeviceToHost",
    "cudaStreamAddCaptureDependencies",
    "cudaErrorStreamCaptureIsolation",
    "__vimin3_u16x2",
    "__nv_fp128_floor",
    "cudaMemcpy2DToArray",
    "_fast",
    "_val",
    "_isLocal",
    "cudaGLMapFlagsNone",
    "cudaEglColorFormatYVU422Planar",
    "cudaStream_t",
    "__alignof()",
    "cudaDevAttrGPUDirectRDMAWritesOrdering",
    "_extended",
    "cudaResourceTypeLinear",
    "cudaExternalMemoryHandleTypeD3D11ResourceKmt",
    "_EXTERNAL",
    "__nv_fp128_asinh",
    "cudaMemcpyToSymbolAsync",
    "cudaArrayGetSparseProperties",
    "__pipeline_wait_prior",
    "cudaEglColorFormatYVU422SemiPlanar_ER",
    "cudaSignalExternalSemaphoresAsync",
    "cudaGraphicsMapFlagsNone",
    "cudaMemLocation",
    "cudaMemPoolSetAttribute",
    "_CACHE",
    "_ru",
    "__mbarrier_token_t",
    "cudaGraphConditionalHandleCreate_v2",
    "cudaDeviceUnregisterAsyncNotification",
    "cudaGraphMemsetNodeGetParams",
    "cudaEglColorFormatYUVA_ER",
    "cudaEglColorFormatYVU420SemiPlanar_ER",
    "cudaChannelFormatKindSignedBlockCompressed4",
    "cudaDevAttrMaxTexture3DDepthAlt",
    "cudaDevAttrReserved123",
    "__nv_atomic_exchange_n()",
    "cudaStreamAttrValue",
    "cudaDevWorkqueueConfigResource",
    "_thread",
    "cudaGraphicsVDPAURegisterVideoSurface",
    "cudaErrorAssert",
    "cudaLogsCallbackHandle",
    "cudamemcpyasync",
    "__nv_atomic_exchange",
    "_NORM",
    "cudaMemset3DAsync",
    "cudaGraphNodeGetType",
    "cudaGPUDirectRDMAWritesOrderingOwner",
    "cudaDevAttrPciBusId",
    "cudaErrorJitCompilationDisabled",
    "cudaAtomicOperationIntegerDecrement",
    "cudaLaunchAttributeValue",
    "cudaD3D9ResourceGetMappedSize",
    "cudaErrorProfilerAlreadyStopped",
    "cudaDevAttrGPUDirectRDMAFlushWritesOptions",
    "_DEVICES",
    "_forceinline",
    "cudaDevAttrUnifiedAddressing",
    "_AUTO",
    "cudaErrorMixedDeviceExecution",
    "cudaStreamTailLaunch",
    "cudaDevResourceTypeWorkqueueConfig",
    "cudaJitOption",
    "cudaJitGenerateDebugInfo",
    "cudaDevAttrPciDomainId",
    "_size",
    "__nv_atomic_load_n()",
    "cudaGraphReleaseUserObject",
    "__maxnreg__()",
    "cudaDevAttrComputePreemptionSupported",
    "__cvta_generic_to_shared()",
    "__isGlobal",
    "cudaJitMaxThreadsPerBlock",
    "_128B",
    "__vimin3_s16x2",
    "_WIN32",
    "__builtin_expect",
    "cudaDevAttrHostRegisterReadOnlySupported",
    "__NV_ATOMIC_RELAXED",
    "cudaLibraryBinaryIsPreserved",
    "__nv_atomic_xor()",
    "__ballot",
    "cudaResViewFormatUnsignedChar1",
    "_L2",
    "_v10010",
    "cudaGraphDebugDotPrint",
    "cudaChannelFormatKindUnsignedBlockCompressed7SRGB",
    "cudaD3D9RegisterFlagsNone",
    "cudaDevAttrMemoryPoolsSupported",
    "cudaDevP2PAttrAccessSupported",
    "cudaStreamSetAttribute",
    "cudaErrorPriorLaunchFailure",
    "cudaEglColorFormatYVU420Planar_709",
    "cudaChannelFormatKindUnsignedBlockCompressed7",
    "cudaErrorProfilerDisabled",
    "cudaGetDevice",
    "_atomic",
    "_MAP",
    "cudaMemAllocNodeParams",
    "cudaErrorNotMappedAsPointer",
    "cudaChannelFormatKindUnsignedNormalized8X4",
    "cudaDevAttrReserved132",
    "cudaGraphExternalSemaphoresSignalNodeSetParams",
    "cudaNvSciSyncAttrSignal",
    "__NV_THREAD_SCOPE_BLOCK",
    "cudaGraphAddMemcpyNodeFromSymbol",
    "_GRAPHS",
    "cudaDeviceGetLimit",
    "cudaErrorUnsupportedLimit",
    "cudaGLMapBufferObjectAsync",
    "__NV_THREAD_SCOPE_DEVICE",
    "cudaOffset3D",
    "cudaErrorCompatNotSupportedOnDevice",
    "cudaGraphKernelNodeField",
    "cudaErrorHostMemoryNotRegistered",
    "__ldg",
    "_builtin",
    "cudaBoundaryModeTrap",
    "cudaErrorAlreadyMapped",
    "cudaDevSmResourceGroupBackfill",
    "cudaResViewFormatSignedShort2",
    "__reduce_sync",
    "__cluster_dims__()",
    "_v2",
    "cudaMemcpy3D",
    "cudaArray",
    "cudaDevAttrMaxRegistersPerMultiprocessor",
    "cudaGraphExecUpdateSuccess",
    "cudaDevSmResource",
    "cudaD3D10DeviceListCurrentFrame",
    "cudaMipmappedArray_const_t",
    "__prof_trigger()",
    "__nv_atomic_sub()",
    "cudaChannelFormatKindUnsignedBlockCompressed2",
    "cudaD3D9GetDirect3DDevice",
    "cudaEglColorFormatY12V12U12_444SemiPlanar",
    "cudaGraphMemAttrReservedMemHigh",
    "cudaEglColorFormatRG",
    "_64B",
    "cudaAtomicCapabilityUnsigned",
    "__ldcs",
    "cudaInitDeviceFlagsAreValid",
    "cudaFuncAttributeMaxDynamicSharedMemorySize",
    "cudaFuncAttributeRequiredClusterHeight",
    "cudaMemLocationTypeDevice",
    "_ArgTypes",
    "_Z3foo1S",
    "_out",
    "cudaGraphicsCubeFacePositiveX",
    "cudaLibraryUnload",
    "cudaErrorStreamCaptureInvalidated",
    "__isGridConstant()",
    "cudaLaunchAttributeMemSyncDomainMap",
    "_hint",
    "__nv_is_extended_device_lambda_with_preserved_return_type()",
    "cudaCtxResetPersistingL2Cache",
    "cudaLibraryGetGlobal",
    "cudaGraphExecGetId",
    "cudaD3D9ResourceGetMappedPitch",
    "cudaDeviceGetP2PAttribute",
    "cudaGraphNodeTypeEmpty",
    "cudaGraphNodeTypeKernel",
    "cudaResViewFormatUnsignedBlockCompressed4",
    "cudaGraphicsGLRegister",
    "cudaImportExternalMemory",
    "cudaGraphAddDependencies",
    "__tanf",
    "__assume()",
    "cudaApiGetDriverEntryPoint",
    "_EXCEPTION",
    "cudaMemPoolPtrExportData",
    "cudaIpcMemHandle_t",
    "__isConstant()",
    "cudaErrorInvalidNormSetting",
    "cudaEGLStreamConsumerConnectWithFlags",
    "cudaMemLocationTypeInvalid",
    "cudaResourceTypeArray",
    "cudaPitchedPtr",
    "cudaJitCacheOptionCG",
    "cudaProfilerStop",
    "cudaStreamGetFlags",
    "cudaGraphExternalSemaphoresSignalNodeGetParams",
    "cudaGraphMemsetNodeSetParams",
    "__nv_fp128_expm1",
    "cudaGraphGetEdges",
    "cudaMemPoolExportPointer",
    "cudaDevAttrMultiGpuBoardGroupID",
    "_OPAQUE",
    "cudaDevAttrMaxSurface3DDepth",
    "__vibmin_s16x2",
    "__grid_constant_",
    "cudaChannelFormatKindUnsignedNormalized16X2",
    "cudaGraphChildGraphOwnershipMove",
    "_device",
    "cudaExternalSemaphoreSignalNodeParams",
    "cudaCpuDeviceId",
    "cudaErrorInvalidChannelDescriptor",
    "cudaEvents",
    "cudaD3D9DeviceListNextFrame",
    "cudaErrorTimeout",
    "cudaLaunchAttributePreferredClusterDimension",
    "cudaErrorInvalidFilterSetting",
    "__cvta_generic_to_local()",
    "cudaChannelFormatKindSignedBlockCompressed5",
    "cudaDevAttrMaxSurfaceCubemapWidth",
    "cudaGraphicsRegisterFlagsReadOnly",
    "cudaEglColorFormatYVU420SemiPlanar_2020",
    "cudaTriggerProgrammaticLaunchCompletion",
    "_ctaid",
    "cudaExternalMemoryGetMappedBuffer",
    "__nv_atomic_or()",
    "cudaEglFrame",
    "cudaEglColorFormatUYVY2020",
    "cudaDevAttrMaxTexture2DLinearWidth",
    "__fmul_",
    "cudaDevAttrMaxTextureCubemapWidth",
    "_atomicity",
    "__fdiv_",
    "__reduce_or_sync",
    "_103a",
    "cudaEventRecordDefault",
    "__longlong_as_double",
    "__host____device__cudaError_t",
    "cudaGraphInstantiateInvalidStructure",
    "cudaErrorMpsMaxConnectionsReached",
    "__brkpt()",
    "cudaLimitStackSize",
    "cudaMemPoolTrimTo",
    "cudaEglColorFormatYVU444SemiPlanar_ER",
    "__nv_fp128_round",
    "cudaDeviceAttr",
    "__fmaf_",
    "cudaExternalMemory_t",
    "cudaD3D9GetDevices",
    "cudaEglColorFormatBayer14RGGB",
    "cudaGLTypedefs",
    "cudaEventWaitDefault",
    "cudaMemAccessFlagsProtRead",
    "__syncwarp()",
    "cudaErrorLaunchIncompatibleTexturing",
    "cudaEglColorFormatBayerRGGB",
    "_ref",
    "cudaLaunchAttributeClusterSchedulingPolicyPreference",
    "cudaStreamLegacy",
    "cudaFuncAttributeNonPortableClusterSizeAllowed",
    "cudaDevAttrTimelineSemaphoreInteropSupported",
    "cudaMemset2DAsync",
    "cudaErrorUnmapBufferObjectFailed",
    "cudaDevAttrCanMapHostMemory",
    "cudaFuncAttributeClusterSchedulingPolicyPreference",
    "cudaGraphicsRegisterFlags",
    "cudaHostRegisterMapped",
    "cudaMemRangeAttributePreferredLocation",
    "cudaDevResource",
    "__vimax3_s16x2_relu",
    "cudaSharedmemCarveoutMaxShared",
    "cudaEglColorFormatYVU420Planar_2020",
    "cudaLaunchAttributeAccessPolicyWindow",
    "cudaGraphInstantiateWithFlags",
    "cudaGraphInstantiateNodeOperationNotSupported",
    "__nv_fp128_sinh",
    "cudaMemoryTypeHost",
    "__int_as_float",
    "cudaGraphAddMemAllocNode",
    "cudaEglColorFormatUYVY709",
    "__activemask()",
    "cudaHostGetDevicePointer",
    "cudaErrorJitCompilerNotFound",
    "cudaMemDiscardAndPrefetchBatchAsync",
    "cudaMemPoolGetAccess",
    "_and",
    "cudaFuncCachePreferNone",
    "_MODULE",
    "cudaD3D10ResourceSetMapFlags",
    "__stwb",
    "cudaUserObjectCreate",
    "cudaGraphInstantiateParams",
    "cudaAccessProperty",
    "__shfl_down",
    "cudaEventInterprocess",
    "_store",
    "cudaExternalMemoryBufferDesc",
    "cudaInteropResourceSetMapFlags",
    "cudaSetDevice",
    "cudaD3D10ResourceGetMappedSize",
    "cudalaunchdevice",
    "cudaGraphNodeGetLocalId",
    "cudaEglColorFormatY10V10U10_420SemiPlanar_709_ER",
    "cudaGraphicsD9D",
    "__vimin3_s16x2_relu",
    "cudaChannelFormatKindUnsignedBlockCompressed4",
    "cudaResViewFormatFloat4",
    "cudaDevAttrMaxSurfaceCubemapLayeredWidth",
    "cudaEglColorFormatBayer20GBRG",
    "__CUDA_ARCH_SPECIFIC__",
    "cudaFuncAttributeRequiredClusterDepth",
    "cudaHostRegisterIoMemory",
    "cudaErrorSynchronizationError",
    "cudaFuncAttributeMax",
    "_v3",
    "cudaMemcpy2DFromArrayAsync",
    "cudaEglColorFormatBayer10CCCC",
    "cudaImportExternalSemaphore",
    "cudaErrorNoKernelImageForDevice",
    "cudaLaunchAttributeProgrammaticStreamSerialization",
    "__managed__",
    "__frsqrt_rn",
    "__syncthreads_or",
    "cudaErrorProfilerAlreadyStarted",
    "cudaFlushGPUDirectRDMAWritesOptionHost",
    "cudaErrorFunctionNotLoaded",
    "__nv_atomic_store_n()",
    "__vimin3_s32_relu",
    "cudaErrorStreamCaptureUnjoined",
    "cudaConfigureCall",
    "cudaResViewFormatHalf2",
    "_nctaid",
    "cudaFilterModePoint",
    "__cvta_constant_to_generic()",
    "cudaGLUnmapBufferObjectAsync",
    "cudaExecutionCtxSynchronize",
    "cudaMemAttachSingle",
    "cudaErrorDevicesUnavailable",
    "cudaDevResourceType",
    "cudaEglColorFormatY12V12U12_444SemiPlanar_709_ER",
    "cudaGraphDebugDotFlagsHostNodeParams",
    "cudaDevAttrSurfaceAlignment",
    "__mbarrier_pending_count",
    "cudaArrayGetPlane",
    "_add",
    "_10x",
    "cudaD3D10DeviceList",
    "cudaResViewFormatUnsignedBlockCompressed5",
    "cudaChannelFormatKindUnsignedBlockCompressed6H",
    "_load",
    "cudaDeviceGetGraphMemAttribute",
    "cudamempoolreusefolloweventdependencies",
    "cudaDevAttrMaxTexture1DMipmappedWidth",
    "cudaMipmappedArray_t",
    "cudaDeviceSetSharedMemConfig",
    "cudaDevAttrCanFlushRemoteWrites",
    "__nv_atomic_load_n",
    "cudaMemcpy2DAsync",
    "__CUDACC_EXTENDED_LAMBDA__",
    "cudaMemAdviseSetReadMostly",
    "cudaDevAttrReserved94",
    "cudaGraphDebugDotFlagsKernelNodeAttributes",
    "cudaErrorCdpVersionMismatch",
    "__nv_fp128_fmax",
    "__nv_atomic_fetch_max",
    "__match_any_sync",
    "cudaGraphMemAttrUsedMemCurrent",
    "cudaDeviceSetCacheConfig",
    "cudaMemDiscardBatchAsync",
    "cudaErrorSyncDepthExceeded",
    "cudamallocmanaged",
    "cudaProfiler",
    "cudaEglColorFormatY_709_ER",
    "cudaMalloc3DArray",
    "cudaEglColorFormatYVU420Planar_ER",
    "__nv_atomic_fetch_or()",
    "cudaD3D10ResourceGetMappedPointer",
    "cudaLuid",
    "__vimin_s16x2_relu",
    "cudaEglColorFormatAYUV_ER",
    "cudaErrorStartupFailure",
    "__half",
    "cudadevrt",
    "cudaDeviceScheduleSpin",
    "cudaD3D11DeviceListNextFrame",
    "__ffs",
    "cudaGetTextureObjectResourceViewDesc",
    "cudaStreamGetAttribute",
    "cudaDevAttrMaxBlockDimZ",
    "cudaStreamGraphFireAndForget",
    "cudaMemsetParams",
    "cudaGraphExecMemcpyNodeSetParamsToSymbol",
    "cudaGetTextureAlignmentOffset",
    "cudaRuntimeGetVersion",
    "cudaMemAdviceUnsetReadMostly",
    "__reduce_and_sync",
    "_DEFAULT",
    "cudaD3D9ResourceGetSurfaceDimensions",
    "_ptds",
    "cudaMemAttachHost",
    "cudaExternalMemoryHandleTypeOpaqueWin32",
    "cudaGraphExecUpdate",
    "cudaDevAttrMaxAccessPolicyWindowSize",
    "cudaGraphExecExternalSemaphoresWaitNodeSetParams",
    "__nv_fp128_modf",
    "cudaLaunchAttributeLaunchCompletionEvent",
    "cudaDevAttrReserved124",
    "__cvta_local_to_generic",
    "cudaStreamGetDevice",
    "cudaErrorUnknown",
    "__syncthreads",
    "cudaChannelFormatKindUnsignedBlockCompressed3SRGB",
    "cudaEGLTypedefs",
    "__cvta_constant_to_generic",
    "cudaErrorDuplicateTextureName",
    "cudaGraphKernelNodePortProgrammatic",
    "cudaEventRecordExternal",
    "cudaDevAttrL2CacheSize",
    "cudaLaunchAttributePriority",
    "cudaErrorPeerAccessAlreadyEnabled",
    "cudaClusterSchedulingPolicySpread",
    "cudaGraphGetRootNodes",
    "_64",
    "cudaArrayColorAttachment",
    "cudaStreamDefault",
    "cudaResViewFormatUnsignedChar2",
    "_EXT",
    "cudaResourceDesc",
    "cudaGetDriverEntryPointFlags",
    "cudaDeviceGetDevResource",
    "cudaGraphicsCubeFaceNegativeX",
    "cudaFuncCachePreferShared",
    "cudaD3D9MapFlagsNone",
    "cudaGraphExecUpdateErrorFunctionChanged",
    "cudaEglColorFormatVYUY",
    "_v10000",
    "cudaGraphicsRegisterFlagsWriteDiscard",
    "cudaThreadGetCacheConfig",
    "cudaAtomicCapabilityScalar128",
    "__float2half_rn",
    "cudaCGScopeGrid",
    "__isLocal()",
    "cudaMemcpyDeviceTo",
    "cudaErrorSharedObjectSymbolNotFound",
    "cudaGraphMemFreeNodeGetParams",
    "cudaMemPreftchAsync",
    "__cvta_global_to_generic()",
    "__vimax3_s32",
    "cudaLaunchHostFunc",
    "__viaddmin_s32_relu",
    "cudaGraphKernelNodeCopyAttributes",
    "__vimax3_s16x2",
    "__vibmax_u16x2",
    "cudaVDPAUSetVDPAUDevice",
    "cudaEglColorFormatBayer12BCCR",
    "cudaDeviceDisablePeerAccess",
    "_MAX",
    "cudaD3D11SetDirect3DDevice",
    "cudaErrorInvalidResourceConfiguration",
    "cudaGraphExecEventRecordNodeSetEvent",
    "__log2f",
    "__isConstant",
    "_sync",
    "_rPZrbFgvMZOpS5lclxw",
    "cudaEventDisableTiming",
    "cudaAtomicCapabilityVector32x4",
    "cudaChannelFormatKindUnsignedBlockCompressed1",
    "cudaErrorExternalDevice",
    "__float_to_tf32",
    "_USE",
    "cudaMemcpyAsync",
    "__dadd_",
    "__nv_atomic_fetch_or",
    "cudaArrayGetInfo",
    "cudaGraphKernelNodeFieldEnabled",
    "cudaDevAttrComputeCapabilityMajor",
    "__builtin_assume()",
    "cudaEglColorFormatYVU444SemiPlanar",
    "cudaDevAttrMaxSharedMemoryPerBlockOptin",
    "cudaResViewFormatUnsignedShort1",
    "cudafreeasync",
    "cudaD3D9DeviceListCurrentFrame",
    "_idx0",
    "cudaGraphCondAssignDefault",
    "cudaGraphDebugDotFlagsExtSemasSignalNodeParams",
    "cudaGraphicsResource",
    "cudaGraphInstantiateFlagAutoFreeOnLaunch",
    "cudaResViewFormatUnsignedShort4",
    "cudaFuncSetCacheConfig",
    "cudaDeviceNumaConfigNone",
    "__ballot_sync()",
    "cudaError_t",
    "cudaErrorMapBufferObjectFailed",
    "__stwt",
    "__double_as_longlong",
    "__nv_atomic_fetch_max()",
    "cudaStreamCaptureStatusNone",
    "_proxy",
    "cudaUserObjectRelease",
    "_memory",
    "cudaStreamAttrID",
    "cudaAtomicOperationIntegerAdd",
    "cudaResViewFormatUnsignedInt2",
    "cudaMipmappedArrayGetLevel",
    "cudaD3D10MapFlagsNone",
    "_preserve",
    "cudaEglColorFormatYVU422SemiPlanar",
    "__fma_",
    "cudaVDPAUTypedefs",
    "cudaErrorTextureNotBound",
    "cudaHostFn_t",
    "cudaStreamRecordEvent",
    "cudaFlushGPUDirectRDMAWritesToAllDevices",
    "cudaGraphicsResource_t",
    "__shfl_xor_sync",
    "cudaGraphNodeTypeMemAlloc",
    "cudaErrorLaunchFileScopedSurf",
    "cudaChannelFormatKindSignedNormalized16X2",
    "_MAXSIZE",
    "cudaStreamCallback_t",
    "cudaFlushGPUDirectRDMAWritesOptionMemOps",
    "cudaExternalSemaphoreSignalNodeParamsV2",
    "__syncthreads_count",
    "cudaPeerAccessDefault",
    "cudaErrorLaunchFileScopedTex",
    "__all_sync",
    "cudaPos",
    "__nv_fp128_fdim",
    "cudaAccessPropertyStreaming",
    "cudaDevResourceTypeWorkqueue",
    "cudaErrorNotMappedAsArray",
    "cudaErrorInvalidValue",
    "cudaGraphMemcpyNodeSetParams1D",
    "cudaEglColorFormatY10V10U10_444SemiPlanar",
    "cudaMallocManaged",
    "__nv_atomic_xor",
    "cudaDevAttrMaxTexture2DLinearHeight",
    "cudaJitCacheOptionNone",
    "cudaMemcpyNodeParams",
    "cudaDeviceSetMemPool",
    "cudaDevAttrReserved96"
  ],
  "Keyword/Type": [
    "float4",
    "constant",
    "host",
    "dim3",
    "ptx",
    "uint32_t",
    "uint64_t",
    "device",
    "int4",
    "global",
    "nvcc",
    "shared"
  ],
  "Metric": [
    "occupancy",
    "latency",
    "throughput",
    "bandwidth",
    "efficiency",
    "scalability",
    "utilization"
  ],
  "Tool": [
    "cuda-gdb",
    "visual profiler",
    "nsight compute",
    "debugger",
    "profiler",
    "nvprof"
  ]
}