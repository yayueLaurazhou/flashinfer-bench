# 3.1. A Set of SIMT Multiprocessors

## 3.1. [A Set of SIMT Multiprocessors](https://docs.nvidia.com/cuda/parallel-thread-execution/#set-of-simt-multiprocessors)[](https://docs.nvidia.com/cuda/parallel-thread-execution/#set-of-simt-multiprocessors "Permalink to this headline")

The NVIDIA GPU architecture is built around a scalable array of multithreaded *Streaming
Multiprocessors (SMs)*. When a host program invokes a kernel grid, the blocks of the grid are
enumerated and distributed to multiprocessors with available execution capacity. The threads of a
thread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are
launched on the vacated multiprocessors.

A multiprocessor consists of multiple *Scalar Processor (SP)* cores, a multithreaded instruction
unit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent
threads in hardware with zero scheduling overhead. It implements a single-instruction barrier
synchronization. Fast barrier synchronization together with lightweight thread creation and
zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for
example, a low granularity decomposition of problems by assigning one thread to each data element
(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).

To manage hundreds of threads running several different programs, the multiprocessor employs an
architecture we call *SIMT (single-instruction, multiple-thread)*. The multiprocessor maps each
thread to one scalar processor core, and each scalar thread executes independently with its own
instruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,
and executes threads in groups of parallel threads called *warps*. (This term originates from
weaving, the first parallel thread technology.) Individual threads composing a SIMT warp start
together at the same program address but are otherwise free to branch and execute independently.

When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that
get scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp
contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.

At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues
the next instruction to the active threads of the warp. A warp executes one common instruction at a
time, so full efficiency is realized when all threads of a warp agree on their execution path. If
threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each
branch path taken, disabling threads that are not on that path, and when all paths complete, the
threads converge back to the same execution path. Branch divergence occurs only within a warp;
different warps execute independently regardless of whether they are executing common or disjointed
code paths.

SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a
single instruction controls multiple processing elements. A key difference is that SIMD vector
organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution
and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables
programmers to write thread-level parallel code for independent, scalar threads, as well as
data-parallel code for coordinated threads. For the purposes of correctness, the programmer can
essentially ignore the SIMT behavior; however, substantial performance improvements can be realized
by taking care that the code seldom requires threads in a warp to diverge. In practice, this is
analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when
designing for correctness but must be considered in the code structure when designing for peak
performance. Vector architectures, on the other hand, require the software to coalesce loads into
vectors and manage divergence manually.

How many blocks a multiprocessor can process at once depends on how many registers per thread and
how much shared memory per block are required for a given kernel since the multiprocessor’s
registers and shared memory are split among all the threads of the batch of blocks. If there are not
enough registers or shared memory available per multiprocessor to process at least one block, the
kernel will fail to launch.

![_images/hardware-model.png](./ptx_files/hardware-model.png)


Figure 4 Hardware Model[](https://docs.nvidia.com/cuda/parallel-thread-execution/#set-of-simt-multiprocessors-hardware-model "Permalink to this image")

A set of SIMT multiprocessors with on-chip shared memory.