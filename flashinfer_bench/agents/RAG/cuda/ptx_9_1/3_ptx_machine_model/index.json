{
  "description": "Documentation for 3. PTX Machine Model",
  "directories": [],
  "files": [
    {
      "id": "overview",
      "title": "Overview",
      "filename": "overview.md",
      "tags": [
        "overview"
      ],
      "summary": "Overview of this section."
    },
    {
      "id": "31_a_set_of_simt_multiprocessors",
      "title": "3.1. A Set of SIMT Multiprocessors",
      "tags": [
        "ptx",
        "documentation"
      ],
      "summary": "The NVIDIA GPU architecture is built around a scalable array of multithreaded *Streaming Multiprocessors (SMs)*. When a host program invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block exe...",
      "filename": "31_a_set_of_simt_multiprocessors.md"
    },
    {
      "id": "32_independent_thread_scheduling",
      "title": "3.2. Independent Thread Scheduling",
      "tags": [
        "ptx",
        "documentation"
      ],
      "summary": "On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each o...",
      "filename": "32_independent_thread_scheduling.md"
    },
    {
      "id": "33_on_chip_shared_memory",
      "title": "3.3. On-chip Shared Memory",
      "tags": [
        "ptx",
        "documentation"
      ],
      "summary": "As illustrated by [Figure 4](https://docs.nvidia.com/cuda/parallel-thread-execution/#set-of-simt-multiprocessors-hardware-model), each multiprocessor has on-chip memory of the four following types:",
      "filename": "33_on_chip_shared_memory.md"
    }
  ]
}