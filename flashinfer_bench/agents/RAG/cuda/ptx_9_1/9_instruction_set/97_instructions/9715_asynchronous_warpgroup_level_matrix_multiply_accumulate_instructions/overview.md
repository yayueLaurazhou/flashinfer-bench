# 9.7.15. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions

### 9.7.15. [Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-instructions)[ÔÉÅ](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-instructions "Permalink to this headline")

The warpgroup level matrix multiply and accumulate operation has either of the following forms,
where matrix `D` is called accumulator:

* `D = A * B + D`
* `D = A * B`, where the input from accumulator D is disabled.

The `wgmma` instructions perform warpgroup level matrix multiply-and-accumulate operation by
having all threads in a warpgroup collectively perform the following actions:

1. Load matrices A, B and D into registers or into shared memory.
2. Perform the following `fence` operations:

   * `wgmma.fence` operations to indicate that the register/shared-memory across the warpgroup
     have been written into.
   * `fence.proxy.async` operation to make the generic proxy operations visible to the async
     proxy.
3. Issue the asynchronous matrix multiply and accumulate operations using the `wgmma.mma_async`
   operation on the input matrices. The `wgmma.mma_async` operation is performed in the async
   proxy.
4. Create a wgmma-group and commit all the prior outstanding `wgmma.mma_async` operations into the
   group, by using `wgmma.commit_group` operation.
5. Wait for the completion of the required wgmma-group.
6. Once the wgmma-group completes, all the `wgmma.mma_async` operations have been performed and
   completed.