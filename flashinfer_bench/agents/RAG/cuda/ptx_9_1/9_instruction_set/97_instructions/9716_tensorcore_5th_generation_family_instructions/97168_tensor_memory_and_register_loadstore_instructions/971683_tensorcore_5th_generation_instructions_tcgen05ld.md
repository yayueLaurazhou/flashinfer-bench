# 9.7.16.8.3. Tensorcore 5th Generation Instructions: tcgen05.ld

##### 9.7.16.8.3. [Tensorcore 5th Generation Instructions: `tcgen05.ld`](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-ld)[](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-ld "Permalink to this headline")

`tcgen05.ld`

Asynchronous collective load from tensor memory into registers.

Syntax

```
// Base load instruction:

tcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];

tcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;

.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }
.shape2 = { .16x32bx2 }
.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }
.pack   = { .pack::16b }

// Floating point type load along with reduction :

tcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];

tcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;

// Integer type load along with reduction :

tcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];

tcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;

.shape3 = { .32x32b   }
.shape4 = { .16x32bx2 }
.redOp  = { .min, .max }
.type   = { .u32, .s32 }
```

Copy to clipboard

Description

Instruction `tcgen05.ld` asynchronously loads data from the [Tensor Memory](https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory)
at the location specified by the 32-bit address operand `taddr` into the destination
register `r`, collectively across all threads of the warps.

All the threads in the warp must specify the same value of `taddr`, which must be the
base address of the collective load operation. Otherwise, the behavior is undefined.

The `.shape` qualifier and the `.num` qualifier together determines the total
dimension of the data which is loaded from the [Tensor Memory](https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory). The `.shape`
qualifier indicates the base dimension of data to be accessed as described in the
[Data Movement Shape](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-data-movement-shape). The `.num` qualifier indicates
the repeat factor on the base dimension resulting in the total dimension of the data that
is accessed.

The shape `.16x32bx2` performs two accesses into Tensor Memory of the shape `.16x32b`.
The base address of the first access is specified by taddr and the base address of the
second access is specified by `taddr+immHalfSplitoff`, where `immHalfSplitoff` is an
immediate argument.

The destination operand `r` is a brace-enclosed vector expression consisting of one
or more 32-bit registers as per the value of `.shape` and `.num`. The size of the
vector for various combinations of `.num` and `.shape` is shown in
[Table 49](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-num-shapes-ld).

Table 49 Various-combinations of .num and .shape[](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-num-shapes-ld "Permalink to this table")






| .num | .shape | | |
| --- | --- | --- | --- |
| .16x32bx2 / .16x64b / .32x32b | .16x128b | .16x256b |
| `.x1` | 1 | 2 | 4 |
| `.x2` | 2 | 4 | 8 |
| `.x4` | 4 | 8 | 16 |
| `.x8` | 8 | 16 | 32 |
| `.x16` | 16 | 32 | 64 |
| `.x32` | 32 | 64 | 128 |
| `.x64` | 64 | 128 | NA |
| `.x128` | 128 | NA | NA |

The qualifier `.red` specifies that the reduction operation specified by `.redOp` is
performed on the data that is loaded across columns in each lane. The result of the
reduction operation is written into the corresponding thread’s 32-bit destination register
operand `redVal`. When `.red` qualifier is specified, `.num` modifier must be at least
`.x2`.

The optional qualifier `.pack::16b` can be used to pack two 16-bit elements from adjacent
columns into a single 32-bit element during the load as shown in the section
[Packing and Unpacking](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-tensor-memory-ld-st-packing-unpacking).

The mandatory `.sync` qualifier indicates that `tcgen05.ld` causes the executing thread
to wait until all threads in the warp execute the same `tcgen05.ld` instruction before
resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warp must execute the
same `tcgen05.ld` instruction. In conditionally executed code, a `tcgen05.ld` instruction
should only be used if it is known that all threads in the warp evaluate the condition
identically, otherwise behavior is undefined.

The behavior of `tcgen05.ld` is undefined if all threads do not use the same values of `taddr`,
or if any thread in the warp has exited.

The instruction `tcgen05.ld` is performed asynchronously and more details are specified in the
section [Memory Consistency Model for 5th generation of TensorCore operations](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.6.

`tcgen05.ld.red` is introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

`tcgen05.ld.red` is supported on following architectures:

* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_103f` or higher in the same family
* `sm_110f` or higher in the same family

Examples

```
tcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];

tcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];

tcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},
                                                 redVal, [taddr3], 16;
```

Copy to clipboard