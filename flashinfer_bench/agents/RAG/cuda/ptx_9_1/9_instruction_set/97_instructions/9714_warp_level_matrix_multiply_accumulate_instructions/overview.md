# 9.7.14. Warp Level Matrix Multiply-Accumulate Instructions

### 9.7.14. [Warp Level Matrix Multiply-Accumulate Instructions](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions)[ÔÉÅ](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions "Permalink to this headline")

The matrix multiply and accumulate operation has the following form:

```
D = A * B + C
```

Copy to clipboard

where `D` and `C` are called accumulators and may refer to the same matrix.

PTX provides two ways to perform matrix multiply-and-accumulate computation:

* Using `wmma` instructions:

  + This warp-level computation is performed collectively by all threads in the warp as follows:

    - Load matrices A, B and C from memory into registers using the `wmma.load` operation. When
      the operation completes, the destination registers in each thread hold a fragment of the
      loaded matrix.
    - Perform the matrix multiply and accumulate operation using the `wmma.mma` operation on the
      loaded matrices. When the operation completes, the destination registers in each thread hold
      a fragment of the result matrix returned by the `wmma.mma` operation.
    - Store result Matrix D back to memory using the `wmma.store` operation. Alternately, result
      matrix D can also be used as argument C for a subsequent `wmma.mma` operation.

    The `wmma.load` and `wmma.store` instructions implicitly handle the organization of matrix
    elements when loading the input matrices from memory for the `wmma.mma` operation and when
    storing the result back to memory.
* Using `mma` instruction:

  + Similar to `wmma`, `mma` also requires computation to be performed collectively by all
    threads in the warp however distribution of matrix elements across different threads in warp
    needs to be done explicitly before invoking the `mma` operation. The `mma` instruction
    supports both dense as well as sparse matrix A. The sparse variant can be used when A is a
    structured sparse matrix as described in [Sparse matrix storage](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-sparse-matrix-storage).