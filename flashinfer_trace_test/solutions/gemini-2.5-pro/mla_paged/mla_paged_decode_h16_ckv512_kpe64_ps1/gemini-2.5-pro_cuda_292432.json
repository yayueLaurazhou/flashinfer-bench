{
  "name": "gemini-2.5-pro_cuda_292432",
  "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": [],
    "destination_passing_style": false,
    "binding": "torch"
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#pragma once\n\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda_bf16.h>\n\n// Struct to hold tensor pointers and metadata\nstruct MlaPagedDecodeParams {\n    // Inputs\n    const __nv_bfloat16* q_nope_ptr;\n    const __nv_bfloat16* q_pe_ptr;\n    const __nv_bfloat16* ckv_cache_ptr;\n    const __nv_bfloat16* kpe_cache_ptr;\n    const int* kv_indptr_ptr;\n    const int* kv_indices_ptr;\n    float sm_scale;\n\n    // Outputs\n    __nv_bfloat16* output_ptr;\n    float* lse_ptr;\n\n    // Dimensions\n    int batch_size;\n};\n\n// Host function to launch the CUDA kernel\nvoid mla_paged_decode_launch(const MlaPagedDecodeParams& params, cudaStream_t stream);"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <cmath>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n// --- Constants based on specification ---\nconstexpr int kNumQoHeads = 16;\nconstexpr int kHeadDimCkv = 512;\nconstexpr int kHeadDimKpe = 64;\nconstexpr int kPageSize = 1;\n\n// --- Kernel Tuning Parameters ---\nconstexpr int kBlockThreads = 256;\nconstexpr int kTileK = 16;\nconstexpr int kWarpSize = 32;\n\n// --- Derived Constants ---\nconstexpr int kAccVecsPerThread = kHeadDimCkv / kBlockThreads;\nconstexpr int kNumWarps = kBlockThreads / kWarpSize;\n\n// --- Device Helpers ---\n\n// Warp-level reduction for sum\n__device__ __forceinline__ float warp_reduce_sum(float val, const cg::thread_block_tile<kWarpSize>& warp) {\n    for (int offset = kWarpSize / 2; offset > 0; offset /= 2) {\n        val += warp.shfl_down(val, offset);\n    }\n    return val;\n}\n\n// Warp-level reduction for max\n__device__ __forceinline__ float warp_reduce_max(float val, const cg::thread_block_tile<kWarpSize>& warp) {\n    for (int offset = kWarpSize / 2; offset > 0; offset /= 2) {\n        val = max(val, warp.shfl_down(val, offset));\n    }\n    return val;\n}\n\n// --- Main Kernel Implementation ---\n\n__global__ void __launch_bounds__(kBlockThreads)\nmla_paged_decode_kernel(const MlaPagedDecodeParams params) {\n    const int batch_idx = blockIdx.x;\n    const int head_idx = blockIdx.y;\n\n    // This block is responsible for one query vector (one head of one batch item)\n    if (batch_idx >= params.batch_size) return;\n\n    // --- Shared Memory Allocation ---\n    extern __shared__ char smem[];\n    // Q vectors\n    __nv_bfloat16* q_c_smem = reinterpret_cast<__nv_bfloat16*>(smem);\n    __nv_bfloat16* q_p_smem = q_c_smem + kHeadDimCkv;\n    // K tiles\n    __nv_bfloat16* k_c_tile_smem = q_p_smem + kHeadDimKpe;\n    __nv_bfloat16* k_p_tile_smem = k_c_tile_smem + kTileK * kHeadDimCkv;\n    // Float buffers (aligned)\n    float* float_smem_base = reinterpret_cast<float*>(reinterpret_cast<uintptr_t>(k_p_tile_smem + kTileK * kHeadDimKpe + 3) & ~3);\n    float* logits_smem = float_smem_base;\n    float* attn_smem = logits_smem + kTileK;\n    float* scratch_smem = attn_smem + kTileK; // For broadcast/reduction\n\n    // --- Thread Indexing ---\n    const int thread_id = threadIdx.x;\n    const cg::thread_block block = cg::this_thread_block();\n    const cg::thread_block_tile<kWarpSize> warp = cg::tiled_partition<kWarpSize>(block);\n    const int warp_id = thread_id / kWarpSize;\n    const int lane_id = thread_id % kWarpSize;\n\n    // --- Load Q vectors into Shared Memory ---\n    const __nv_bfloat16* q_c_gmem = params.q_nope_ptr + (batch_idx * kNumQoHeads + head_idx) * kHeadDimCkv;\n    const __nv_bfloat16* q_p_gmem = params.q_pe_ptr + (batch_idx * kNumQoHeads + head_idx) * kHeadDimKpe;\n\n    for (int i = thread_id; i < kHeadDimCkv / 2; i += kBlockThreads) {\n        reinterpret_cast<__nv_bfloat162*>(q_c_smem)[i] = reinterpret_cast<const __nv_bfloat162*>(q_c_gmem)[i];\n    }\n    for (int i = thread_id; i < kHeadDimKpe / 2; i += kBlockThreads) {\n        reinterpret_cast<__nv_bfloat162*>(q_p_smem)[i] = reinterpret_cast<const __nv_bfloat162*>(q_p_gmem)[i];\n    }\n\n    // --- Get KV sequence length for this batch item ---\n    const int page_start_offset = params.kv_indptr_ptr[batch_idx];\n    const int page_end_offset = params.kv_indptr_ptr[batch_idx + 1];\n    const int seq_len = page_end_offset - page_start_offset;\n\n    if (seq_len <= 0) {\n        __nv_bfloat16* out_ptr = params.output_ptr + (batch_idx * kNumQoHeads + head_idx) * kHeadDimCkv;\n        for (int i = thread_id; i < kHeadDimCkv / 2; i += kBlockThreads) {\n            reinterpret_cast<__nv_bfloat162*>(out_ptr)[i] = __float2bfloat162_rn(0.0f);\n        }\n        if (thread_id == 0) {\n            params.lse_ptr[batch_idx * kNumQoHeads + head_idx] = -INFINITY;\n        }\n        return;\n    }\n    block.sync();\n\n    // --- Initialize Accumulators ---\n    float o_acc[kAccVecsPerThread];\n    for (int i = 0; i < kAccVecsPerThread; ++i) o_acc[i] = 0.0f;\n\n    float max_logit = -INFINITY;\n    float sum_exp = 0.0f;\n\n    // --- Main Loop over KV cache in tiles ---\n    for (int tile_offset = 0; tile_offset < seq_len; tile_offset += kTileK) {\n        const int current_tile_size = min(kTileK, seq_len - tile_offset);\n\n        // --- a. Load K tile into Shared Memory (Parallelized) ---\n        for (int i = thread_id; i < current_tile_size * (kHeadDimCkv / 2); i += kBlockThreads) {\n            int k = i / (kHeadDimCkv / 2);\n            int d_idx = i % (kHeadDimCkv / 2);\n            int page_idx = params.kv_indices_ptr[page_start_offset + tile_offset + k];\n            reinterpret_cast<__nv_bfloat162*>(k_c_tile_smem)[k * (kHeadDimCkv / 2) + d_idx] =\n                reinterpret_cast<const __nv_bfloat162*>(params.ckv_cache_ptr + page_idx * kHeadDimCkv)[d_idx];\n        }\n        for (int i = thread_id; i < current_tile_size * (kHeadDimKpe / 2); i += kBlockThreads) {\n            int k = i / (kHeadDimKpe / 2);\n            int d_idx = i % (kHeadDimKpe / 2);\n            int page_idx = params.kv_indices_ptr[page_start_offset + tile_offset + k];\n            reinterpret_cast<__nv_bfloat162*>(k_p_tile_smem)[k * (kHeadDimKpe / 2) + d_idx] =\n                reinterpret_cast<const __nv_bfloat162*>(params.kpe_cache_ptr + page_idx * kHeadDimKpe)[d_idx];\n        }\n        block.sync();\n\n        // --- b. Compute tile logits (Q @ K.T) ---\n        for (int k_outer = 0; k_outer < current_tile_size; k_outer += kNumWarps) {\n            const int k = k_outer + warp_id;\n            if (k < current_tile_size) {\n                float partial_sum = 0.0f;\n                #pragma unroll\n                for (int d = lane_id; d < kHeadDimCkv; d += kWarpSize) {\n                    partial_sum += __bfloat162float(q_c_smem[d]) * __bfloat162float(k_c_tile_smem[k * kHeadDimCkv + d]);\n                }\n                #pragma unroll\n                for (int d = lane_id; d < kHeadDimKpe; d += kWarpSize) {\n                    partial_sum += __bfloat162float(q_p_smem[d]) * __bfloat162float(k_p_tile_smem[k * kHeadDimKpe + d]);\n                }\n                float total_logit = warp_reduce_sum(partial_sum, warp);\n                if (lane_id == 0) logits_smem[k] = total_logit * params.sm_scale;\n            }\n        }\n        block.sync();\n\n        // --- c. Online Softmax update ---\n        float tile_max_logit = -INFINITY;\n        if (warp_id == 0) {\n            float local_max = (lane_id < current_tile_size) ? logits_smem[lane_id] : -INFINITY;\n            tile_max_logit = warp_reduce_max(local_max, warp);\n            if (lane_id == 0) scratch_smem[0] = tile_max_logit;\n        }\n        block.sync();\n        tile_max_logit = scratch_smem[0];\n\n        float old_max_logit = max_logit;\n        max_logit = max(max_logit, tile_max_logit);\n\n        float scale = expf(old_max_logit - max_logit);\n        sum_exp *= scale;\n\n        float tile_sum_exp = 0.0f;\n        if (warp_id == 0) {\n            float local_sum = 0.0f;\n            if (lane_id < current_tile_size) {\n                float val = expf(logits_smem[lane_id] - max_logit);\n                attn_smem[lane_id] = val;\n                local_sum = val;\n            }\n            tile_sum_exp = warp_reduce_sum(local_sum, warp);\n            if (lane_id == 0) scratch_smem[0] = tile_sum_exp;\n        }\n        block.sync();\n        sum_exp += scratch_smem[0];\n\n        // --- d. Update output accumulator (Attn @ V - Parallelized) ---\n        for (int i = 0; i < kAccVecsPerThread; ++i) o_acc[i] *= scale;\n\n        for (int k = 0; k < current_tile_size; ++k) {\n            float attn_k = attn_smem[k];\n            for (int i = 0; i < kAccVecsPerThread; ++i) {\n                int d = thread_id + i * kBlockThreads;\n                if (d < kHeadDimCkv) {\n                    o_acc[i] += attn_k * __bfloat162float(k_c_tile_smem[k * kHeadDimCkv + d]);\n                }\n            }\n        }\n        block.sync();\n    }\n\n    // --- Finalize and Store ---\n    float inv_sum_exp = (sum_exp > 1e-8f) ? 1.0f / sum_exp : 0.0f;\n    __nv_bfloat16* out_ptr = params.output_ptr + (batch_idx * kNumQoHeads + head_idx) * kHeadDimCkv;\n    for (int i = 0; i < kAccVecsPerThread; ++i) {\n        int d = thread_id + i * kBlockThreads;\n        if (d < kHeadDimCkv) {\n            out_ptr[d] = __float2bfloat16(o_acc[i] * inv_sum_exp);\n        }\n    }\n\n    if (thread_id == 0) {\n        const float log2_e = 1.44269504089f;\n        float lse_val = (sum_exp > 1e-8f) ? max_logit * log2_e + log2f(sum_exp) : -INFINITY;\n        params.lse_ptr[batch_idx * kNumQoHeads + head_idx] = lse_val;\n    }\n}\n\nvoid mla_paged_decode_launch(const MlaPagedDecodeParams& params, cudaStream_t stream) {\n    if (params.batch_size == 0) return;\n\n    dim3 grid_dim(params.batch_size, kNumQoHeads);\n    dim3 block_dim(kBlockThreads);\n\n    size_t q_sz = (kHeadDimCkv + kHeadDimKpe) * sizeof(__nv_bfloat16);\n    size_t k_sz = kTileK * (kHeadDimCkv + kHeadDimKpe) * sizeof(__nv_bfloat16);\n    size_t temp_sz = (kTileK + kTileK + 16) * sizeof(float); // logits + attn + scratch + alignment padding\n    size_t smem_size = q_sz + k_sz + temp_sz;\n\n    mla_paged_decode_kernel<<<grid_dim, block_dim, smem_size, stream>>>(params);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"CUDA kernel launch error in mla_paged_decode: %s\\n\", cudaGetErrorString(err));\n    }\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include \"kernel.h\"\n#include <torch/extension.h>\n#include <pybind11/pybind11.h>\n#include <pybind11/stl.h>\n\nnamespace py = pybind11;\n\n// --- Helper for Tensor Validation ---\nvoid validate_tensor(const torch::Tensor& t, const std::string& name, torch::ScalarType dtype, int dims) {\n    TORCH_CHECK(t.is_cuda(), name, \" must be a CUDA tensor\");\n    TORCH_CHECK(t.dtype() == dtype, name, \" must have dtype \", dtype);\n    TORCH_CHECK(t.dim() == dims, name, \" must be \", dims, \"D\");\n    TORCH_CHECK(t.is_contiguous(), name, \" must be contiguous\");\n}\n\n// --- Main 'run' function exposed to Python ---\npy::dict run(\n    torch::Tensor q_nope,\n    torch::Tensor q_pe,\n    torch::Tensor ckv_cache,\n    torch::Tensor kpe_cache,\n    torch::Tensor kv_indptr,\n    torch::Tensor kv_indices,\n    float sm_scale\n) {\n    // --- Input Validation ---\n    validate_tensor(q_nope, \"q_nope\", torch::kBFloat16, 3);\n    validate_tensor(q_pe, \"q_pe\", torch::kBFloat16, 3);\n    validate_tensor(ckv_cache, \"ckv_cache\", torch::kBFloat16, 3);\n    validate_tensor(kpe_cache, \"kpe_cache\", torch::kBFloat16, 3);\n    validate_tensor(kv_indptr, \"kv_indptr\", torch::kInt32, 1);\n    validate_tensor(kv_indices, \"kv_indices\", torch::kInt32, 1);\n\n    const int batch_size = q_nope.size(0);\n    const int num_qo_heads = q_nope.size(1);\n    const int head_dim_ckv = q_nope.size(2);\n    const int head_dim_kpe = q_pe.size(2);\n    const int page_size = ckv_cache.size(1);\n\n    // Check fixed dimensions from spec\n    TORCH_CHECK(num_qo_heads == 16, \"num_qo_heads must be 16, but got \", num_qo_heads);\n    TORCH_CHECK(head_dim_ckv == 512, \"head_dim_ckv must be 512, but got \", head_dim_ckv);\n    TORCH_CHECK(head_dim_kpe == 64, \"head_dim_kpe must be 64, but got \", head_dim_kpe);\n    TORCH_CHECK(page_size == 1, \"page_size must be 1, but got \", page_size);\n\n    // Check constraints\n    TORCH_CHECK(kv_indptr.size(0) == batch_size + 1, \"len_indptr must be batch_size + 1\");\n    // The following check requires synchronization and is expensive.\n    // It is assumed the user provides valid inputs as per spec.\n    // torch::Tensor last_indptr = kv_indptr.index({-1}).to(torch::kCPU);\n    // TORCH_CHECK(kv_indices.size(0) == last_indptr.item<int>(), \"num_kv_indices must equal kv_indptr[-1]\");\n\n    // --- Create Output Tensors ---\n    auto output = torch::empty_like(q_nope);\n    auto lse = torch::empty({batch_size, num_qo_heads}, q_nope.options().dtype(torch::kFloat32));\n\n    // --- Prepare Kernel Parameters ---\n    MlaPagedDecodeParams params;\n    params.q_nope_ptr = reinterpret_cast<const __nv_bfloat16*>(q_nope.data_ptr());\n    params.q_pe_ptr = reinterpret_cast<const __nv_bfloat16*>(q_pe.data_ptr());\n    params.ckv_cache_ptr = reinterpret_cast<const __nv_bfloat16*>(ckv_cache.data_ptr());\n    params.kpe_cache_ptr = reinterpret_cast<const __nv_bfloat16*>(kpe_cache.data_ptr());\n    params.kv_indptr_ptr = kv_indptr.data_ptr<int>();\n    params.kv_indices_ptr = kv_indices.data_ptr<int>();\n    params.sm_scale = sm_scale;\n\n    params.output_ptr = reinterpret_cast<__nv_bfloat16*>(output.data_ptr());\n    params.lse_ptr = lse.data_ptr<float>();\n\n    params.batch_size = batch_size;\n\n    // --- Launch Kernel ---\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    mla_paged_decode_launch(params, stream);\n    \n    // --- Return Results ---\n    py::dict result;\n    result[\"output\"] = output;\n    result[\"lse\"] = lse;\n    return result;\n}\n\n// --- Pybind11 Module Definition ---\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"MLA Paged Decode Kernel (BFLOAT16, Optimized)\",\n          py::arg(\"q_nope\"),\n          py::arg(\"q_pe\"),\n          py::arg(\"ckv_cache\"),\n          py::arg(\"kpe_cache\"),\n          py::arg(\"kv_indptr\"),\n          py::arg(\"kv_indices\"),\n          py::arg(\"sm_scale\"));\n}"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for mla_paged_decode_h16_ckv512_kpe64_ps1 (round 10)"
}
