{
  "name": "gemini-2.5-pro_triton_dorbxs",
  "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": [],
    "destination_passing_style": false
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_L': 32, 'BLOCK_DCKV': 64, 'BLOCK_DKPE': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_L': 64, 'BLOCK_DCKV': 64, 'BLOCK_DKPE': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_L': 32, 'BLOCK_DCKV': 128, 'BLOCK_DKPE': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_L': 64, 'BLOCK_DCKV': 128, 'BLOCK_DKPE': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_L': 128, 'BLOCK_DCKV': 128, 'BLOCK_DKPE': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_L': 32, 'BLOCK_DCKV': 256, 'BLOCK_DKPE': 64}, num_warps=8, num_stages=2),\n    ],\n    key=['HEAD_DIM_CKV', 'HEAD_DIM_KPE'],\n)\n@triton.jit\ndef mla_paged_decode_h16_ckv512_kpe64_ps1_kernel(\n    # Pointers to tensors\n    q_nope_ptr, q_pe_ptr, ckv_cache_ptr, kpe_cache_ptr,\n    kv_indptr_ptr, kv_indices_ptr,\n    output_ptr, lse_ptr,\n    # Scalar inputs\n    sm_scale,\n    # Strides\n    q_nope_stride_bs, q_nope_stride_h,\n    q_pe_stride_bs, q_pe_stride_h,\n    ckv_cache_stride_n,\n    kpe_cache_stride_n,\n    output_stride_bs, output_stride_h,\n    lse_stride_bs, lse_stride_h,\n    # Compile-time constants\n    HEAD_DIM_CKV: tl.constexpr,\n    HEAD_DIM_KPE: tl.constexpr,\n    # Tuning parameters\n    BLOCK_L: tl.constexpr,\n    BLOCK_DCKV: tl.constexpr,\n    BLOCK_DKPE: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for paged multi-level attention decode.\n    Each program instance computes one head for one batch element.\n    \"\"\"\n    # Grid computes (batch_size, num_qo_heads)\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    log2 = 1.4426950408889634  # 1.0 / math.log(2.0)\n\n    # 1. --- Get sequence length for this batch element ---\n    page_beg = tl.load(kv_indptr_ptr + b_idx)\n    page_end = tl.load(kv_indptr_ptr + b_idx + 1)\n    L_tokens = page_end - page_beg\n\n    # 2. --- Initialize pointers and accumulators ---\n    q_nope_ptr += b_idx * q_nope_stride_bs + h_idx * q_nope_stride_h\n    q_pe_ptr += b_idx * q_pe_stride_bs + h_idx * q_pe_stride_h\n    output_ptr += b_idx * output_stride_bs + h_idx * output_stride_h\n    lse_ptr += b_idx * lse_stride_bs + h_idx * lse_stride_h\n\n    m_i = -float('inf')\n    l_i = 0.0\n\n    NUM_CHUNKS = HEAD_DIM_CKV // BLOCK_DCKV\n    acc0 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc1 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc2 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc3 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc4 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc5 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc6 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n    acc7 = tl.zeros([BLOCK_DCKV], dtype=tl.float32)\n\n    # 3. --- Handle empty sequences ---\n    if L_tokens <= 0:\n        out_dtype = output_ptr.dtype.element_ty\n        zero_chunk = tl.zeros([BLOCK_DCKV], dtype=tl.float32).to(out_dtype)\n        for i in range(NUM_CHUNKS):\n            tl.store(output_ptr + i * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), zero_chunk)\n        tl.store(lse_ptr, m_i)\n        return\n\n    # 4. --- Main loop over the KV sequence in blocks ---\n    for l_start in range(0, L_tokens, BLOCK_L):\n        l_offs = l_start + tl.arange(0, BLOCK_L)\n        l_mask = l_offs < L_tokens\n        indices = tl.load(kv_indices_ptr + page_beg + l_offs, mask=l_mask, other=0)\n\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n\n        # Contribution from kpe\n        for d_start in range(0, HEAD_DIM_KPE, BLOCK_DKPE):\n            d_offs = d_start + tl.arange(0, BLOCK_DKPE)\n            q_pe_tile = tl.load(q_pe_ptr + d_offs)\n            k_kpe_ptrs = kpe_cache_ptr + indices[:, None] * kpe_cache_stride_n + d_offs[None, :]\n            k_kpe_tile = tl.load(k_kpe_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_pe_tile to 2D for tl.dot, then squeeze result\n            q_pe_tile_2d = tl.reshape(q_pe_tile, (BLOCK_DKPE, 1))\n            s_update = tl.dot(k_kpe_tile, q_pe_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n\n        # --- Online softmax update ---\n        s_block = tl.where(l_mask, s_block * sm_scale, -float('inf'))\n        m_i_old = m_i\n        m_i = tl.maximum(m_i, tl.max(s_block, axis=0))\n\n        # NUMERICAL STABILITY: guard against nan from exp(-inf - (-inf))\n        s_block_shifted = s_block - m_i\n        s_block_shifted = tl.where(m_i == -float('inf'), -float('inf'), s_block_shifted)\n        p_block = tl.exp(s_block_shifted)\n\n        l_i_new = tl.sum(p_block, axis=0)\n        \n        alpha = tl.exp(m_i_old - m_i)\n        # NUMERICAL STABILITY: if m_i_old == m_i, alpha should be 1.0. Handles -inf case.\n        alpha = tl.where(m_i_old == m_i, 1.0, alpha)\n        \n        l_i = alpha * l_i + l_i_new\n        p_block = p_block.to(ckv_cache_ptr.dtype.element_ty)\n\n        # --- Update output accumulator ---\n        if NUM_CHUNKS == 8:\n            acc0 *= alpha; acc1 *= alpha; acc2 *= alpha; acc3 *= alpha\n            acc4 *= alpha; acc5 *= alpha; acc6 *= alpha; acc7 *= alpha\n        elif NUM_CHUNKS == 4:\n            acc0 *= alpha; acc1 *= alpha; acc2 *= alpha; acc3 *= alpha\n        elif NUM_CHUNKS == 2:\n            acc0 *= alpha; acc1 *= alpha\n\n        # Add contribution from the current block (p_block @ v_block)\n        for i in range(NUM_CHUNKS):\n            d_offs = i * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV)\n            v_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            v_ckv_tile = tl.load(v_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D p_block to 2D for tl.dot, then squeeze result\n            p_block_2d = tl.reshape(p_block, (1, BLOCK_L))\n            update_2d = tl.dot(p_block_2d, v_ckv_tile)\n            update = tl.squeeze(update_2d, axis=0)\n            if i == 0: acc0 += update\n            elif i == 1: acc1 += update\n            elif i == 2: acc2 += update\n            elif i == 3: acc3 += update\n            elif i == 4: acc4 += update\n            elif i == 5: acc5 += update\n            elif i == 6: acc6 += update\n            elif i == 7: acc7 += update\n\n    # 5. --- Finalize and store results ---\n    l_i_reciprocal = tl.where(l_i > 0.0, 1.0 / l_i, 0.0)\n    out_dtype = output_ptr.dtype.element_ty\n    if NUM_CHUNKS == 8:\n        tl.store(output_ptr + 0 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc0 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 1 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc1 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 2 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc2 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 3 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc3 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 4 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc4 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 5 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc5 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 6 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc6 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 7 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc7 * l_i_reciprocal).to(out_dtype))\n    elif NUM_CHUNKS == 4:\n        tl.store(output_ptr + 0 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc0 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 1 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc1 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 2 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc2 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 3 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc3 * l_i_reciprocal).to(out_dtype))\n    elif NUM_CHUNKS == 2:\n        tl.store(output_ptr + 0 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc0 * l_i_reciprocal).to(out_dtype))\n        tl.store(output_ptr + 1 * BLOCK_DCKV + tl.arange(0, BLOCK_DCKV), (acc1 * l_i_reciprocal).to(out_dtype))\n\n    final_lse = (m_i + tl.log(l_i)) * log2\n    # handle case where l_i is 0, which makes tl.log(l_i) -> -inf\n    final_lse = tl.where(l_i > 0.0, final_lse, -float('inf'))\n    tl.store(lse_ptr, final_lse)\n\n\ndef mla_paged_decode_h16_ckv512_kpe64_ps1(q_nope, q_pe, ckv_cache, kpe_cache, kv_indptr, kv_indices, sm_scale):\n    \"\"\"\n    Wrapper function for the Triton kernel.\n    Handles device management, grid computation, and kernel launch.\n    \"\"\"\n    # 1. --- Check inputs and constants ---\n    batch_size, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n\n    assert num_qo_heads == 16, \"num_qo_heads must be 16\"\n    assert head_dim_ckv == 512, \"head_dim_ckv must be 512\"\n    assert head_dim_kpe == 64, \"head_dim_kpe must be 64\"\n    assert ckv_cache.shape[1] == 1, \"page_size must be 1\"\n\n    # 2. --- Device Management ---\n    input_device = q_nope.device\n    is_cpu = input_device.type == 'cpu'\n    if is_cpu:\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available, but input tensors are on CPU.\")\n        q_nope = q_nope.cuda()\n        q_pe = q_pe.cuda()\n        ckv_cache = ckv_cache.cuda()\n        kpe_cache = kpe_cache.cuda()\n        kv_indptr = kv_indptr.cuda()\n        kv_indices = kv_indices.cuda()\n\n    # 3. --- Prepare outputs and grid ---\n    output = torch.empty_like(q_nope)\n    lse = torch.empty((batch_size, num_qo_heads), dtype=torch.float32, device=q_nope.device)\n\n    ckv_cache_squeezed = ckv_cache.squeeze(1)\n    kpe_cache_squeezed = kpe_cache.squeeze(1)\n\n    grid = (batch_size, num_qo_heads)\n\n    # 4. --- Launch kernel ---\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n        q_nope, q_pe, ckv_cache_squeezed, kpe_cache_squeezed,\n        kv_indptr, kv_indices,\n        output, lse,\n        sm_scale,\n        q_nope.stride(0), q_nope.stride(1),\n        q_pe.stride(0), q_pe.stride(1),\n        ckv_cache_squeezed.stride(0),\n        kpe_cache_squeezed.stride(0),\n        output.stride(0), output.stride(1),\n        lse.stride(0), lse.stride(1),\n        HEAD_DIM_CKV=head_dim_ckv,\n        HEAD_DIM_KPE=head_dim_kpe,\n    )\n\n    # 5. --- Restore device and return ---\n    if is_cpu:\n        output = output.to(input_device)\n        lse = lse.to(input_device)\n\n    return {\"output\": output, \"lse\": lse}\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Public entry point. Handles both args and kwargs for flexibility.\n    \"\"\"\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for mla_paged_decode_h16_ckv512_kpe64_ps1 (round 10)"
}
