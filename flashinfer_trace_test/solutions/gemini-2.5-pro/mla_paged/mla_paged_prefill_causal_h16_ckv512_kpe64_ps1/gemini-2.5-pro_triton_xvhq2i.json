{
  "name": "gemini-2.5-pro_triton_xvhq2i",
  "definition": "mla_paged_prefill_causal_h16_ckv512_kpe64_ps1",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": [],
    "destination_passing_style": false
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\nimport inspect\n\n#\n# Triton kernel for paged prefill attention\n#\n# This kernel is optimized for a specific attention variant:\n# - Causal attention for prefill (each query attends to keys up to its own position).\n# - Paged KV cache with page_size = 1, meaning each entry in kv_indices points to a single token's KV state.\n# - Mixed-Logit Attention: Logits are computed from two separate dot products, one for the main content (ckv) and one for positional embeddings (kpe).\n#   `logits = (q_nope @ K_ckv.T) + (q_pe @ K_kpe.T)`\n# - It computes the attention output and the 2-based log-sum-exp (LSE) of the logits for stable backward passes.\n#\n# Grid:\n# - The grid is 2D: (total_q, num_qo_heads).\n# - Each program instance computes the attention output for a single query token and a single head.\n#\n# Optimization Strategy:\n# - Correctness First: The reference `torch.softmax` is a base-e operation. To compute this correctly while using Triton's fast base-2 intrinsics (`tl.exp2`, `tl.log2`), the logits are scaled by `log(2)`. This is based on the identity `softmax_e(x) == softmax_2(x * log(2))`. This ensures numerical alignment with the reference implementation.\n# - Two-Pass Stability: A two-pass approach ensures numerical stability for long sequences.\n#   1. Pass 1 computes the true base-2 log-sum-exp (LSE) using a stable online algorithm.\n#   2. Pass 2 re-computes logits and uses the LSE from Pass 1 to calculate the final attention probabilities and output vector.\n# - B200 Optimization: The kernel is tuned with block sizes and parallelization settings (num_warps, num_stages) that are effective on modern architectures. It uses `num_stages` > 1 to pipeline memory loads and compute.\n# - Online Softmax: The kernel uses an online (one-pass) softmax algorithm within each pass to handle variable sequence lengths without materializing a large attention matrix.\n# - Blocked Computation: All loops over sequence length and head dimensions are blocked to improve data locality.\n#\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_CKV': 64, 'BLOCK_KPE': 64, 'BLOCK_KV': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_CKV': 64, 'BLOCK_KPE': 64, 'BLOCK_KV': 128}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_CKV': 128, 'BLOCK_KPE': 64, 'BLOCK_KV': 64}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_CKV': 64, 'BLOCK_KPE': 64, 'BLOCK_KV': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_CKV': 32, 'BLOCK_KPE': 32, 'BLOCK_KV': 128}, num_stages=3, num_warps=4),\n    ],\n    key=['total_q', 'num_kv_indices'],\n)\n@triton.jit\ndef _mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel(\n    # Inputs\n    q_nope_ptr, q_pe_ptr, ckv_cache_ptr, kpe_cache_ptr,\n    qo_indptr_ptr, kv_indptr_ptr, kv_indices_ptr, q_to_batch_idx_ptr,\n    sm_scale,\n\n    # Outputs\n    output_ptr, lse_ptr,\n\n    # Strides\n    stride_q_total_q, stride_q_num_heads, stride_q_head_dim_ckv,\n    stride_qpe_total_q, stride_qpe_num_heads, stride_qpe_head_dim_kpe,\n    stride_ckv_pages, stride_ckv_page_size, stride_ckv_head_dim,\n    stride_kpe_pages, stride_kpe_page_size, stride_kpe_head_dim,\n    stride_out_total_q, stride_out_num_heads, stride_out_head_dim,\n    stride_lse_total_q, stride_lse_num_heads,\n\n    # Axes\n    total_q: tl.constexpr,\n    num_pages: tl.constexpr,\n    len_indptr: tl.constexpr,\n    num_kv_indices: tl.constexpr,\n\n    # Constants\n    NUM_QO_HEADS: tl.constexpr,\n    HEAD_DIM_CKV: tl.constexpr,\n    HEAD_DIM_KPE: tl.constexpr,\n    PAGE_SIZE: tl.constexpr,\n    LOG2_E: tl.constexpr,\n\n    # Autotune configs\n    BLOCK_CKV: tl.constexpr,\n    BLOCK_KPE: tl.constexpr,\n    BLOCK_KV: tl.constexpr,\n):\n    # =========================================================================\n    # 1. Program and Grid Setup\n    # =========================================================================\n    pid_qt = tl.program_id(0)\n    pid_h = tl.program_id(1)\n\n    if pid_qt >= total_q:\n        return\n\n    # =========================================================================\n    # 2. Determine Sequence Boundaries\n    # =========================================================================\n    batch_idx = tl.load(q_to_batch_idx_ptr + pid_qt)\n    q_start = tl.load(qo_indptr_ptr + batch_idx)\n    kv_pages_start = tl.load(kv_indptr_ptr + batch_idx)\n    kv_pages_end = tl.load(kv_indptr_ptr + batch_idx + 1)\n    kv_len = kv_pages_end - kv_pages_start\n\n    if kv_len == 0:\n        out_ptr_base = output_ptr + pid_qt * stride_out_total_q + pid_h * stride_out_num_heads\n        offs_dh = tl.arange(0, BLOCK_CKV)\n        # Iterate over output head dim to zero out the full vector\n        for ckv_off in range(0, HEAD_DIM_CKV, BLOCK_CKV):\n            mask = (ckv_off + offs_dh) < HEAD_DIM_CKV\n            tl.store(out_ptr_base + ckv_off + offs_dh, tl.zeros((BLOCK_CKV,), dtype=tl.bfloat16), mask=mask)\n        lse_val_ptr = lse_ptr + pid_qt * stride_lse_total_q + pid_h * stride_lse_num_heads\n        tl.store(lse_val_ptr, -float('inf'))\n        return\n\n    q_end = tl.load(qo_indptr_ptr + batch_idx + 1)\n    q_len = q_end - q_start\n    prefix_len = kv_len - q_len\n    q_idx_in_seq = pid_qt - q_start\n    abs_pos_q = prefix_len + q_idx_in_seq\n\n    q_nope_offset = pid_qt * stride_q_total_q + pid_h * stride_q_num_heads\n    q_pe_offset = pid_qt * stride_qpe_total_q + pid_h * stride_qpe_num_heads\n\n    # =========================================================================\n    # 3. Pass 1: Compute LSE (Log-Sum-Exp)\n    # =========================================================================\n    m_i = -float(\"inf\")\n    l_i = 0.0\n\n    for kv_block_start in range(0, kv_len, BLOCK_KV):\n        offs_kv_indices = kv_pages_start + kv_block_start + tl.arange(0, BLOCK_KV)\n        mask_kv_indices = offs_kv_indices < kv_pages_end\n        page_indices = tl.load(kv_indices_ptr + offs_kv_indices, mask=mask_kv_indices, other=0)\n\n        logits = tl.zeros([BLOCK_KV], dtype=tl.float32)\n\n        # CKV component\n        for ckv_off in range(0, HEAD_DIM_CKV, BLOCK_CKV):\n            offs_d_ckv = ckv_off + tl.arange(0, BLOCK_CKV)\n            mask_d_ckv = offs_d_ckv < HEAD_DIM_CKV\n            q_nope_fragment = tl.load(q_nope_ptr + q_nope_offset + offs_d_ckv, mask=mask_d_ckv, other=0.0).to(tl.float32)\n            k_ckv = tl.load(ckv_cache_ptr + page_indices[:, None] * stride_ckv_pages + offs_d_ckv[None, :],\n                            mask=mask_kv_indices[:, None] & mask_d_ckv[None, :], other=0.0).to(tl.float32)\n            logits += tl.sum(q_nope_fragment[None, :] * k_ckv, axis=1)\n\n        # KPE component\n        for kpe_off in range(0, HEAD_DIM_KPE, BLOCK_KPE):\n            offs_d_kpe = kpe_off + tl.arange(0, BLOCK_KPE)\n            mask_d_kpe = offs_d_kpe < HEAD_DIM_KPE\n            q_pe_fragment = tl.load(q_pe_ptr + q_pe_offset + offs_d_kpe, mask=mask_d_kpe, other=0.0).to(tl.float32)\n            k_kpe = tl.load(kpe_cache_ptr + page_indices[:, None] * stride_kpe_pages + offs_d_kpe[None, :],\n                            mask=mask_kv_indices[:, None] & mask_d_kpe[None, :], other=0.0).to(tl.float32)\n            logits += tl.sum(q_pe_fragment[None, :] * k_kpe, axis=1)\n\n        logits *= sm_scale\n        # Scale logits by log(2) to compute base-e softmax using base-2 instructions.\n        # softmax_e(x) == softmax_2(x * log2(e))\n        logits *= LOG2_E\n\n        kv_seq_indices = kv_block_start + tl.arange(0, BLOCK_KV)\n        causal_mask = kv_seq_indices <= abs_pos_q\n        final_mask = mask_kv_indices & causal_mask\n        logits = tl.where(final_mask, logits, -float(\"inf\"))\n\n        m_i_new = tl.maximum(m_i, tl.max(logits, axis=0))\n        p = tl.exp2(logits - m_i_new)\n        l_i_new = tl.exp2(m_i - m_i_new) * l_i + tl.sum(p, axis=0)\n        m_i = m_i_new\n        l_i = l_i_new\n\n    # Final LSE is log2(sum(exp(original_logits * sm_scale)))\n    lse_val = m_i + tl.log2(l_i)\n    lse_val_ptr = lse_ptr + pid_qt * stride_lse_total_q + pid_h * stride_lse_num_heads\n    tl.store(lse_val_ptr, lse_val)\n\n    # =========================================================================\n    # 4. Pass 2: Compute Attention Output\n    # =========================================================================\n    out_ptr_base = output_ptr + pid_qt * stride_out_total_q + pid_h * stride_out_num_heads\n    # This pass iterates over the output dimension to keep the accumulator in registers.\n    for ckv_out_offset in range(0, HEAD_DIM_CKV, BLOCK_CKV):\n        acc = tl.zeros([BLOCK_CKV], dtype=tl.float32)\n\n        # Loop over KV sequence again\n        for kv_block_start in range(0, kv_len, BLOCK_KV):\n            offs_kv_indices = kv_pages_start + kv_block_start + tl.arange(0, BLOCK_KV)\n            mask_kv_indices = offs_kv_indices < kv_pages_end\n            page_indices = tl.load(kv_indices_ptr + offs_kv_indices, mask=mask_kv_indices, other=0)\n\n            # Re-compute logits\n            logits = tl.zeros([BLOCK_KV], dtype=tl.float32)\n            for ckv_off in range(0, HEAD_DIM_CKV, BLOCK_CKV):\n                offs_d_ckv = ckv_off + tl.arange(0, BLOCK_CKV)\n                mask_d_ckv = offs_d_ckv < HEAD_DIM_CKV\n                q_nope_fragment = tl.load(q_nope_ptr + q_nope_offset + offs_d_ckv, mask=mask_d_ckv, other=0.0).to(tl.float32)\n                k_ckv = tl.load(ckv_cache_ptr + page_indices[:, None] * stride_ckv_pages + offs_d_ckv[None, :],\n                                mask=mask_kv_indices[:, None] & mask_d_ckv[None, :], other=0.0).to(tl.float32)\n                logits += tl.sum(q_nope_fragment[None, :] * k_ckv, axis=1)\n\n            for kpe_off in range(0, HEAD_DIM_KPE, BLOCK_KPE):\n                offs_d_kpe = kpe_off + tl.arange(0, BLOCK_KPE)\n                mask_d_kpe = offs_d_kpe < HEAD_DIM_KPE\n                q_pe_fragment = tl.load(q_pe_ptr + q_pe_offset + offs_d_kpe, mask=mask_d_kpe, other=0.0).to(tl.float32)\n                k_kpe = tl.load(kpe_cache_ptr + page_indices[:, None] * stride_kpe_pages + offs_d_kpe[None, :],\n                                mask=mask_kv_indices[:, None] & mask_d_kpe[None, :], other=0.0).to(tl.float32)\n                logits += tl.sum(q_pe_fragment[None, :] * k_kpe, axis=1)\n\n            logits *= sm_scale\n            logits *= LOG2_E # Re-apply scaling for base-2 probability calculation\n\n            kv_seq_indices = kv_block_start + tl.arange(0, BLOCK_KV)\n            causal_mask = kv_seq_indices <= abs_pos_q\n            final_mask = mask_kv_indices & causal_mask\n            logits = tl.where(final_mask, logits, -float(\"inf\"))\n\n            # Compute attention probabilities using the final LSE from Pass 1\n            p = tl.exp2(logits - lse_val)\n\n            # Load V block for the current output slice and update accumulator\n            offs_v_ckv = ckv_out_offset + tl.arange(0, BLOCK_CKV)\n            mask_v_ckv = offs_v_ckv < HEAD_DIM_CKV\n            v_ckv = tl.load(ckv_cache_ptr + page_indices[:, None] * stride_ckv_pages + offs_v_ckv[None, :],\n                            mask=mask_kv_indices[:, None] & mask_v_ckv[None, :], other=0.0)\n\n            p = p.to(v_ckv.dtype)\n            acc += tl.sum(p[:, None] * v_ckv, axis=0)\n\n        # Store this block of the output vector\n        offs_out = ckv_out_offset + tl.arange(0, BLOCK_CKV)\n        mask_out = offs_out < HEAD_DIM_CKV\n        tl.store(out_ptr_base + offs_out, acc.to(tl.bfloat16), mask=mask_out)\n\ndef _get_sig_bound_args(fn, args, kwargs):\n    \"\"\"Binds `args` and `kwargs` to the signature of `fn`.\"\"\"\n    sig = inspect.signature(fn)\n    bound_args = sig.bind(*args, **kwargs)\n    bound_args.apply_defaults()\n    return bound_args.arguments\n\ndef _forward(q_nope, q_pe, ckv_cache, kpe_cache, qo_indptr, kv_indptr, kv_indices, sm_scale):\n    # Shape checks and constants\n    total_q, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    num_pages, page_size, _ = ckv_cache.shape\n    len_indptr = qo_indptr.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n    batch_size = len_indptr - 1\n\n    # Assertions for fixed dimensions\n    assert num_qo_heads == 16, f\"Expected num_qo_heads=16, got {num_qo_heads}\"\n    assert head_dim_ckv == 512, f\"Expected head_dim_ckv=512, got {head_dim_ckv}\"\n    assert head_dim_kpe == 64, f\"Expected head_dim_kpe=64, got {head_dim_kpe}\"\n    assert page_size == 1, f\"Expected page_size=1, got {page_size}\"\n\n    # Create output tensors\n    output = torch.empty_like(q_nope)\n    lse = torch.empty((total_q, num_qo_heads), dtype=torch.float32, device=q_nope.device)\n\n    # Pre-compute a mapping from query token index to its batch index\n    q_to_batch_idx = torch.zeros(total_q, dtype=torch.int32, device=q_nope.device)\n    if total_q > 0 and batch_size > 0:\n        q_starts = qo_indptr[:-1].long()\n        q_ends = qo_indptr[1:].long()\n        for i in range(batch_size):\n            q_to_batch_idx[q_starts[i]:q_ends[i]] = i\n\n    # Grid for kernel launch\n    grid = (total_q, num_qo_heads)\n\n    # Call the Triton kernel\n    _mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel[grid](\n        q_nope, q_pe, ckv_cache, kpe_cache,\n        qo_indptr, kv_indptr, kv_indices, q_to_batch_idx,\n        sm_scale,\n        output, lse,\n        # Strides\n        q_nope.stride(0), q_nope.stride(1), q_nope.stride(2),\n        q_pe.stride(0), q_pe.stride(1), q_pe.stride(2),\n        ckv_cache.stride(0), ckv_cache.stride(1), ckv_cache.stride(2),\n        kpe_cache.stride(0), kpe_cache.stride(1), kpe_cache.stride(2),\n        output.stride(0), output.stride(1), output.stride(2),\n        lse.stride(0), lse.stride(1),\n        # Axes\n        total_q, num_pages, len_indptr, num_kv_indices,\n        # Constants\n        NUM_QO_HEADS=num_qo_heads,\n        HEAD_DIM_CKV=head_dim_ckv,\n        HEAD_DIM_KPE=head_dim_kpe,\n        PAGE_SIZE=page_size,\n        LOG2_E=math.log2(math.e),\n    )\n\n    return output, lse\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Wrapper function for the paged prefill attention kernel.\n    Handles device management and argument binding.\n    \"\"\"\n    bound_args = _get_sig_bound_args(_forward, args, kwargs)\n\n    # Extract tensors from bound arguments\n    input_tensors_names = ['q_nope', 'q_pe', 'ckv_cache', 'kpe_cache', 'qo_indptr', 'kv_indptr', 'kv_indices']\n    input_tensors = [bound_args[name] for name in input_tensors_names]\n\n    original_device = input_tensors[0].device\n    is_cpu = original_device.type == 'cpu'\n\n    if is_cpu:\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available, but required for Triton kernel execution from CPU tensors.\")\n        gpu_tensors = [t.cuda() for t in input_tensors]\n        for name, tensor in zip(input_tensors_names, gpu_tensors):\n            bound_args[name] = tensor\n    elif not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available, but input tensors are on a CUDA device.\")\n\n    # Execute Forward Pass with potentially moved tensors\n    output, lse = _forward(**bound_args)\n\n    # Restore Original Device if necessary\n    if is_cpu:\n        output = output.to(original_device)\n        lse = lse.to(original_device)\n\n    return output, lse"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for mla_paged_prefill_causal_h16_ckv512_kpe64_ps1 (round 10)"
}
