{
  "name": "gemini-2.5-pro_cuda_a7821e",
  "definition": "mla_paged_prefill_causal_h16_ckv512_kpe64_ps1",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": [],
    "destination_passing_style": false,
    "binding": "torch"
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#ifndef MLA_PAGED_PREFILL_CAUSAL_H\n#define MLA_PAGED_PREFILL_CAUSAL_H\n\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda_bf16.h>\n\n/**\n * @brief Launches the CUDA kernel for paged prefill attention with causal masking.\n *\n * This function serves as the C++ interface to the CUDA kernel. It is responsible for\n * setting up kernel launch parameters, including grid and block dimensions and dynamic\n * shared memory size. It passes tensor data pointers from PyTorch to the CUDA kernel\n * for execution on the specified stream.\n *\n * @param output Output tensor of shape [total_q, num_qo_heads, head_dim_ckv].\n * @param lse Log-sum-exp tensor of shape [total_q, num_qo_heads].\n * @param q_nope Query tensor component without positional encoding.\n * @param q_pe Query tensor component with positional encoding.\n * @param ckv_cache Compressed key-value cache (acts as both K and V).\n * @param kpe_cache Key positional encoding cache.\n * @param qo_indptr Indirection pointer for query tokens per sequence.\n * @param kv_indptr Indirection pointer for KV cache pages per sequence.\n * @param kv_indices Page indices for the KV cache.\n * @param q_to_b_map A precomputed map from a query's absolute index to its batch index.\n * @param sm_scale The scale factor for the softmax operation.\n * @param stream The CUDA stream for asynchronous execution.\n */\nvoid launch_mla_paged_prefill_causal(\n    torch::Tensor output,\n    torch::Tensor lse,\n    torch::Tensor q_nope,\n    torch::Tensor q_pe,\n    torch::Tensor ckv_cache,\n    torch::Tensor kpe_cache,\n    torch::Tensor qo_indptr,\n    torch::Tensor kv_indptr,\n    torch::Tensor kv_indices,\n    torch::Tensor q_to_b_map,\n    float sm_scale,\n    cudaStream_t stream);\n\n#endif // MLA_PAGED_PREFILL_CAUSAL_H"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n\n#include <cfloat>\n#include <cmath>\n\n// Kernel-specific constants derived from the specification.\n// Hardcoding these allows the compiler to optimize indexing and loop bounds.\nconstexpr int kNumHeads = 16;\nconstexpr int kHeadDimCkv = 512;\nconstexpr int kHeadDimKpe = 64;\nconstexpr int kPageSize = 1;\nconstexpr int kBlockSize = 256;\nconstexpr int kWarpsPerBlock = kBlockSize / 32;\n// Tile size for K/V vectors, processed in shared memory. Set to the number of warps.\nconstexpr int kTileK = kWarpsPerBlock;\n// Number of CKV vector elements each thread is responsible for in the final accumulation.\nconstexpr int kCkvVecsPerThread = kHeadDimCkv / kBlockSize; // 512 / 256 = 2\nstatic_assert(kHeadDimCkv % kBlockSize == 0, \"kHeadDimCkv must be divisible by kBlockSize\");\n\n// --- Warp and Block Reduction Utilities ---\n// These functions use efficient CUDA intrinsics (__shfl_down_sync) for fast\n// parallel reductions, crucial for the softmax computation.\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset /= 2) {\n        // Use fmaxf for explicit float max to avoid compiler ambiguity with std::max.\n        // This is a common fix for CUDA compile errors.\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    }\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_max(float val, float* shared_mem) {\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n\n    val = warp_reduce_max(val);\n\n    if (lane_id == 0) {\n        shared_mem[warp_id] = val;\n    }\n    __syncthreads();\n\n    val = (tid < kWarpsPerBlock) ? shared_mem[tid] : -FLT_MAX;\n    if (warp_id == 0) {\n        val = warp_reduce_max(val);\n    }\n\n    if (tid == 0) {\n        shared_mem[0] = val;\n    }\n    __syncthreads();\n    return shared_mem[0];\n}\n\n/**\n * @brief Main CUDA kernel for paged prefill attention.\n *\n * Grid: (total_q, num_heads) -> Each block computes one query-head pair.\n * Block: (256 threads)\n *\n * Implements a two-pass \"online softmax\" for numerical stability:\n * 1. Pass 1: Iterate through key vectors to find the maximum logit score.\n * 2. Pass 2: Recompute logits, scale by the max logit, compute softmax probabilities,\n *    and perform a weighted sum of value vectors.\n *\n * Shared memory is used to cache the Q vector and tiles of K vectors to minimize\n * global memory access. Causal masking is implemented efficiently by adjusting\n * the loop bounds to only consider valid past keys.\n */\n__global__ void mla_paged_prefill_causal_kernel(\n    __nv_bfloat16* __restrict__ output,\n    float* __restrict__ lse,\n    const __nv_bfloat16* __restrict__ q_nope,\n    const __nv_bfloat16* __restrict__ q_pe,\n    const __nv_bfloat16* __restrict__ ckv_cache,\n    const __nv_bfloat16* __restrict__ kpe_cache,\n    const int32_t* __restrict__ qo_indptr,\n    const int32_t* __restrict__ kv_indptr,\n    const int32_t* __restrict__ kv_indices,\n    const int32_t* __restrict__ q_to_b_map,\n    const float sm_scale)\n{\n    const int q_idx = blockIdx.x;\n    const int head_idx = blockIdx.y;\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n\n    // Use precomputed map for efficient lookup of sequence boundaries.\n    const int b_idx = q_to_b_map[q_idx];\n    const int q_start = qo_indptr[b_idx];\n    const int q_end = qo_indptr[b_idx + 1];\n    const int kv_start = kv_indptr[b_idx];\n    const int kv_end = kv_indptr[b_idx + 1];\n\n    // Handle empty sequences.\n    if (q_start >= q_end || kv_start >= kv_end) {\n        if (tid == 0) {\n            lse[q_idx * kNumHeads + head_idx] = -INFINITY;\n        }\n        for (int i = tid; i < kHeadDimCkv; i += kBlockSize) {\n             output[(q_idx * kNumHeads + head_idx) * kHeadDimCkv + i] = __float2bfloat16(0.0f);\n        }\n        return;\n    }\n\n    // Causal mask implementation: only attend to keys up to the current query's position.\n    const int q_len = q_end - q_start;\n    const int kv_len = kv_end - kv_start;\n    const int prefix_len = kv_len - q_len;\n    const int q_pos_in_seq = q_idx - q_start;\n    const int kv_len_causal = prefix_len + q_pos_in_seq + 1;\n\n    // --- Shared Memory Layout ---\n    extern __shared__ float smem[];\n    __nv_bfloat16* q_nope_sh = (__nv_bfloat16*)smem;\n    __nv_bfloat16* q_pe_sh = q_nope_sh + kHeadDimCkv;\n    __nv_bfloat16* k_nope_tile_sh = q_pe_sh + kHeadDimKpe;\n    __nv_bfloat16* k_pe_tile_sh = k_nope_tile_sh + kTileK * kHeadDimCkv;\n    float* logits_tile_sh = (float*)(k_pe_tile_sh + kTileK * kHeadDimKpe);\n    float* reduce_smem = logits_tile_sh + kTileK;\n\n    // --- Load Q vectors into Shared Memory ---\n    const __nv_bfloat16* q_nope_ptr = q_nope + (q_idx * kNumHeads + head_idx) * kHeadDimCkv;\n    const __nv_bfloat16* q_pe_ptr = q_pe + (q_idx * kNumHeads + head_idx) * kHeadDimKpe;\n    for (int i = tid; i < kHeadDimCkv; i += kBlockSize) {\n        q_nope_sh[i] = q_nope_ptr[i];\n    }\n    for (int i = tid; i < kHeadDimKpe; i += kBlockSize) {\n        q_pe_sh[i] = q_pe_ptr[i];\n    }\n    __syncthreads();\n\n    // --- Pass 1: Find max_logit for numerical stability ---\n    float max_logit = -FLT_MAX;\n    for (int k_base = 0; k_base < kv_len_causal; k_base += kTileK) {\n        const int num_k_in_tile = min(kTileK, kv_len_causal - k_base);\n\n        // Load K tile into shared memory. Each warp loads one K vector.\n        if (warp_id < num_k_in_tile) {\n            const int k_idx = k_base + warp_id;\n            const int page_idx = kv_indices[kv_start + k_idx];\n            for (int i = lane_id; i < kHeadDimCkv; i += 32) {\n                k_nope_tile_sh[warp_id * kHeadDimCkv + i] = ckv_cache[page_idx * kHeadDimCkv + i];\n            }\n            for (int i = lane_id; i < kHeadDimKpe; i += 32) {\n                k_pe_tile_sh[warp_id * kHeadDimKpe + i] = kpe_cache[page_idx * kHeadDimKpe + i];\n            }\n        }\n        __syncthreads();\n\n        // Compute logits. Each warp computes one logit (one Q-K dot product).\n        if (warp_id < num_k_in_tile) {\n            float dot_nope = 0.0f, dot_pe = 0.0f;\n            for (int i = lane_id; i < kHeadDimCkv; i += 32) dot_nope += __bfloat162float(q_nope_sh[i]) * __bfloat162float(k_nope_tile_sh[warp_id * kHeadDimCkv + i]);\n            for (int i = lane_id; i < kHeadDimKpe; i += 32) dot_pe += __bfloat162float(q_pe_sh[i]) * __bfloat162float(k_pe_tile_sh[warp_id * kHeadDimKpe + i]);\n            \n            dot_nope = warp_reduce_sum(dot_nope);\n            dot_pe = warp_reduce_sum(dot_pe);\n\n            if (lane_id == 0) {\n                float logit = (dot_nope + dot_pe) * sm_scale;\n                max_logit = fmaxf(max_logit, logit);\n            }\n        }\n    }\n\n    max_logit = block_reduce_max(max_logit, reduce_smem);\n    if (kv_len_causal == 0) max_logit = 0.0f; // Prevent -inf for empty sequences.\n\n    // --- Pass 2: Compute sum_exp and weighted sum of V (V is ckv_cache) ---\n    float sum_exp = 0.0f;\n    float out_accum[kCkvVecsPerThread] = {0.0f};\n\n    for (int k_base = 0; k_base < kv_len_causal; k_base += kTileK) {\n        const int num_k_in_tile = min(kTileK, kv_len_causal - k_base);\n        \n        // Reload K tile (also serves as V tile).\n        if (warp_id < num_k_in_tile) {\n            const int k_idx = k_base + warp_id;\n            const int page_idx = kv_indices[kv_start + k_idx];\n            for (int i = lane_id; i < kHeadDimCkv; i += 32) k_nope_tile_sh[warp_id * kHeadDimCkv + i] = ckv_cache[page_idx * kHeadDimCkv + i];\n            for (int i = lane_id; i < kHeadDimKpe; i += 32) k_pe_tile_sh[warp_id * kHeadDimKpe + i] = kpe_cache[page_idx * kHeadDimKpe + i];\n        }\n        __syncthreads();\n\n        // Re-compute logits, calculate probs, and accumulate output.\n        if (warp_id < num_k_in_tile) {\n            float dot_nope = 0.0f, dot_pe = 0.0f;\n            for (int i = lane_id; i < kHeadDimCkv; i += 32) dot_nope += __bfloat162float(q_nope_sh[i]) * __bfloat162float(k_nope_tile_sh[warp_id * kHeadDimCkv + i]);\n            for (int i = lane_id; i < kHeadDimKpe; i += 32) dot_pe += __bfloat162float(q_pe_sh[i]) * __bfloat162float(k_pe_tile_sh[warp_id * kHeadDimKpe + i]);\n            \n            dot_nope = warp_reduce_sum(dot_nope);\n            dot_pe = warp_reduce_sum(dot_pe);\n\n            if (lane_id == 0) {\n                float logit = (dot_nope + dot_pe) * sm_scale;\n                logits_tile_sh[warp_id] = expf(logit - max_logit);\n            }\n        }\n        __syncthreads();\n        \n        // Accumulate sum_exp and weighted values.\n        for (int k_tile = 0; k_tile < num_k_in_tile; ++k_tile) {\n            float prob = logits_tile_sh[k_tile];\n            sum_exp += prob;\n            const __nv_bfloat16* v_vec = k_nope_tile_sh + k_tile * kHeadDimCkv; // V = ckv_cache\n            #pragma unroll\n            for (int i = 0; i < kCkvVecsPerThread; ++i) {\n                int dim_idx = tid + i * kBlockSize;\n                out_accum[i] += prob * __bfloat162float(v_vec[dim_idx]);\n            }\n        }\n        __syncthreads();\n    }\n\n    // --- Finalization and Store to Global Memory ---\n    const float inv_sum_exp = (sum_exp > 0.0f) ? 1.0f / sum_exp : 0.0f;\n    __nv_bfloat16* output_ptr = output + (q_idx * kNumHeads + head_idx) * kHeadDimCkv;\n    #pragma unroll\n    for (int i = 0; i < kCkvVecsPerThread; ++i) {\n        int dim_idx = tid + i * kBlockSize;\n        output_ptr[dim_idx] = __float2bfloat16(out_accum[i] * inv_sum_exp);\n    }\n\n    if (tid == 0) {\n        if (sum_exp > 0.0f) {\n            lse[q_idx * kNumHeads + head_idx] = (max_logit + logf(sum_exp)) / logf(2.0f);\n        } else {\n            lse[q_idx * kNumHeads + head_idx] = -INFINITY;\n        }\n    }\n}\n\nvoid launch_mla_paged_prefill_causal(\n    torch::Tensor output,\n    torch::Tensor lse,\n    torch::Tensor q_nope,\n    torch::Tensor q_pe,\n    torch::Tensor ckv_cache,\n    torch::Tensor kpe_cache,\n    torch::Tensor qo_indptr,\n    torch::Tensor kv_indptr,\n    torch::Tensor kv_indices,\n    torch::Tensor q_to_b_map,\n    float sm_scale,\n    cudaStream_t stream)\n{\n    const int total_q = q_nope.size(0);\n    const dim3 grid_dim(total_q, kNumHeads, 1);\n    const dim3 block_dim(kBlockSize, 1, 1);\n\n    // Calculate dynamic shared memory size needed by the kernel.\n    size_t q_sh_size = (kHeadDimCkv + kHeadDimKpe) * sizeof(__nv_bfloat16);\n    size_t k_sh_size = kTileK * (kHeadDimCkv + kHeadDimKpe) * sizeof(__nv_bfloat16);\n    size_t temp_sh_size = kTileK * sizeof(float) + kWarpsPerBlock * sizeof(float);\n    size_t shared_mem_size = q_sh_size + k_sh_size + temp_sh_size;\n\n    mla_paged_prefill_causal_kernel<<<grid_dim, block_dim, shared_mem_size, stream>>>(\n        reinterpret_cast<__nv_bfloat16*>(output.data_ptr()),\n        lse.data_ptr<float>(),\n        reinterpret_cast<const __nv_bfloat16*>(q_nope.data_ptr()),\n        reinterpret_cast<const __nv_bfloat16*>(q_pe.data_ptr()),\n        reinterpret_cast<const __nv_bfloat16*>(ckv_cache.data_ptr()),\n        reinterpret_cast<const __nv_bfloat16*>(kpe_cache.data_ptr()),\n        qo_indptr.data_ptr<int32_t>(),\n        kv_indptr.data_ptr<int32_t>(),\n        kv_indices.data_ptr<int32_t>(),\n        q_to_b_map.data_ptr<int32_t>(),\n        sm_scale\n    );\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include \"kernel.h\"\n\n#include <torch/extension.h>\n#include <pybind11/pybind11.h>\n#include <pybind11/stl.h>\n\n#include <vector>\n#include <stdexcept>\n#include <string>\n#include <cmath>\n\nnamespace py = pybind11;\n\n#define CUDA_CHECK(call)                                                    \\\n  do {                                                                      \\\n    cudaError_t err = call;                                                 \\\n    if (err != cudaSuccess) {                                               \\\n      throw std::runtime_error(std::string(\"CUDA Error: \") +                 \\\n                               cudaGetErrorString(err) + \" at \" + __FILE__ + \\\n                               \":\" + std::to_string(__LINE__));             \\\n    }                                                                       \\\n  } while (0)\n\n/**\n * @brief Python entry point for the MLA Paged Prefill kernel.\n *\n * This function is exposed to Python via Pybind11. It performs extensive input\n * validation, prepares output tensors, pre-computes a helper mapping (`q_to_b_map`)\n * on the CPU for efficiency, and then calls the CUDA kernel launcher.\n *\n * @return A pair of torch::Tensor objects: the attention output and the LSE values.\n */\nstd::pair<torch::Tensor, torch::Tensor> run(\n    torch::Tensor q_nope,\n    torch::Tensor q_pe,\n    torch::Tensor ckv_cache,\n    torch::Tensor kpe_cache,\n    torch::Tensor qo_indptr,\n    torch::Tensor kv_indptr,\n    torch::Tensor kv_indices,\n    py::object sm_scale_obj)\n{\n    // --- Input Validation ---\n    TORCH_CHECK(q_nope.is_cuda(), \"q_nope must be a CUDA tensor\");\n    TORCH_CHECK(q_nope.scalar_type() == torch::kBFloat16, \"q_nope must be of bfloat16 type\");\n    TORCH_CHECK(q_pe.is_cuda() && q_pe.scalar_type() == torch::kBFloat16, \"q_pe must be a CUDA bfloat16 tensor\");\n    TORCH_CHECK(ckv_cache.is_cuda() && ckv_cache.scalar_type() == torch::kBFloat16, \"ckv_cache must be a CUDA bfloat16 tensor\");\n    TORCH_CHECK(kpe_cache.is_cuda() && kpe_cache.scalar_type() == torch::kBFloat16, \"kpe_cache must be a CUDA bfloat16 tensor\");\n    TORCH_CHECK(qo_indptr.is_cuda() && qo_indptr.scalar_type() == torch::kInt32, \"qo_indptr must be a CUDA int32 tensor\");\n    TORCH_CHECK(kv_indptr.is_cuda() && kv_indptr.scalar_type() == torch::kInt32, \"kv_indptr must be a CUDA int32 tensor\");\n    TORCH_CHECK(kv_indices.is_cuda() && kv_indices.scalar_type() == torch::kInt32, \"kv_indices must be a CUDA int32 tensor\");\n\n    // Check dimensions against constants\n    const int64_t total_q = q_nope.size(0);\n    const int64_t num_qo_heads = q_nope.size(1);\n    const int64_t head_dim_ckv = q_nope.size(2);\n    const int64_t head_dim_kpe = q_pe.size(2);\n    const int64_t page_size = ckv_cache.size(1);\n\n    TORCH_CHECK(num_qo_heads == 16, \"num_qo_heads must be 16\");\n    TORCH_CHECK(head_dim_ckv == 512, \"head_dim_ckv must be 512\");\n    TORCH_CHECK(head_dim_kpe == 64, \"head_dim_kpe must be 64\");\n    TORCH_CHECK(page_size == 1, \"page_size must be 1\");\n\n    // --- Softmax Scale ---\n    float sm_scale;\n    if (sm_scale_obj.is_none()) {\n        sm_scale = 1.0f / std::sqrt(128.0f + 64.0f);\n    } else {\n        sm_scale = py::cast<float>(sm_scale_obj);\n    }\n\n    // --- Prepare Outputs ---\n    auto output = torch::empty_like(q_nope);\n    auto lse = torch::empty({total_q, num_qo_heads}, q_nope.options().dtype(torch::kFloat32));\n\n    // --- Host-side Pre-computation of q_to_b_map ---\n    // This map allows the kernel to quickly find the batch index for any given query token,\n    // avoiding a complex and divergent search within the kernel.\n    torch::Tensor qo_indptr_cpu = qo_indptr.to(torch::kCPU);\n    auto qo_indptr_acc = qo_indptr_cpu.accessor<int32_t, 1>();\n    const int32_t batch_size = qo_indptr_cpu.size(0) - 1;\n\n    auto q_to_b_map = torch::empty({total_q}, torch::TensorOptions().dtype(torch::kInt32));\n    auto q_to_b_map_acc = q_to_b_map.accessor<int32_t, 1>();\n\n    #pragma omp parallel for\n    for (int b = 0; b < batch_size; ++b) {\n        int32_t q_start = qo_indptr_acc[b];\n        int32_t q_end = qo_indptr_acc[b + 1];\n        for (int q_idx = q_start; q_idx < q_end; ++q_idx) {\n            if (q_idx < total_q) {\n                 q_to_b_map_acc[q_idx] = b;\n            }\n        }\n    }\n    auto q_to_b_map_gpu = q_to_b_map.to(q_nope.device());\n\n    // --- Launch Kernel ---\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    launch_mla_paged_prefill_causal(\n        output, lse, q_nope, q_pe, ckv_cache, kpe_cache,\n        qo_indptr, kv_indptr, kv_indices, q_to_b_map_gpu, sm_scale, stream);\n\n    CUDA_CHECK(cudaGetLastError());\n\n    return {output, lse};\n}\n\n// --- Pybind11 Module Definition ---\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"MLA Paged Prefill (Causal)\",\n        py::arg(\"q_nope\"),\n        py::arg(\"q_pe\"),\n        py::arg(\"ckv_cache\"),\n        py::arg(\"kpe_cache\"),\n        py::arg(\"qo_indptr\"),\n        py::arg(\"kv_indptr\"),\n        py::arg(\"kv_indices\"),\n        py::arg(\"sm_scale\") = py::none()\n    );\n}"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for mla_paged_prefill_causal_h16_ckv512_kpe64_ps1 (round 10)"
}
