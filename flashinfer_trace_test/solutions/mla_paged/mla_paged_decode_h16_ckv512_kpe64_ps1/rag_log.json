{
  "definition": {
    "name": "mla_paged_decode_h16_ckv512_kpe64_ps1",
    "op_type": "mla_paged",
    "description": "Name: mla_paged_decode_h16_ckv512_kpe64_ps1\nType: mla_paged\n\nAxes:\n  batch_size: variable\n  num_qo_heads: constant = 16 (Number of query heads after tensor parallel split (128/8=16).)\n  head_dim_ckv: constant = 512\n  head_dim_kpe: constant = 64\n  page_size: constant = 1\n  num_pages: variable (Total number of allocated pages in the KV cache.)\n  len_indptr: variable (Length of kv_indptr array.)\n  num_kv_indices: variable (Total number of KV page indices.)\n\nInputs:\n  q_nope: [batch_size, num_qo_heads, head_dim_ckv] (bfloat16) - Query tensor without positional encoding component.\n  q_pe: [batch_size, num_qo_heads, head_dim_kpe] (bfloat16) - Query positional encoding component.\n  ckv_cache: [num_pages, page_size, head_dim_ckv] (bfloat16) - Compressed key-value cache.\n  kpe_cache: [num_pages, page_size, head_dim_kpe] (bfloat16) - Key positional encoding cache.\n  kv_indptr: [len_indptr] (int32) - KV page offsets for each sequence. For decode (single-query), we don't need qo_indptr.\n  kv_indices: [num_kv_indices] (int32) - Page indices for KV cache lookups.\n  sm_scale: scalar (float32) - Softmax scale. Default is (1/sqrt(128 + 64) = 1/sqrt(192)), based on head dimensions before matrix absorption.\n\nOutputs:\n  output: [batch_size, num_qo_heads, head_dim_ckv] (bfloat16)\n  lse: [batch_size, num_qo_heads] (float32) - The 2-based log-sum-exp of attention logits.\n\nConstraints:\n  - len_indptr == batch_size + 1\n  - num_kv_indices == kv_indptr[-1].item()\n\n\nReference Implementation:\nimport math\nimport torch\n\n\n@torch.no_grad()\ndef run(q_nope, q_pe, ckv_cache, kpe_cache, kv_indptr, kv_indices, sm_scale):\n    batch_size, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    page_size = ckv_cache.shape[1]\n    len_indptr = kv_indptr.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n\n    # Check constants\n    assert num_qo_heads == 16\n    assert head_dim_ckv == 512\n    assert head_dim_kpe == 64\n    assert page_size == 1\n\n    # Check constraints\n    assert len_indptr == batch_size + 1\n    assert num_kv_indices == kv_indptr[-1].item()\n\n    device = q_nope.device\n\n    Kc_all = ckv_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_ckv]\n    Kp_all = kpe_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_kpe]\n\n    output = torch.zeros(\n        (batch_size, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=device\n    )\n    lse = torch.full((batch_size, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=device)\n\n    for b in range(batch_size):\n        page_beg = int(kv_indptr[b].item())\n        page_end = int(kv_indptr[b + 1].item())\n\n        if page_beg >= page_end:\n            # No KV cache for this batch element\n            output[b].zero_()\n            continue\n\n        pages = kv_indices[page_beg:page_end]\n        # Derive kv_len from kv_indptr (for page_size=1, num_pages == num_tokens)\n        L_tokens = page_end - page_beg\n\n        if L_tokens <= 0 or pages.numel() == 0:\n            output[b].zero_()\n            continue\n\n        # Pages are token indices for page_size=1\n        tok_idx = pages[:L_tokens].to(torch.long)\n\n        Kc = Kc_all[tok_idx]  # [L_tokens, head_dim_ckv]\n        Kp = Kp_all[tok_idx]  # [L_tokens, head_dim_kpe]\n        qn = q_nope[b].to(torch.float32)  # [num_qo_heads, head_dim_ckv]\n        qp = q_pe[b].to(torch.float32)  # [num_qo_heads, head_dim_kpe]\n\n        logits = (qn @ Kc.T) + (qp @ Kp.T)  # [num_qo_heads, L_tokens]\n        logits_scaled = logits * sm_scale\n\n        # Compute 2-base LSE\n        lse[b] = torch.logsumexp(logits_scaled, dim=-1) / math.log(2.0)\n\n        attn = torch.softmax(logits_scaled, dim=-1)  # [num_qo_heads, L_tokens]\n        out = attn @ Kc  # [num_qo_heads, head_dim_ckv]\n        output[b] = out.to(torch.bfloat16)\n\n    return output, lse"
  },
  "model": "gemini-3-flash-preview",
  "language": "cuda",
  "target_gpu": "H100",
  "rag_rounds": [
    {
      "mode": "sequential",
      "round": 0,
      "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
      "rag_data": "--- Reference Document 1: # #define CUDA_ARRAY3D_VIDEO_ENCODE_DECODE 0x100\n\nThis flag indicates that the CUDA array will be used for hardware accelerated video encode/decode\noperations.\n\n--- Reference Document 2: # void *cudaKernelNodeParamsV2::func\n\nKernel to launch\n\n--- Reference Document 3: # CUfunction CUDA_KERNEL_NODE_PARAMS_v2::func\n\nKernel to launch\n\n--- Reference Document 4: # 9.7.4.10. Half Precision Floating Point Instructions: ex2\n\n#### 9.7.4.10. [Half Precision Floating Point Instructions: `ex2`](https://docs.nvidia.com/cuda/parallel-thread-execution/#half-precision-floating-point-instructions-ex2)[\uf0c1](https://docs.nvidia.com/cuda/parallel-thread-execution/#half-precision-floating-point-instructions-ex2 \"Permalink to this headline\")\n\n`ex2`\n\nFind the base-2 exponent of input.\n\nSyntax\n\n```\nex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2}\n```\n\nCopy to clipboard\n\nDescription\n\nRaise 2 to the power `a`.\n\nThe type of operands `d` and `a` are as specified by `.type`.\n\nFor `.f16x2` or `.bf16x2` instruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a `.f16x2` or `.bf16x2`.\n\nSemantics\n\n```\nif (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n}\n```\n\nCopy to clipboard\n\nNotes\n\n`ex2.approx.{f16, f16x2, bf16, bf16x2}` implement a fast approximation to 2a.\n\nFor the `.f16` type, subnormal inputs are supported. `ex2.approx.ftz.bf16` flushes subnormal\ninputs and results to sign-preserving zero.\n\nResults of `ex2.approx.ftz.bf16` for various corner-case inputs are as follows:\n\n| Input | Result |\n| --- | --- |\n| -Inf | +0.0 |\n| -subnormal | +1.0 |\n| -0.0 | +1.0 |\n| +0.0 | +1.0 |\n| +subnormal | +1.0 |\n| +Inf | +Inf |\n| NaN | NaN |\n\nResults of `ex2.approx.f16` for various corner-case inputs are as follows:\n\n| Input | Result |\n| --- | --- |\n| -Inf | +0.0 |\n| -0.0 | +1.0 |\n| +0.0 | +1.0 |\n| +Inf | +Inf |\n| NaN | NaN |\n\nThe maximum relative error for `.f16` type is 2-9.9. The maximum relative error for `.bf16` type\nis 2-7.\n\nPTX ISA Notes\n\nIntroduced in PTX ISA version 7.0.\n\n`ex2.approx.ftz.{bf16/bf16x2}` introduced in PTX ISA version 7.8.\n\nTarget ISA Notes\n\nRequires `sm_75` or higher.\n\n`ex2.approx.ftz.{bf16/bf16x2}` requires `sm_90` or higher.\n\nExamples\n\n```\nex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2;\n```\n\nCopy to clipboard\n\n--- Reference Document 5: # 10.24.2. Alternate Floating Point\n\n### 10.24.2. Alternate Floating Point[\uf0c1](#alternate-floating-point \"Permalink to this headline\")\n\nTensor Cores support alternate types of floating point operations on devices with compute capability 8.0 and higher.\n\n`__nv_bfloat16`\n:   This data format is an alternate fp16 format that has the same range as f32 but reduced precision (7 bits). You can use this data format directly with the `__nv_bfloat16` type available in `cuda_bf16.h`. Matrix fragments with `__nv_bfloat16` data types are required to be composed with accumulators of `float` type. The shapes and operations supported are the same as with `__half`.\n\n`tf32`\n:   This data format is a special floating point format supported by Tensor Cores, with the same range as f32 and reduced precision (>=10 bits). The internal layout of this format is implementation defined. In order to use this floating point format with WMMA operations, the input matrices must be manually converted to tf32 precision.\n\n    To facilitate conversion, a new intrinsic `__float_to_tf32` is provided. While the input and output arguments to the intrinsic are of `float` type, the output will be `tf32` numerically. This new precision is intended to be used with Tensor Cores only, and if mixed with other `float`type operations, the precision and range of the result will be undefined.\n\n    Once an input matrix (`matrix_a` or `matrix_b`) is converted to tf32 precision, the combination of a `fragment` with `precision::tf32` precision, and a data type of `float` to `load_matrix_sync` will take advantage of this new capability. Both the accumulator fragments must have `float` data types. The only supported matrix size is 16x16x8 (m-n-k).\n\n    The elements of the fragment are represented as `float`, hence the mapping from `element_type<T>` to `storage_element_type<T>` is:\n\n    ```\n    precision::tf32 -> float\n    ```\n\n--- Reference Document 6: >    > >   >   ptr = H(flag); // ERROR: H() attempts to refer to host variables 'qqq' and 'www'.\n   >    > >   >                  // code will compile, but will NOT execute correctly.\n   >    > >   >   return *ptr;\n   >    > >   > }\n   >    > >   > ```\n   >    > > * Use of exceptions (`throw/catch`) and RTTI (`typeid, dynamic_cast`). Example:\n   >    > >\n   >    > >   > ```\n   >    > >   > struct Base { };\n   >    > >   > struct Derived : public Base { };\n   >    > >   >\n   >    > >   > // NOTE: \"H\" is emitted in generated device code\n   >    > >   > constexpr int H(bool b, Base *ptr) {\n   >    > >   >   if (b) {\n   >    > >   >     return 1;\n   >    > >   >   } else if (typeid(ptr) == typeid(Derived)) { // ERROR: use of typeid in code executing on the GPU\n   >    > >   >     return 2;\n   >    > >   >   } else {\n   >    > >   >     throw int{4}; // ERROR: use of throw in code executing on the GPU\n   >    > >   >   }\n   >    > >   > }\n   >    > >   > __device__ void doit(bool flag) {\n   >    > >   >   int val;\n   >    > >   >   Derived d;\n   >    > >   >   val = H(flag, &d); //ERROR: H() attempts use typeid and throw(), which are not allowed in code that executes on the GPU\n   >    > >   > }\n   >    > >   > ```\n   > 3. During host code generation, the body of a `__device__`-only constexpr function `D` is preserved in the code sent to the host compiler. If the body of `D` attempts to ODR-use a namespace scope device variable or a `__device__`-only non-constexpr function, then the call to `D` from host code is not supported (code may build without compiler diagnostics, but may behave incorrectly at run time). Example:\n   >\n   >    > ```\n   >    > __device__ int qqq, www;\n   >    > constexpr __device__ int* D(bool b) { return b ? &qqq : &www; };\n   >    >\n   >    > int doit(bool flag) {\n   >    >   int *ptr;\n   >    >   ptr = D(flag); // ERROR: D() attempts to refer to device variables 'qqq' and 'www'\n   >    >                  // code will compile, but will NOT execute correctly.\n   >    >   return *ptr;\n   >    > }\n   >    > ```\n   > 4. **Note: Given above restrictions and lack of compiler diagnostics for incorrect usage, be careful when calling a constexpr \\_\\_host\\_\\_ function in the standard C++ headers from device code**, since the implementation of the function will vary depending on the host platform, e.g., based on the `libstdc++` version for gcc host compiler. Such code may break silently when being ported to a different platform or host compiler version (if the target C++ library implementation odr-uses a host code variable or function, as described earlier).\n   >\n   >    > Example:\n   >    >\n   >    > ```\n   >    > __device__ int get(int in) {\n   >    >  int val = std::foo(in); // \"std::foo\" is constexpr function defined in the host compiler's standard library header\n   >    >                          // WARNING: if std::foo implementation ODR-uses host variables or functions,\n   >    >                          // code will not work corre\n... (truncated)\n\n--- Reference Document 7: # 9.7.16.3.1.2. Absolute address mode for K dimension being 48B\n\n###### 9.7.16.3.1.2. [Absolute address mode for K dimension being 48B](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-leading-dimension-byte-offset-absolute-address)[\uf0c1](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-leading-dimension-byte-offset-absolute-address \"Permalink to this headline\")\n\nThe `tcgen05.mma` instruction with *K-dimension* of 48B would overflow the 128B\nshared memory boundary if the data is packed contiguously.\n\nIn this case, the absolute address mode can be used to break up the data in the\nshared memory into two chunks such that both these chunks are laid out within\nthe aligned 128-byte address boundary.\nThe leading dimension absolute address can point to the second data chunk in the shared memory.\n\n--- Reference Document 8: # 9.7.16.4.1. Shared memory descriptor\n\n##### 9.7.16.4.1. [Shared memory descriptor](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor)[\uf0c1](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor \"Permalink to this headline\")\n\nThe shared memory descriptor describes the properties of multiplicand matrix in shared\nmemory including its location in the shared memory of the current *CTA*. It is a 64-bit\nvalue contained in a register with the following layout:\n\nTable 40 Shared memory descriptor layout[\uf0c1](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-desc-layout \"Permalink to this table\")\n\n\n\n\n\n| Bit-field | Size in bits | Description |\n| --- | --- | --- |\n| 0-13 | 14 | matrix-descriptor-encode (Matrix start address) |\n| 16-29 | 14 | matrix-descriptor-encode ([Leading dimension byte offset relative](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-leading-dimension-byte-offset))  OR  matrix-descriptor-encode ([Leading dimension byte address absolute](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-leading-dimension-byte-offset)) |\n| 32-45 | 14 | matrix-descriptor-encode ([Stride dimension byte offset](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-stride-dimension-byte-offset)) |\n| 46-48 | 3 | Fixed constant value of 0b001 |\n| 49-51 | 3 | Matrix base offset |\n| 52 | 1 | Leading dimension stride mode: - 0: byte offset relative - 1: byte address absolute |\n| 53-60 | 8 | Fixed constant value of 0xb00000000 |\n| 61-63 | 3 | Specifies the swizzling mode to be used: 0. No swizzling 1. 128-Byte with 32B atomic swizzling 2. 128-Byte swizzling 4. 64-Byte swizzling 6. 32-Byte swizzling  Note: Values 3, 5 and 7 are invalid |\n\nwhere matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\n\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode\nstarts as per shown in [Table 41](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-start-addr-swizzle-mode).\n\nTable 41 Starting address of repeating pattern for various swizzling modes[\uf0c1](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-start-addr-swizzle-mode \"Permalink to this table\")\n\n\n\n\n| Swizzling mode | Starting address of the repeating pattern |\n| --- | --- |\n| 128-Byte swizzle | 1024-Byte boundary |\n| 64-Byte swizzle | 512-Byte boundary |\n| 32-Byte swizzle | 256-Byte boundary |\n\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\n`base offset = (pattern start addr >> 0x7) & 0x7`\n\nThe following must be 16-byte aligned:\n\n1. Matrix start address\n2. Leading dimension byte offset\n3. Stride dimension byte offset\n\n--- Reference Document 9: # 10.28.2. Multi-Stage Asynchronous Data Copies using cuda::pipeline\n\n### 10.28.2. Multi-Stage Asynchronous Data Copies using `cuda::pipeline`[\uf0c1](#multi-stage-asynchronous-data-copies-using-cuda-pipeline \"Permalink to this headline\")\n\nIn the previous examples with [cooperative\\_groups::wait](#collectives-cg-wait) and [cuda::barrier](#aw-barrier), the kernel threads immediately wait for the data transfer to shared memory to complete. This avoids data transfers from global memory into registers, but does not *hide* the latency of the `memcpy_async` operation by overlapping computation.\n\nFor that we use the CUDA [pipeline](#pipeline-interface) feature in the following example. It provides a mechanism for managing a sequence of `memcpy_async` batches, enabling CUDA kernels to overlap memory transfers with computation. The following example implements a two-stage pipeline that overlaps data-transfer with computation. It:\n\n* Initializes the pipeline shared state (more below)\n* Kickstarts the pipeline by scheduling a `memcpy_async` for the first batch.\n* Loops over all the batches: it schedules `memcpy_async` for the next batch, blocks all threads on the completion of the `memcpy_async` for the previous batch, and then overlaps the computation on the previous batch with the asynchronous copy of the memory for the next batch.\n* Finally, it drains the pipeline by performing the computation on the last batch.\n\nNote that, for interoperability with `cuda::pipeline`, `cuda::memcpy_async` from the `cuda/pipeline` header is used here.\n\n```\n#include <cooperative_groups/memcpy_async.h>\n#include <cuda/pipeline>\n\n__device__ void compute(int* global_out, int const* shared_in);\n__global__ void with_staging(int* global_out, int const* global_in, size_t size, size_t batch_sz) {\n    auto grid = cooperative_groups::this_grid();\n    auto block = cooperative_groups::this_thread_block();\n    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size\n\n    constexpr size_t stages_count = 2; // Pipeline with two stages\n    // Two batches must fit in shared memory:\n    extern __shared__ int shared[];  // stages_count * block.size() * sizeof(int) bytes\n    size_t shared_offset[stages_count] = { 0, block.size() }; // Offsets to each batch\n\n    // Allocate shared storage for a two-stage cuda::pipeline:\n    __shared__ cuda::pipeline_shared_state<\n        cuda::thread_scope::thread_scope_block,\n        stages_count\n    > shared_state;\n    auto pipeline = cuda::make_pipeline(block, &shared_state);\n\n    // Each thread processes `batch_sz` elements.\n    // Compute offset of the batch `batch` of this thread block in global memory:\n    auto block_batch = [&](size_t batch) -> int {\n      return block.group_index().x * block.size() + grid.size() * batch;\n    };\n\n    // Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block:\n    if (batch_sz == 0) return;\n    pipeline.producer_acquire();\n    cuda::memcpy_async(block, s\n... (truncated)"
    }
  ]
}