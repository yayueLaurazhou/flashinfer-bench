{
  "name": "gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0",
  "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
  "author": "gemini-3-flash-preview",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.cpp::run"
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#ifndef KERNEL_H\n#define KERNEL_H\n\n#include <cuda_bf16.h>\n#include <cuda_runtime.h>\n#include <stdint.h>\n\nvoid mla_paged_decode_launcher(\n    const __nv_bfloat16* q_nope,\n    const __nv_bfloat16* q_pe,\n    const __nv_bfloat16* ckv_cache,\n    const __nv_bfloat16* kpe_cache,\n    const int32_t* kv_indptr,\n    const int32_t* kv_indices,\n    float sm_scale,\n    int batch_size,\n    int num_qo_heads,\n    __nv_bfloat16* output,\n    float* lse,\n    cudaStream_t stream\n);\n\n#endif // KERNEL_H"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <cuda_bf16.h>\n#include <cuda_runtime.h>\n#include <math_constants.h>\n\n// Efficient warp reduction\n__device__ __forceinline__ float warpReduceSum(float val) {\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void __launch_bounds__(256) mla_paged_decode_kernel(\n    const __nv_bfloat16* __restrict__ q_nope,      // [B, 16, 512]\n    const __nv_bfloat16* __restrict__ q_pe,        // [B, 16, 64]\n    const __nv_bfloat16* __restrict__ ckv_cache,   // [Pages, 512]\n    const __nv_bfloat16* __restrict__ kpe_cache,   // [Pages, 64]\n    const int32_t* __restrict__ kv_indptr,\n    const int32_t* __restrict__ kv_indices,\n    float sm_scale_log2,\n    int batch_size,\n    __nv_bfloat16* __restrict__ output,\n    float* __restrict__ lse\n) {\n    // Grid: (16, batch_size)\n    const int head_idx = blockIdx.x;\n    const int batch_idx = blockIdx.y;\n    const int tid = threadIdx.x;\n\n    // Load queries into registers (each thread handles 2 dims)\n    const int q_nope_base = (batch_idx * 16 + head_idx) * 256; // 512 / 2\n    const __nv_bfloat162 qn_vec = ((const __nv_bfloat162*)q_nope)[q_nope_base + tid];\n    \n    __nv_bfloat162 qp_vec = __floats2bfloat162_rn(0.0f, 0.0f);\n    if (tid < 32) { // 64 / 2\n        const int q_pe_base = (batch_idx * 16 + head_idx) * 32;\n        qp_vec = ((const __nv_bfloat162*)q_pe)[q_pe_base + tid];\n    }\n\n    const int page_beg = kv_indptr[batch_idx];\n    const int page_end = kv_indptr[batch_idx + 1];\n\n    // Online softmax accumulators\n    float m = -CUDART_INF_F;\n    float d = 0.0f;\n    float acc_x = 0.0f;\n    float acc_y = 0.0f;\n\n    __shared__ float s_logit_reduce[8]; // For 8 warps\n\n    for (int p = page_beg; p < page_end; ++p) {\n        const int page_idx = kv_indices[p];\n        \n        // 1. Dot Product Query_nope * CKV\n        const __nv_bfloat162 ck_vec = ((const __nv_bfloat162*)ckv_cache)[page_idx * 256 + tid];\n        float dot = __bfloat162float(qn_vec.x) * __bfloat162float(ck_vec.x) + \n                    __bfloat162float(qn_vec.y) * __bfloat162float(ck_vec.y);\n        \n        // 2. Dot Product Query_pe * KPE\n        if (tid < 32) {\n            const __nv_bfloat162 kp_vec = ((const __nv_bfloat162*)kpe_cache)[page_idx * 32 + tid];\n            dot += __bfloat162float(qp_vec.x) * __bfloat162float(kp_vec.x) + \n                   __bfloat162float(qp_vec.y) * __bfloat162float(kp_vec.y);\n        }\n\n        // 3. Block Reduction for Logit\n        float warp_sum = warpReduceSum(dot);\n        if ((tid & 31) == 0) s_logit_reduce[tid >> 5] = warp_sum;\n        __syncthreads();\n\n        float logit;\n        if (tid < 32) {\n            float b = (tid < 8) ? s_logit_reduce[tid] : 0.0f;\n            #pragma unroll\n            for (int offset = 4; offset > 0; offset /= 2)\n                b += __shfl_down_sync(0xff, b, offset);\n            if (tid == 0) s_logit_reduce[0] = b;\n        }\n        __syncthreads();\n        logit = s_logit_reduce[0] * sm_scale_log2;\n\n        // 4. Online Softmax Update\n        float m_old = m;\n        m = fmaxf(m, logit);\n        float p_old = exp2f(m_old - m);\n        float p_curr = exp2f(logit - m);\n\n        d = d * p_old + p_curr;\n        acc_x = acc_x * p_old + p_curr * __bfloat162float(ck_vec.x);\n        acc_y = acc_y * p_old + p_curr * __bfloat162float(ck_vec.y);\n    }\n\n    // 5. Final Normalization and Store\n    const float inv_d = 1.0f / (d + 1e-9f);\n    const int out_base = (batch_idx * 16 + head_idx) * 256;\n    ((__nv_bfloat162*)output)[out_base + tid] = __floats2bfloat162_rn(acc_x * inv_d, acc_y * inv_d);\n\n    if (tid == 0) {\n        lse[batch_idx * 16 + head_idx] = m + log2f(d + 1e-9f);\n    }\n}\n\nvoid mla_paged_decode_launcher(\n    const __nv_bfloat16* q_nope,\n    const __nv_bfloat16* q_pe,\n    const __nv_bfloat16* ckv_cache,\n    const __nv_bfloat16* kpe_cache,\n    const int32_t* kv_indptr,\n    const int32_t* kv_indices,\n    float sm_scale,\n    int batch_size,\n    int num_qo_heads,\n    __nv_bfloat16* output,\n    float* lse,\n    cudaStream_t stream\n) {\n    if (batch_size <= 0) return;\n\n    // Convert sm_scale to log2 base for exp2f\n    const float sm_scale_log2 = sm_scale * 1.44269504089f;\n\n    dim3 grid(num_qo_heads, batch_size);\n    dim3 block(256);\n\n    mla_paged_decode_kernel<<<grid, block, 0, stream>>>(\n        q_nope, q_pe, ckv_cache, kpe_cache, kv_indptr, kv_indices,\n        sm_scale_log2, batch_size, output, lse\n    );\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include <torch/extension.h>\n#include <at/cuda/CUDAContext.h>\n#include <cuda_runtime.h>\n#include \"kernel.h\"\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_BF16(x) TORCH_CHECK(x.dtype() == torch::kBFloat16, #x \" must be bfloat16\")\n\nstd::vector<torch::Tensor> run(\n    torch::Tensor q_nope,\n    torch::Tensor q_pe,\n    torch::Tensor ckv_cache,\n    torch::Tensor kpe_cache,\n    torch::Tensor kv_indptr,\n    torch::Tensor kv_indices,\n    float sm_scale\n) {\n    CHECK_CUDA(q_nope); CHECK_BF16(q_nope); CHECK_CONTIGUOUS(q_nope);\n    CHECK_CUDA(q_pe); CHECK_BF16(q_pe); CHECK_CONTIGUOUS(q_pe);\n    CHECK_CUDA(ckv_cache); CHECK_BF16(ckv_cache); CHECK_CONTIGUOUS(ckv_cache);\n    CHECK_CUDA(kpe_cache); CHECK_BF16(kpe_cache); CHECK_CONTIGUOUS(kpe_cache);\n    CHECK_CUDA(kv_indptr); CHECK_CONTIGUOUS(kv_indptr);\n    CHECK_CUDA(kv_indices); CHECK_CONTIGUOUS(kv_indices);\n\n    int batch_size = q_nope.size(0);\n    int num_qo_heads = q_nope.size(1);\n    \n    auto output = torch::empty_like(q_nope);\n    auto lse = torch::empty({batch_size, num_qo_heads}, q_nope.options().dtype(torch::kFloat32));\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    mla_paged_decode_launcher(\n        reinterpret_cast<const __nv_bfloat16*>(q_nope.data_ptr<at::BFloat16>()),\n        reinterpret_cast<const __nv_bfloat16*>(q_pe.data_ptr<at::BFloat16>()),\n        reinterpret_cast<const __nv_bfloat16*>(ckv_cache.data_ptr<at::BFloat16>()),\n        reinterpret_cast<const __nv_bfloat16*>(kpe_cache.data_ptr<at::BFloat16>()),\n        kv_indptr.data_ptr<int32_t>(),\n        kv_indices.data_ptr<int32_t>(),\n        sm_scale,\n        batch_size,\n        num_qo_heads,\n        reinterpret_cast<__nv_bfloat16*>(output.data_ptr<at::BFloat16>()),\n        lse.data_ptr<float>(),\n        stream\n    );\n\n    return {output, lse};\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"MLA Paged Decode Kernel (H100 Optimized)\",\n          py::arg(\"q_nope\"),\n          py::arg(\"q_pe\"),\n          py::arg(\"ckv_cache\"),\n          py::arg(\"kpe_cache\"),\n          py::arg(\"kv_indptr\"),\n          py::arg(\"kv_indices\"),\n          py::arg(\"sm_scale\"));\n}"
    }
  ],
  "description": "gemini-3-flash-preview optimized kernel for mla_paged_decode_h16_ckv512_kpe64_ps1 (round 3, candidate 0)"
}