{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":8},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indices"}},"uuid":"00cb2bc2-c7c7-43a1-b857-b516eb2ce061"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:01.147591","log":"[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":8},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indices"}},"uuid":"00cb2bc2-c7c7-43a1-b857-b516eb2ce061"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:15:19.700918","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indices"}},"uuid":"c2e8ef55-ec17-442c-acd9-30ba9e9ecd4f"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:01.920360","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indices"}},"uuid":"c2e8ef55-ec17-442c-acd9-30ba9e9ecd4f"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:01.938438","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indices"}},"uuid":"deb5f26c-b29b-47f4-b5e6-bb70f89080e4"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:02.709406","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indices"}},"uuid":"deb5f26c-b29b-47f4-b5e6-bb70f89080e4"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:02.740961","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":308},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indices"}},"uuid":"7cd78cb9-ecb1-469c-a651-1063ffa4d662"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:03.510766","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":308},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indices"}},"uuid":"7cd78cb9-ecb1-469c-a651-1063ffa4d662"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:03.511052","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indices"}},"uuid":"951de642-3020-4954-b87e-8d21ed0edf70"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:04.281627","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indices"}},"uuid":"951de642-3020-4954-b87e-8d21ed0edf70"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:04.282383","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":508},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indices"}},"uuid":"4d9f5e3e-84d0-43d4-bb18-c451ef8009c0"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:05.048333","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":508},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indices"}},"uuid":"4d9f5e3e-84d0-43d4-bb18-c451ef8009c0"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:05.048062","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":608},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indices"}},"uuid":"5c200fe9-c07f-4dab-a8e8-dc8ff9b17c20"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:05.817397","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":608},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indices"}},"uuid":"5c200fe9-c07f-4dab-a8e8-dc8ff9b17c20"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:05.817860","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indices"}},"uuid":"d232cde2-c7ff-4f74-aac4-f6c86d59c0b6"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:06.586840","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indices"}},"uuid":"d232cde2-c7ff-4f74-aac4-f6c86d59c0b6"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:06.586652","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":808},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indices"}},"uuid":"2cde88e6-dc7a-4cd0-8a97-980cd2c02740"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:07.354866","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":808},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indices"}},"uuid":"2cde88e6-dc7a-4cd0-8a97-980cd2c02740"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:07.355498","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indices"}},"uuid":"53a7dfec-6b9f-479d-afcd-1878dbe090bd"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:08.118612","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indices"}},"uuid":"53a7dfec-6b9f-479d-afcd-1878dbe090bd"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:08.118035","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1908},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indices"}},"uuid":"f26d4f7f-b6fd-4868-953f-84fa4af0e120"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:08.888126","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1908},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indices"}},"uuid":"f26d4f7f-b6fd-4868-953f-84fa4af0e120"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:08.887721","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1008},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indices"}},"uuid":"e6bef587-9992-4a28-8339-f4a7c197d457"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:09.657392","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1008},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indices"}},"uuid":"e6bef587-9992-4a28-8339-f4a7c197d457"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:09.657076","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indices"}},"uuid":"d0da33e2-2d94-42b5-be8a-09111f9f2649"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:10.419928","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indices"}},"uuid":"d0da33e2-2d94-42b5-be8a-09111f9f2649"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:10.420482","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indices"}},"uuid":"990b57e3-2975-41a1-be67-ecd1ba020887"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:11.189824","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indices"}},"uuid":"990b57e3-2975-41a1-be67-ecd1ba020887"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:11.190284","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indices"}},"uuid":"28210082-1728-4ee1-a365-9d3f0f64170a"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:11.960623","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indices"}},"uuid":"28210082-1728-4ee1-a365-9d3f0f64170a"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:11.961278","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":2757},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indices"}},"uuid":"5e86857b-bb94-4f2c-b9e6-6000c253dc10"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:13.758971","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":2757},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indices"}},"uuid":"5e86857b-bb94-4f2c-b9e6-6000c253dc10"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:13.759403","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":1857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indices"}},"uuid":"34026642-ef29-42d6-88b9-4b0ff96c5553"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:15.566603","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":1857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indices"}},"uuid":"34026642-ef29-42d6-88b9-4b0ff96c5553"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:15.567031","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":3557},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indices"}},"uuid":"89070a1d-4d70-4122-8955-03cae1300f24"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:17.342555","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":3557},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indices"}},"uuid":"89070a1d-4d70-4122-8955-03cae1300f24"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:17.342974","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":4357},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indices"}},"uuid":"1250b1f6-1957-4001-ad14-9f511b3d6c83"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:19.134386","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":4357},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indices"}},"uuid":"1250b1f6-1957-4001-ad14-9f511b3d6c83"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:19.133966","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":7257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indices"}},"uuid":"5bfb8416-4862-448a-a52c-c684372f03d2"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:20.920399","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":7257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indices"}},"uuid":"5bfb8416-4862-448a-a52c-c684372f03d2"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:20.919973","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indices"}},"uuid":"57106903-f8b5-491b-8b6c-58e2d77b222e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:22.715140","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indices"}},"uuid":"57106903-f8b5-491b-8b6c-58e2d77b222e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:22.714720","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indices"}},"uuid":"373074ba-f398-4a23-b628-90778267ef79"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:24.497621","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indices"}},"uuid":"373074ba-f398-4a23-b628-90778267ef79"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:24.498071","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":9657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indices"}},"uuid":"c4e14a53-dad1-404e-ae4f-bbe3355d8504"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:26.278159","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":9657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indices"}},"uuid":"c4e14a53-dad1-404e-ae4f-bbe3355d8504"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:26.277748","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indices"}},"uuid":"f42ce054-47e4-4db1-a4f8-650674a8d6d1"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:28.062114","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indices"}},"uuid":"f42ce054-47e4-4db1-a4f8-650674a8d6d1"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:28.062733","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":10857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indices"}},"uuid":"220b10b5-0a65-4892-8bf0-4d0ecf5b69d2"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:29.856844","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":10857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indices"}},"uuid":"220b10b5-0a65-4892-8bf0-4d0ecf5b69d2"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:29.857263","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":6657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indices"}},"uuid":"787d2d2f-548c-46ab-9ded-55fd30b1de20"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:31.629256","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":6657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indices"}},"uuid":"787d2d2f-548c-46ab-9ded-55fd30b1de20"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:31.628848","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":12857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indices"}},"uuid":"e417264f-195d-4204-89fa-3ebdb539f1cf"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:33.452353","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":12857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indices"}},"uuid":"e417264f-195d-4204-89fa-3ebdb539f1cf"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:33.452075","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":457},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indices"}},"uuid":"a128f96f-583a-48cb-aceb-d70e0015aca3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:35.290302","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":457},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indices"}},"uuid":"a128f96f-583a-48cb-aceb-d70e0015aca3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:35.290898","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":14857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indices"}},"uuid":"bd2dae14-7bae-4edb-964f-2163accf506e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:37.078684","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":14857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indices"}},"uuid":"bd2dae14-7bae-4edb-964f-2163accf506e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:37.079157","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:38.868666","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:38.869137","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indices"}},"uuid":"7a9dc58c-7844-4b82-9a02-722027bcf4f3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:40.658336","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indices"}},"uuid":"7a9dc58c-7844-4b82-9a02-722027bcf4f3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:40.658531","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":27545},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indices"}},"uuid":"fd4b2558-ee4f-4d9e-ab3f-7a8333db6340"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:45.947159","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":27545},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indices"}},"uuid":"fd4b2558-ee4f-4d9e-ab3f-7a8333db6340"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:45.947462","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":30745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indices"}},"uuid":"7e083fc8-60fc-4a06-8536-5bb47511e81d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:51.299539","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":30745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indices"}},"uuid":"7e083fc8-60fc-4a06-8536-5bb47511e81d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:51.300197","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":33945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indices"}},"uuid":"60b29b12-9c97-44ea-b82b-765fd04e2a38"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:56.685848","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":33945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indices"}},"uuid":"60b29b12-9c97-44ea-b82b-765fd04e2a38"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:16:56.686439","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":37145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indices"}},"uuid":"b018a35e-fff0-4baa-9c2f-397db906c529"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:02.053383","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":37145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indices"}},"uuid":"b018a35e-fff0-4baa-9c2f-397db906c529"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:02.053842","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":40345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indices"}},"uuid":"86a0eb47-e5b4-4854-adac-7b85b2774afe"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:07.408983","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":40345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indices"}},"uuid":"86a0eb47-e5b4-4854-adac-7b85b2774afe"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:07.409256","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":44845},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indices"}},"uuid":"80797c3d-c602-495e-be70-a6e87f2d0479"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:12.771692","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":44845},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indices"}},"uuid":"80797c3d-c602-495e-be70-a6e87f2d0479"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:12.772146","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":48045},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indices"}},"uuid":"f4367b30-b3e2-4e11-8554-e10602245b1d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:18.082581","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":48045},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indices"}},"uuid":"f4367b30-b3e2-4e11-8554-e10602245b1d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:18.082733","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":51245},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indices"}},"uuid":"b15c7099-4863-4c19-a8c2-c8c7201f38f6"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:23.422092","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":51245},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indices"}},"uuid":"b15c7099-4863-4c19-a8c2-c8c7201f38f6"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:23.422537","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":54445},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indices"}},"uuid":"65082ffb-4d9d-4ad4-bc55-5216464087c7"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:28.788308","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":54445},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indices"}},"uuid":"65082ffb-4d9d-4ad4-bc55-5216464087c7"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:28.788833","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":57645},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indices"}},"uuid":"d771f51c-9886-42bf-a55f-2ff996ba3725"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:34.090355","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":57645},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indices"}},"uuid":"d771f51c-9886-42bf-a55f-2ff996ba3725"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:34.091218","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":75145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indices"}},"uuid":"5bef8d88-0f74-4ccb-a256-b02842951df3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:39.417787","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":75145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indices"}},"uuid":"5bef8d88-0f74-4ccb-a256-b02842951df3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:39.418074","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":9945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indices"}},"uuid":"45be562f-a643-46ca-be28-229e56a23e87"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:44.749987","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":9945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indices"}},"uuid":"45be562f-a643-46ca-be28-229e56a23e87"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:44.750452","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":62345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indices"}},"uuid":"939f995a-1ab2-4d19-8d94-50f07e73542d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:50.206366","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":62345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indices"}},"uuid":"939f995a-1ab2-4d19-8d94-50f07e73542d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:50.206965","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":16345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indices"}},"uuid":"c9ba5e7f-839b-446f-9c02-7ab25e23fb7e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:55.506734","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":16345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indices"}},"uuid":"c9ba5e7f-839b-446f-9c02-7ab25e23fb7e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:17:55.507528","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":22745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indices"}},"uuid":"0c746a7a-977c-46cd-a4be-2403b80ad7ef"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:18:03.196558","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":22745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indices"}},"uuid":"0c746a7a-977c-46cd-a4be-2403b80ad7ef"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:18:03.198967","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":68745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indices"}},"uuid":"1c3743b9-d48e-453a-a023-9b52b1b73989"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:18:10.832551","log":"Solution skipped after 3 failures. Last error: [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function ‘pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)’:\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: ‘getCurrentCUDAStream’ is not a member of ‘at::cuda’\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu(11): warning #177-D: variable \"kPageSize\" was declared but never referenced\n  constexpr int kPageSize = 1;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":68745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indices"}},"uuid":"1c3743b9-d48e-453a-a023-9b52b1b73989"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-06T14:18:10.832693","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":8},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indices"}},"uuid":"00cb2bc2-c7c7-43a1-b857-b516eb2ce061"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:24.363456","log":"[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":8},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_a16f582b73fce6caef5f134e603b1b7f.safetensors","tensor_key":"kv_indices"}},"uuid":"00cb2bc2-c7c7-43a1-b857-b516eb2ce061"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:13.999248","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indices"}},"uuid":"c2e8ef55-ec17-442c-acd9-30ba9e9ecd4f"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:25.137385","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6b9a98f6fd77891ec2f73e48ace809cc.safetensors","tensor_key":"kv_indices"}},"uuid":"c2e8ef55-ec17-442c-acd9-30ba9e9ecd4f"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:25.156523","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indices"}},"uuid":"deb5f26c-b29b-47f4-b5e6-bb70f89080e4"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:25.928598","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_0d8ad4f1af7fad223887b0038ff557f8.safetensors","tensor_key":"kv_indices"}},"uuid":"deb5f26c-b29b-47f4-b5e6-bb70f89080e4"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:25.947021","log":"Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":308},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indices"}},"uuid":"7cd78cb9-ecb1-469c-a651-1063ffa4d662"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:26.719617","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":308},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_169ed339eea3d24711ae68c75d85b02e.safetensors","tensor_key":"kv_indices"}},"uuid":"7cd78cb9-ecb1-469c-a651-1063ffa4d662"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:26.719999","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indices"}},"uuid":"951de642-3020-4954-b87e-8d21ed0edf70"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:27.495764","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4e9bc468398aa7e78b2d3b8793be5ac8.safetensors","tensor_key":"kv_indices"}},"uuid":"951de642-3020-4954-b87e-8d21ed0edf70"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:27.496236","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":508},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indices"}},"uuid":"4d9f5e3e-84d0-43d4-bb18-c451ef8009c0"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:28.263245","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":508},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4d43ceb91b8279725b8dde472af1e3ec.safetensors","tensor_key":"kv_indices"}},"uuid":"4d9f5e3e-84d0-43d4-bb18-c451ef8009c0"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:28.263066","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":608},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indices"}},"uuid":"5c200fe9-c07f-4dab-a8e8-dc8ff9b17c20"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:29.030804","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":608},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_11d6bd6c6bde4e2f69cb1c3c1fb6ab9a.safetensors","tensor_key":"kv_indices"}},"uuid":"5c200fe9-c07f-4dab-a8e8-dc8ff9b17c20"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:29.031093","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indices"}},"uuid":"d232cde2-c7ff-4f74-aac4-f6c86d59c0b6"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:29.798496","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e32d6f56496a0c2a9aa3e6d863bec7cd.safetensors","tensor_key":"kv_indices"}},"uuid":"d232cde2-c7ff-4f74-aac4-f6c86d59c0b6"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:29.798915","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":808},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indices"}},"uuid":"2cde88e6-dc7a-4cd0-8a97-980cd2c02740"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:30.566673","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":808},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_ac1ab6e74b23946e8c6c1ac471caac0d.safetensors","tensor_key":"kv_indices"}},"uuid":"2cde88e6-dc7a-4cd0-8a97-980cd2c02740"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:30.567279","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indices"}},"uuid":"53a7dfec-6b9f-479d-afcd-1878dbe090bd"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:31.333114","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2708},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_81265163bff775ea75ee1f3f15b718ad.safetensors","tensor_key":"kv_indices"}},"uuid":"53a7dfec-6b9f-479d-afcd-1878dbe090bd"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:31.333596","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1908},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indices"}},"uuid":"f26d4f7f-b6fd-4868-953f-84fa4af0e120"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:32.106878","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1908},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fe99e02ff96ae87d12afb757b78e4d90.safetensors","tensor_key":"kv_indices"}},"uuid":"f26d4f7f-b6fd-4868-953f-84fa4af0e120"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:32.107413","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1008},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indices"}},"uuid":"e6bef587-9992-4a28-8339-f4a7c197d457"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:32.876427","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1008},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_959fc08aa1b67f155c533197b37e7d7c.safetensors","tensor_key":"kv_indices"}},"uuid":"e6bef587-9992-4a28-8339-f4a7c197d457"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:32.876045","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indices"}},"uuid":"d0da33e2-2d94-42b5-be8a-09111f9f2649"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:33.638813","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":2408},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_07c1eccbadab38d43cd39f438c7cca32.safetensors","tensor_key":"kv_indices"}},"uuid":"d0da33e2-2d94-42b5-be8a-09111f9f2649"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:33.639006","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indices"}},"uuid":"990b57e3-2975-41a1-be67-ecd1ba020887"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:34.407529","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1108},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9354c0b72ea7a0a36562b79eb01d6e98.safetensors","tensor_key":"kv_indices"}},"uuid":"990b57e3-2975-41a1-be67-ecd1ba020887"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:34.408118","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indices"}},"uuid":"28210082-1728-4ee1-a365-9d3f0f64170a"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:35.176479","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":1,"num_pages":989669,"len_indptr":2,"num_kv_indices":1208},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4109d6ae4964a18dd219a8556373cfff.safetensors","tensor_key":"kv_indices"}},"uuid":"28210082-1728-4ee1-a365-9d3f0f64170a"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:35.176949","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":2757},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indices"}},"uuid":"5e86857b-bb94-4f2c-b9e6-6000c253dc10"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:36.963571","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":2757},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_790adb67e7850c1c00f1f6e5e009e659.safetensors","tensor_key":"kv_indices"}},"uuid":"5e86857b-bb94-4f2c-b9e6-6000c253dc10"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:36.963983","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":1857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indices"}},"uuid":"34026642-ef29-42d6-88b9-4b0ff96c5553"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:38.752722","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":1857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2493ac7a5a05d0fcc43b2112a9c1575f.safetensors","tensor_key":"kv_indices"}},"uuid":"34026642-ef29-42d6-88b9-4b0ff96c5553"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:38.753172","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":3557},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indices"}},"uuid":"89070a1d-4d70-4122-8955-03cae1300f24"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:40.517682","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":3557},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b59d36606ec2938e8550395f0033d9bf.safetensors","tensor_key":"kv_indices"}},"uuid":"89070a1d-4d70-4122-8955-03cae1300f24"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:40.518108","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":4357},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indices"}},"uuid":"1250b1f6-1957-4001-ad14-9f511b3d6c83"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:42.302152","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":4357},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2bb5b9ced1f640f3dd48e5332c63a167.safetensors","tensor_key":"kv_indices"}},"uuid":"1250b1f6-1957-4001-ad14-9f511b3d6c83"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:42.301877","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":7257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indices"}},"uuid":"5bfb8416-4862-448a-a52c-c684372f03d2"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:44.096070","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":7257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cda0842f74af9d547e6378cec71fbbb9.safetensors","tensor_key":"kv_indices"}},"uuid":"5bfb8416-4862-448a-a52c-c684372f03d2"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:44.096776","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indices"}},"uuid":"57106903-f8b5-491b-8b6c-58e2d77b222e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:45.866073","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f1207cb3e0fa210e8a00f265761e3dd9.safetensors","tensor_key":"kv_indices"}},"uuid":"57106903-f8b5-491b-8b6c-58e2d77b222e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:45.865670","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indices"}},"uuid":"373074ba-f398-4a23-b628-90778267ef79"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:47.641711","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5057},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_7afef112b3c879b7aac37e6b3b665d3b.safetensors","tensor_key":"kv_indices"}},"uuid":"373074ba-f398-4a23-b628-90778267ef79"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:47.642060","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":9657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indices"}},"uuid":"c4e14a53-dad1-404e-ae4f-bbe3355d8504"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:49.408729","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":9657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_3434185632dafdf2a3401caad6d53647.safetensors","tensor_key":"kv_indices"}},"uuid":"c4e14a53-dad1-404e-ae4f-bbe3355d8504"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:49.409465","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indices"}},"uuid":"f42ce054-47e4-4db1-a4f8-650674a8d6d1"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:51.169415","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":5857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4ff5d5330a518f6dcb0d251644b25791.safetensors","tensor_key":"kv_indices"}},"uuid":"f42ce054-47e4-4db1-a4f8-650674a8d6d1"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:51.169880","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":10857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indices"}},"uuid":"220b10b5-0a65-4892-8bf0-4d0ecf5b69d2"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:52.941343","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":10857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_74417a078cf1a32aab36f0ef9d1fdcea.safetensors","tensor_key":"kv_indices"}},"uuid":"220b10b5-0a65-4892-8bf0-4d0ecf5b69d2"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:52.941830","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":6657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indices"}},"uuid":"787d2d2f-548c-46ab-9ded-55fd30b1de20"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:54.701295","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":6657},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8f590f2f66ef0a228ff0ec3f17deac3a.safetensors","tensor_key":"kv_indices"}},"uuid":"787d2d2f-548c-46ab-9ded-55fd30b1de20"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:54.701608","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":12857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indices"}},"uuid":"e417264f-195d-4204-89fa-3ebdb539f1cf"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:56.488381","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":12857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_e71450b1e672c0142349b2745345ea99.safetensors","tensor_key":"kv_indices"}},"uuid":"e417264f-195d-4204-89fa-3ebdb539f1cf"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:56.488652","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":457},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indices"}},"uuid":"a128f96f-583a-48cb-aceb-d70e0015aca3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:58.278480","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":457},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_8c87984718c961e9a4e72a8f42fdcb21.safetensors","tensor_key":"kv_indices"}},"uuid":"a128f96f-583a-48cb-aceb-d70e0015aca3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:52:58.279579","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":14857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indices"}},"uuid":"bd2dae14-7bae-4edb-964f-2163accf506e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:00.055897","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":14857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_54d0de621eb533a5abe620ce4d0d8c52.safetensors","tensor_key":"kv_indices"}},"uuid":"bd2dae14-7bae-4edb-964f-2163accf506e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:00.056332","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:01.824580","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:01.825027","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indices"}},"uuid":"7a9dc58c-7844-4b82-9a02-722027bcf4f3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:03.585517","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":8857},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9265f6cfb04dd287318e2a66f1f22035.safetensors","tensor_key":"kv_indices"}},"uuid":"7a9dc58c-7844-4b82-9a02-722027bcf4f3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:03.586245","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":27545},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indices"}},"uuid":"fd4b2558-ee4f-4d9e-ab3f-7a8333db6340"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:08.867461","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":27545},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_451aa21cec34e1ed1e73c6249837dc11.safetensors","tensor_key":"kv_indices"}},"uuid":"fd4b2558-ee4f-4d9e-ab3f-7a8333db6340"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:08.867874","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":30745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indices"}},"uuid":"7e083fc8-60fc-4a06-8536-5bb47511e81d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:14.172833","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":30745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4b3e6a7deeab1e88f9569c7fc848902c.safetensors","tensor_key":"kv_indices"}},"uuid":"7e083fc8-60fc-4a06-8536-5bb47511e81d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:14.173301","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":33945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indices"}},"uuid":"60b29b12-9c97-44ea-b82b-765fd04e2a38"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:19.500485","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":33945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_70288e4ae642a771aaa2aa24b0b917b4.safetensors","tensor_key":"kv_indices"}},"uuid":"60b29b12-9c97-44ea-b82b-765fd04e2a38"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:19.500944","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":37145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indices"}},"uuid":"b018a35e-fff0-4baa-9c2f-397db906c529"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:24.867524","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":37145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_f26200309cc2e18e3b0b365aceab3bd3.safetensors","tensor_key":"kv_indices"}},"uuid":"b018a35e-fff0-4baa-9c2f-397db906c529"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:24.868044","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":40345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indices"}},"uuid":"86a0eb47-e5b4-4854-adac-7b85b2774afe"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:30.176151","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":40345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_fb75ca3b58eb8bec1e77326689648506.safetensors","tensor_key":"kv_indices"}},"uuid":"86a0eb47-e5b4-4854-adac-7b85b2774afe"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:30.176628","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":44845},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indices"}},"uuid":"80797c3d-c602-495e-be70-a6e87f2d0479"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:35.430443","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":44845},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_c50a37e1048689e71b2eb5f1a337dc6a.safetensors","tensor_key":"kv_indices"}},"uuid":"80797c3d-c602-495e-be70-a6e87f2d0479"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:35.430718","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":48045},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indices"}},"uuid":"f4367b30-b3e2-4e11-8554-e10602245b1d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:40.708753","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":48045},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_dbbefce74225521eeca509c30a2ed025.safetensors","tensor_key":"kv_indices"}},"uuid":"f4367b30-b3e2-4e11-8554-e10602245b1d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:40.709369","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":51245},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indices"}},"uuid":"b15c7099-4863-4c19-a8c2-c8c7201f38f6"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:46.015772","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":51245},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_6d70711a5791968863baed2682c3e80e.safetensors","tensor_key":"kv_indices"}},"uuid":"b15c7099-4863-4c19-a8c2-c8c7201f38f6"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:46.016223","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":54445},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indices"}},"uuid":"65082ffb-4d9d-4ad4-bc55-5216464087c7"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:51.334346","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":54445},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_b610529c8fa3c227bed0fceb384f500a.safetensors","tensor_key":"kv_indices"}},"uuid":"65082ffb-4d9d-4ad4-bc55-5216464087c7"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:51.334957","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":57645},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indices"}},"uuid":"d771f51c-9886-42bf-a55f-2ff996ba3725"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:56.626433","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":57645},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_1903a6fbefae3725b642fe3c944bd34f.safetensors","tensor_key":"kv_indices"}},"uuid":"d771f51c-9886-42bf-a55f-2ff996ba3725"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:53:56.626926","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":75145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indices"}},"uuid":"5bef8d88-0f74-4ccb-a256-b02842951df3"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:01.958349","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":75145},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_d3d4b7cbde419fb7293699647a89551f.safetensors","tensor_key":"kv_indices"}},"uuid":"5bef8d88-0f74-4ccb-a256-b02842951df3"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:01.958813","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":9945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indices"}},"uuid":"45be562f-a643-46ca-be28-229e56a23e87"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:07.297570","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":9945},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_4164ab636133dc10ffee1ac14f3eef7b.safetensors","tensor_key":"kv_indices"}},"uuid":"45be562f-a643-46ca-be28-229e56a23e87"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:07.298276","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":62345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indices"}},"uuid":"939f995a-1ab2-4d19-8d94-50f07e73542d"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:12.587929","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":62345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_2623004a2d23ebf6825a01a2050e494d.safetensors","tensor_key":"kv_indices"}},"uuid":"939f995a-1ab2-4d19-8d94-50f07e73542d"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:12.587645","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":16345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indices"}},"uuid":"c9ba5e7f-839b-446f-9c02-7ab25e23fb7e"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:17.893537","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":16345},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9789d6c015f5726df0fd8a3692b0d69e.safetensors","tensor_key":"kv_indices"}},"uuid":"c9ba5e7f-839b-446f-9c02-7ab25e23fb7e"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:17.894013","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":22745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indices"}},"uuid":"0c746a7a-977c-46cd-a4be-2403b80ad7ef"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:23.173090","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":22745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indices"}},"uuid":"0c746a7a-977c-46cd-a4be-2403b80ad7ef"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:23.172685","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":68745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indices"}},"uuid":"1c3743b9-d48e-453a-a023-9b52b1b73989"},"solution":"gemini-2.5-pro_cuda_292432","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:28.451928","log":"Solution skipped after 3 failures. Last error: [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nFAILED: [code=1] kernel.cuda.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output kernel.cuda.o.d -ccbin /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_103,code=sm_103 -gencode=arch=compute_110,code=sm_110 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=compute_121 -gencode=arch=compute_121,code=sm_121 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/kernel.cu -o kernel.cuda.o \nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\nnvcc fatal   : Unsupported gpu architecture 'compute_103'\n[2/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \nFAILED: [code=1] main.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=torch_gemini_2_5_pro_cuda_292432_475bd3 -DTORCH_API_INCLUDE_EXTENSION_H -I/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3 -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/zhouyayue/miniconda3/envs/flashinfer/include/python3.10 -fPIC -std=c++17 -c /home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp -o main.o \n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp: In function 'pybind11::dict run(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)':\n/home/zhouyayue/.cache/flashinfer_bench/cache/torch/torch_gemini_2_5_pro_cuda_292432_475bd3/main.cpp:73:37: error: 'getCurrentCUDAStream' is not a member of 'at::cuda'\n   73 |     cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n      |                                     ^~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\nBuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build\n    subprocess.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 154, in build\n    ext = load(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1692, in load\n    return _jit_compile(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/torch_builder.py\", line 164, in build\n    raise BuildError(f\"Torch build failed for solution '{solution.name}': {e}\") from e\nflashinfer_bench.compile.builder.BuildError: Torch build failed for solution 'gemini-2.5-pro_cuda_292432': Error building extension 'torch_gemini_2_5_pro_cuda_292432_475bd3'","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":68745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_5ef57b2aadf1bd7022585f998e2f2cbb.safetensors","tensor_key":"kv_indices"}},"uuid":"1c3743b9-d48e-453a-a023-9b52b1b73989"},"solution":"gemini-2.5-pro_triton_dorbxs","evaluation":{"status":"RUNTIME_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-07T13:54:28.451736","log":"Solution skipped after 3 failures. Last error: Traceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/evaluators/default.py\", line 116, in check_correctness\n    result = sol_runnable(*inp)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/runnable.py\", line 95, in __call__\n    ret = self._callable(*args)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 249, in run\n    return mla_paged_decode_h16_ckv512_kpe64_ps1(*args, **kwargs)\n  File \"/home/zhouyayue/.cache/flashinfer_bench/cache/triton/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/fib_triton_gemini_2_5_pro_triton_dorbxs_b8cbae/main.py\", line 223, in mla_paged_decode_h16_ckv512_kpe64_ps1\n    mla_paged_decode_h16_ckv512_kpe64_ps1_kernel[grid](\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 419, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 238, in run\n    benchmark()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 227, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 162, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 148, in kernel_call\n    self.fn.run(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 733, in run\n    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/runtime/jit.py\", line 861, in _do_compile\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 300, in compile\n    module = src.make_ir(target, options, codegen_fns, module_map, context)\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 80, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 82:23:\n        # --- Compute logits for the block ---\n        s_block = tl.zeros([BLOCK_L], dtype=tl.float32)\n        # Contribution from ckv\n        for d_start in range(0, HEAD_DIM_CKV, BLOCK_DCKV):\n            d_offs = d_start + tl.arange(0, BLOCK_DCKV)\n            q_nope_tile = tl.load(q_nope_ptr + d_offs)\n            k_ckv_ptrs = ckv_cache_ptr + indices[:, None] * ckv_cache_stride_n + d_offs[None, :]\n            k_ckv_tile = tl.load(k_ckv_ptrs, mask=l_mask[:, None], other=0.0)\n            # CORRECTNESS FIX: Reshape 1D q_nope_tile to 2D for tl.dot, then squeeze result\n            q_nope_tile_2d = tl.reshape(q_nope_tile, (BLOCK_DCKV, 1))\n            s_update = tl.dot(k_ckv_tile, q_nope_tile_2d)\n            s_block += tl.squeeze(s_update, axis=1)\n                       ^\nAttributeError(\"module 'triton.language' has no attribute 'squeeze'\")","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":64,"num_pages":989669,"len_indptr":65,"num_kv_indices":22745},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_9c9c675205492dccce4b60587f0dda20.safetensors","tensor_key":"kv_indices"}},"uuid":"0c746a7a-977c-46cd-a4be-2403b80ad7ef"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:37:38.948163","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_565619/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:40:17.540048","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.h:6:10: fatal error: torch/extension.h: No such file or directory\n    6 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.h:6:10: fatal error: torch/extension.h: No such file or directory\n    6 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r1_c0_2037e7/kernel.h:6:10: fatal error: torch/extension.h: No such file or directory\n    6 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:41:53.383985","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r2_c0_f5267a/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:43:16.622650","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r3_c0_2e00ed/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:44:49.735274","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r4_c0_3c1bbc/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:45:53.253696","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r5_c0_e77c63/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:46:33.832893","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r6_c0_4ad126/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:48:17.493566","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r7_c0_b0871c/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
{"definition":"mla_paged_decode_h16_ckv512_kpe64_ps1","workload":{"axes":{"batch_size":16,"num_pages":989669,"len_indptr":17,"num_kv_indices":17257},"inputs":{"q_nope":{"type":"random"},"q_pe":{"type":"random"},"ckv_cache":{"type":"random"},"kpe_cache":{"type":"random"},"sm_scale":{"type":"scalar","value":0.1352337747812271},"kv_indptr":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indptr"},"kv_indices":{"type":"safetensors","path":"./blob/workloads/mla_paged/mla_paged_decode_h16_ckv512_kpe64_ps1/mla_meta_cadccfc17ec3eccd8872521c3a4dbabd.safetensors","tensor_key":"kv_indices"}},"uuid":"84221f45-78f8-4d44-84f6-998153d2c1fa"},"solution":"gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0","evaluation":{"status":"COMPILE_ERROR","environment":{"hardware":"NVIDIA H100 PCIe","libs":{"torch":"2.9.1+cu128","triton":"3.5.1","cuda":"12.8"}},"timestamp":"2026-02-08T20:49:15.524758","log":"BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nTraceback:\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 264, in build\n    output_lib_path = tvm_ffi.cpp.build(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 1034, in build\n    return _build_impl(\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 540, in _build_impl\n    build_ninja(str(build_dir))\n  File \"/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/cpp/extension.py\", line 412, in build_ninja\n    raise RuntimeError(\"\\n\".join(msg))\nRuntimeError: ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/bench/runner/persistent_runner.py\", line 712, in _persistent_worker_main\n    runnable_sol = registry.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/registry.py\", line 141, in build\n    runnable = builder.build(definition, solution)\n  File \"/home/zhouyayue/flashinfer-bench/flashinfer_bench/compile/builders/tvm_ffi_builder.py\", line 272, in build\n    raise BuildError(\nflashinfer_bench.compile.builder.BuildError: TVM-FFI compilation failed for 'gemini-3-flash-preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0': ninja exited with status 1\nstdout:\n[1/3] /home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\nFAILED: [code=1] cpp_0.o \n/home/zhouyayue/miniconda3/envs/flashinfer/bin/x86_64-conda-linux-gnu-c++ -MMD -MF cpp_0.o.d -std=c++17 -fPIC -O2 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp -o cpp_0.o\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/main.cpp:1:10: fatal error: torch/extension.h: No such file or directory\n    1 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nFAILED: [code=1] cuda_0.o \n/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_0.o.d -Xcompiler -fPIC -std=c++17 -O2 -gencode=arch=compute_90,code=sm_90 -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/miniconda3/envs/flashinfer/lib/python3.10/site-packages/tvm_ffi/include -I/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69 -c /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu -o cuda_0.o\nIn file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:82,\n                 from <command-line>:\n/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:143:2: error: #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n  143 | #error -- unsupported GNU version! gcc versions later than 14 are not supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n      |  ^~~~~\nIn file included from /home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.cu:1:\n/home/zhouyayue/.cache/flashinfer_bench/cache/tvm_ffi/tvm_ffi_gemini_3_flash_preview_mla_paged_decode_h16_ckv512_kpe64_ps1_cuda_optimized_r8_c0_17db69/kernel.h:4:10: fatal error: torch/extension.h: No such file or directory\n    4 | #include <torch/extension.h>\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nninja: build stopped: subcommand failed.\n\n\n","correctness":null,"performance":null}}
